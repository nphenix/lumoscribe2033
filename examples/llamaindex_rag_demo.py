#!/usr/bin/env python3
# generated: python -m src.cli metadata-injector @ 2025-11-19T13:05:21Z
# classification: developer
"""
LlamaIndex RAG ç³»ç»Ÿæ¼”ç¤ºè„šæœ¬

åŸºäº LlamaIndex æœ€ä½³å®è·µæ¼”ç¤ºå®Œæ•´çš„ RAG åŠŸèƒ½ã€‚
"""

import asyncio
import sys
import tempfile
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from llama_index.core import Document
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI

from src.framework.rag.llamaindex_service import LlamaIndexService


async def demonstrate_llamaindex_rag():
    """æ¼”ç¤º LlamaIndex RAG ç³»ç»ŸåŠŸèƒ½"""
    print("ğŸš€ å¼€å§‹ LlamaIndex RAG ç³»ç»Ÿæ¼”ç¤º...")
    
    try:
        # 1. å‡†å¤‡æ¼”ç¤ºæ•°æ®
        print("\nğŸ“ å‡†å¤‡æ¼”ç¤ºæ•°æ®...")
        
        demo_documents = [
            Document(
                text="""
                Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”± Guido van Rossum äº 1991 å¹´åˆ›å»ºã€‚
                Python ä»¥å…¶ç®€æ´æ˜“è¯»çš„è¯­æ³•è€Œé—»åï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼ŒåŒ…æ‹¬
                é¢å‘å¯¹è±¡ã€å‘½ä»¤å¼ã€å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹ã€‚Python çš„è®¾è®¡å“²å­¦å¼ºè°ƒ
                ä»£ç çš„å¯è¯»æ€§å’Œç®€æ´çš„è¯­æ³•ç»“æ„ã€‚
                """,
                metadata={
                    "title": "Python ç¼–ç¨‹è¯­è¨€ä»‹ç»",
                    "category": "programming",
                    "language": "python",
                    "tags": ["python", "programming", "language"]
                }
            ),
            Document(
                text="""
                æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºå¼€å‘ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ï¼Œ
                ä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿæ‰§è¡Œä»»åŠ¡è€Œæ— éœ€æ˜ç¡®çš„æŒ‡ä»¤ã€‚æœºå™¨å­¦ä¹ ç®—æ³•
                åŸºäºæ ·æœ¬æ•°æ®ï¼ˆç§°ä¸º"è®­ç»ƒæ•°æ®"ï¼‰æ„å»ºæ•°å­¦æ¨¡å‹ï¼Œç”¨äºåšå‡ºé¢„æµ‹
                æˆ–å†³ç­–ï¼Œè€Œæ— éœ€ä¸ºä»»åŠ¡ç¼–ç¨‹æ˜ç¡®çš„æŒ‡ä»¤ã€‚ä¸»è¦åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€
                æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰ç§ç±»å‹ã€‚
                """,
                metadata={
                    "title": "æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ",
                    "category": "artificial_intelligence",
                    "field": "machine_learning",
                    "tags": ["machine_learning", "ai", "algorithms"]
                }
            ),
            Document(
                text="""
                ç¥ç»ç½‘ç»œæ˜¯å—ç”Ÿç‰©ç¥ç»ç½‘ç»œå¯å‘çš„è®¡ç®—ç³»ç»Ÿï¼Œé€šè¿‡ä¼°è®¡ç›¸äº’å…³è”çš„
                å•å…ƒï¼ˆç§°ä¸ºç¥ç»å…ƒï¼‰ä¹‹é—´çš„å¤æ‚å…³ç³»æ¥å¤„ç†ä¿¡æ¯ã€‚ç¥ç»ç½‘ç»œä¹Ÿç§°ä¸º
                äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰æˆ–è¿æ¥ä¸»ä¹‰ç³»ç»Ÿã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ª
                å­é›†ï¼Œä½¿ç”¨åŒ…å«å¤šä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œã€‚
                """,
                metadata={
                    "title": "ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ ",
                    "category": "deep_learning",
                    "field": "neural_networks",
                    "tags": ["neural_networks", "deep_learning", "ai"]
                }
            ),
            Document(
                text="""
                æ•°æ®åº“ç®¡ç†ç³»ç»Ÿï¼ˆDBMSï¼‰æ˜¯ä¸€ç§è½¯ä»¶åº”ç”¨ç¨‹åºï¼Œç”¨äºä¸æ•°æ®åº“ç”¨æˆ·ã€ 
                å…¶ä»–åº”ç”¨ç¨‹åºå’Œæ•°æ®åº“æœ¬èº«äº¤äº’ã€‚DBMS çš„ä¸»è¦ç›®æ ‡æ˜¯ä¸ºæ•°æ®çš„
                å­˜å‚¨ã€æ£€ç´¢å’Œç®¡ç†æä¾›ä¸€ç§æ–¹å¼ï¼ŒåŒæ—¶ç¡®ä¿æ•°æ®çš„å®‰å…¨æ€§ã€
                å®Œæ•´æ€§å’Œä¸€è‡´æ€§ã€‚å¸¸è§çš„ DBMS åŒ…æ‹¬ MySQLã€PostgreSQLã€MongoDB ç­‰ã€‚
                """,
                metadata={
                    "title": "æ•°æ®åº“ç®¡ç†ç³»ç»Ÿæ¦‚è¿°",
                    "category": "database",
                    "type": "management_system",
                    "tags": ["database", "dbms", "storage"]
                }
            ),
            Document(
                text="""
                Web å¼€å‘æ˜¯åˆ›å»º Web åº”ç”¨ç¨‹åºçš„è¿‡ç¨‹ï¼Œæ¶‰åŠ Web è®¾è®¡ã€Web å†…å®¹
                å¼€å‘ã€å®¢æˆ·ç«¯/æœåŠ¡å™¨ç«¯è„šæœ¬ã€Web åº”ç”¨ç¨‹åºå¼€å‘å’Œ Web æœåŠ¡å™¨
                é…ç½®ã€‚Web å¼€å‘çš„èŒƒå›´ä»åˆ›å»ºç®€å•çš„é™æ€é¡µé¢åˆ°å¤æ‚çš„ Web
                åº”ç”¨ç¨‹åºã€ç”µå­æ”¿åŠ¡ã€ç”µå­å•†åŠ¡ã€Web é—¨æˆ·ç­‰ã€‚ä¸»è¦æŠ€æœ¯åŒ…æ‹¬
                HTMLã€CSSã€JavaScriptã€Reactã€Vue ç­‰ã€‚
                """,
                metadata={
                    "title": "Web å¼€å‘æŠ€æœ¯",
                    "category": "web_development",
                    "type": "frontend_backend",
                    "tags": ["web_development", "frontend", "backend"]
                }
            ),
            Document(
                text="""
                è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚
                NLP æŠ€æœ¯ä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šã€æ“ä½œå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚åº”ç”¨åŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€
                æƒ…æ„Ÿåˆ†æã€è¯­éŸ³è¯†åˆ«ã€èŠå¤©æœºå™¨äººç­‰ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäº Transformer çš„æ¨¡å‹
                å¦‚ BERTã€GPT ç³»åˆ—åœ¨ NLP ä»»åŠ¡ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
                """,
                metadata={
                    "title": "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
                    "category": "nlp",
                    "field": "computational_linguistics",
                    "tags": ["nlp", "transformer", "bert"]
                }
            )
        ]
        
        print(f"âœ… å‡†å¤‡äº† {len(demo_documents)} ä¸ªæ¼”ç¤ºæ–‡æ¡£")
        
        # 2. åˆå§‹åŒ– LlamaIndex æœåŠ¡
        print("\nğŸ”§ åˆå§‹åŒ– LlamaIndex æœåŠ¡...")
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆå§‹åŒ–æœåŠ¡ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿçš„åµŒå…¥å’Œ LLMï¼‰
            service = LlamaIndexService(
                persist_dir=f"{temp_dir}/llamaindex_storage",
                chunk_size=512,
                chunk_overlap=100,
            )
            
            print("âœ… LlamaIndex æœåŠ¡å·²åˆå§‹åŒ–")
            
            # 3. åˆ›å»ºç´¢å¼•
            print("\nğŸ“Š åˆ›å»ºç´¢å¼•...")
            
            # åˆ›å»ºä¸»è¦ç´¢å¼•
            index = await service.create_index_from_documents(
                demo_documents,
                index_name="demo_index",
                use_auto_retriever=True,
                similarity_top_k=8,
            )
            
            print(f"âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ")
            print(f"   ğŸ“ˆ æ–‡æ¡£æ•°é‡: {len(index.docstore.docs)}")
            print(f"   ğŸ”— èŠ‚ç‚¹æ•°é‡: {len(index.index_struct.nodes)}")
            
            # 4. æ¼”ç¤ºæŸ¥è¯¢åŠŸèƒ½
            print("\nğŸ” æ¼”ç¤ºæŸ¥è¯¢åŠŸèƒ½...")
            
            test_queries = [
                "ä»€ä¹ˆæ˜¯ Python ç¼–ç¨‹è¯­è¨€ï¼Ÿ",
                "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ",
                "ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "æ•°æ®åº“ç®¡ç†ç³»ç»Ÿçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
                "Web å¼€å‘åŒ…æ‹¬å“ªäº›æŠ€æœ¯ï¼Ÿ",
                "è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ"
            ]
            
            for i, query in enumerate(test_queries, 1):
                print(f"\n   é—®é¢˜ {i}: {query}")
                
                try:
                    # æ‰§è¡ŒæŸ¥è¯¢
                    response = await service.query(
                        query_str=query,
                        index_name="demo_index",
                        similarity_top_k=5,
                    )
                    
                    print(f"   ğŸ’¡ å›ç­”: {str(response)[:100]}...")
                    
                    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
                    retrieved_docs = await service.retrieve(
                        query_str=query,
                        index_name="demo_index",
                        similarity_top_k=3,
                    )
                    
                    print(f"   ğŸ“„ æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
                    
                    if retrieved_docs:
                        first_doc_title = retrieved_docs[0].metadata.get("title", "æœªçŸ¥æ ‡é¢˜")
                        print(f"   ğŸ“‹ ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£: {first_doc_title}")
                        
                except Exception as e:
                    print(f"   âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            
            # 5. æ¼”ç¤ºæ··åˆç´¢å¼•
            print("\nğŸ”€ æ¼”ç¤ºæ··åˆç´¢å¼•åŠŸèƒ½...")
            
            try:
                hybrid_index = await service.create_hybrid_index(
                    demo_documents[:3],  # ä½¿ç”¨å‰3ä¸ªæ–‡æ¡£
                    index_name="hybrid_demo",
                )
                print("âœ… æ··åˆç´¢å¼•åˆ›å»ºæˆåŠŸ")
                
                # æµ‹è¯•æ··åˆæŸ¥è¯¢
                hybrid_response = await service.query(
                    query_str="Python å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»",
                    index_name="hybrid_demo",
                )
                print(f"   ğŸ§® æ··åˆæŸ¥è¯¢ç»“æœ: {str(hybrid_response)[:80]}...")
                
            except Exception as e:
                print(f"   âš ï¸ æ··åˆç´¢å¼•æ¼”ç¤ºè·³è¿‡: {e}")
            
            # 6. æ¼”ç¤ºç´¢å¼•ç®¡ç†åŠŸèƒ½
            print("\nğŸ—‚ï¸ æ¼”ç¤ºç´¢å¼•ç®¡ç†åŠŸèƒ½...")
            
            # åˆ—å‡ºæ‰€æœ‰ç´¢å¼•
            indices = service.list_indices()
            print(f"ğŸ“Š å¯ç”¨ç´¢å¼•: {indices}")
            
            # è·å–ç´¢å¼•ä¿¡æ¯
            index_info = await service.get_index_info("demo_index")
            print(f"ğŸ“ˆ ä¸»ç´¢å¼•ä¿¡æ¯:")
            print(f"   åç§°: {index_info.get('name', 'N/A')}")
            print(f"   æ–‡æ¡£æ•°: {index_info.get('document_count', 'N/A')}")
            print(f"   èŠ‚ç‚¹æ•°: {index_info.get('node_count', 'N/A')}")
            
            # 7. æ¼”ç¤ºæ–‡æ¡£ç®¡ç†
            print("\nğŸ“„ æ¼”ç¤ºæ–‡æ¡£ç®¡ç†åŠŸèƒ½...")
            
            # æ·»åŠ æ–°æ–‡æ¡£
            new_doc = Document(
                text="åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼æ•°æ®åº“æŠ€æœ¯ï¼Œé€šè¿‡åŠ å¯†ç¡®ä¿æ•°æ®çš„å®‰å…¨æ€§å’Œå®Œæ•´æ€§ã€‚",
                metadata={
                    "title": "åŒºå—é“¾æŠ€æœ¯ä»‹ç»",
                    "category": "blockchain",
                    "tags": ["blockchain", "distributed", "security"]
                }
            )
            
            try:
                await service.add_documents(
                    [new_doc],
                    index_name="demo_index",
                )
                print("âœ… æ–°æ–‡æ¡£å·²æ·»åŠ åˆ°ç´¢å¼•")
                
                # éªŒè¯æ·»åŠ 
                updated_info = await service.get_index_info("demo_index")
                print(f"ğŸ“ˆ æ›´æ–°åçš„æ–‡æ¡£æ•°é‡: {updated_info.get('document_count', 'N/A')}")
                
            except Exception as e:
                print(f"   âš ï¸ æ–‡æ¡£æ·»åŠ åŠŸèƒ½æ¼”ç¤ºè·³è¿‡: {e}")
            
            # 8. æ¼”ç¤ºé«˜çº§æ£€ç´¢åŠŸèƒ½
            print("\nğŸ¯ æ¼”ç¤ºé«˜çº§æ£€ç´¢åŠŸèƒ½...")
            
            try:
                # è·å–ç›¸å…³èŠ‚ç‚¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                nodes = await service.get_relevant_nodes(
                    "Python ç¼–ç¨‹",
                    index_name="demo_index",
                    top_k=3,
                )
                print(f"ğŸ” è·å–åˆ° {len(nodes)} ä¸ªç›¸å…³èŠ‚ç‚¹")
                
                # æ£€ç´¢ç‰¹å®šç±»å‹çš„æ–‡æ¡£
                python_docs = await service.retrieve(
                    query_str="Python",
                    index_name="demo_index",
                    similarity_top_k=5,
                )
                print(f"ğŸ æ‰¾åˆ° {len(python_docs)} ä¸ªä¸ Python ç›¸å…³çš„æ–‡æ¡£")
                
            except Exception as e:
                print(f"   âš ï¸ é«˜çº§æ£€ç´¢åŠŸèƒ½æ¼”ç¤ºè·³è¿‡: {e}")
            
            print("\nğŸ‰ LlamaIndex RAG æ¼”ç¤ºå®Œæˆï¼")
            print("\nğŸ“š æ€»ç»“:")
            print("   âœ… åŸºäº LlamaIndex æœ€ä½³å®è·µæ„å»º")
            print("   âœ… æ”¯æŒæ–‡æ¡£è‡ªåŠ¨åˆ†å‰²å’Œç´¢å¼•åˆ›å»º")
            print("   âœ… æä¾›å‘é‡æ£€ç´¢å’Œè‡ªåŠ¨æ£€ç´¢å™¨")
            print("   âœ… æ”¯æŒæ··åˆç´¢å¼•ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰")
            print("   âœ… å®ç°æŒä¹…åŒ–å­˜å‚¨å’Œç´¢å¼•ç®¡ç†")
            print("   âœ… æä¾›é«˜çº§æŸ¥è¯¢å’Œæ£€ç´¢åŠŸèƒ½")
            print("   âœ… æ”¯æŒåŠ¨æ€æ–‡æ¡£æ·»åŠ å’Œåˆ é™¤")
            print("   âœ… åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•")
            
            # 9. å¯¹æ¯”ä¼ ç»Ÿæ–¹æ³•çš„ä¼˜åŠ¿
            print("\nğŸš€ LlamaIndex ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•çš„ä¼˜åŠ¿:")
            print("   ğŸ“Š è‡ªåŠ¨æ–‡æ¡£åˆ†å‰²å’ŒèŠ‚ç‚¹ç®¡ç†")
            print("   ğŸ” å†…ç½®çš„ AutoRetriever æ™ºèƒ½æ£€ç´¢")
            print("   ğŸ¯ å¤šç§æ£€ç´¢ç­–ç•¥å’Œåå¤„ç†å™¨")
            print("   ğŸ”„ æ— ç¼çš„æŒä¹…åŒ–å’ŒåŠ è½½æœºåˆ¶")
            print("   ğŸŒ æ”¯æŒå¤šæ¨¡æ€å’Œæ··åˆæ£€ç´¢")
            print("   ğŸ› ï¸ ä¸°å¯Œçš„é…ç½®é€‰é¡¹å’Œæ‰©å±•ç‚¹")
            print("   ğŸ“ˆ å†…ç½®çš„æ€§èƒ½ä¼˜åŒ–å’Œç¼“å­˜æœºåˆ¶")
            
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        try:
            await service.close()
            print("\nğŸ§¹ èµ„æºå·²æ¸…ç†")
        except Exception as e:
            print(f"\nâš ï¸ æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")


async def demonstrate_file_based_indexing():
    """æ¼”ç¤ºåŸºäºæ–‡ä»¶çš„ç´¢å¼•åˆ›å»º"""
    print("\nğŸ“ æ¼”ç¤ºåŸºäºæ–‡ä»¶çš„ç´¢å¼•åˆ›å»º...")
    
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆ›å»ºä¸€äº›ç¤ºä¾‹æ–‡ä»¶
            test_files = {
                "python_guide.md": """
                # Python ç¼–ç¨‹æŒ‡å—
                
                Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ã€‚
                Python å¹¿æ³›åº”ç”¨äº Web å¼€å‘ã€æ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚
                
                ## ä¸»è¦ç‰¹æ€§
                
                - ç®€æ´æ˜“è¯»çš„è¯­æ³•
                - ä¸°å¯Œçš„æ ‡å‡†åº“
                - å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒ
                - è·¨å¹³å°å…¼å®¹æ€§
                """,
                "machine_learning.txt": """
                æœºå™¨å­¦ä¹ æŠ€æœ¯æ¦‚è¿°
                
                æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
                
                1. ç›‘ç£å­¦ä¹ ï¼šä½¿ç”¨æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒ
                2. æ— ç›‘ç£å­¦ä¹ ï¼šå‘ç°æ•°æ®ä¸­çš„æ¨¡å¼
                3. å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡è¯•é”™å­¦ä¹ æœ€ä¼˜ç­–ç•¥
                
                åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿç­‰ã€‚
                """,
                "web_development.html": """
                <html>
                <head><title>Web å¼€å‘æŠ€æœ¯</title></head>
                <body>
                <h1>Web å¼€å‘æŠ€æœ¯æŒ‡å—</h1>
                <p>Web å¼€å‘æ¶‰åŠå‰ç«¯å’Œåç«¯æŠ€æœ¯çš„ç»“åˆã€‚</p>
                <ul>
                <li>HTML: ç»“æ„æ ‡è®°</li>
                <li>CSS: æ ·å¼è®¾è®¡</li>
                <li>JavaScript: äº¤äº’é€»è¾‘</li>
                </ul>
                </body>
                </html>
                """
            }
            
            # å†™å…¥æµ‹è¯•æ–‡ä»¶
            for filename, content in test_files.items():
                file_path = Path(temp_dir) / filename
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            
            print(f"âœ… åˆ›å»ºäº† {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
            
            # åˆ›å»ºåŸºäºæ–‡ä»¶çš„ç´¢å¼•
            service = LlamaIndexService(
                persist_dir=f"{temp_dir}/file_storage",
            )
            
            # ä»ç›®å½•åˆ›å»ºç´¢å¼•
            index = await service.create_index_from_directory(
                directory_path=temp_dir,
                index_name="file_based_index",
                file_extensions=[".md", ".txt", ".html"],
                similarity_top_k=6,
            )
            
            print(f"âœ… åŸºäºæ–‡ä»¶çš„ç´¢å¼•åˆ›å»ºæˆåŠŸ")
            print(f"   ğŸ“ˆ ç´¢å¼•ä¸­çš„æ–‡æ¡£æ•°é‡: {len(index.docstore.docs)}")
            
            # æµ‹è¯•æŸ¥è¯¢
            test_queries = [
                "Python çš„ä¸»è¦ç‰¹æ€§æ˜¯ä»€ä¹ˆï¼Ÿ",
                "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ",
                "Web å¼€å‘åŒ…æ‹¬å“ªäº›æŠ€æœ¯ï¼Ÿ"
            ]
            
            for query in test_queries:
                response = await service.query(
                    query_str=query,
                    index_name="file_based_index",
                )
                print(f"   ğŸ“‹ é—®é¢˜: {query}")
                print(f"   ğŸ’¡ ç­”æ¡ˆ: {str(response)[:100]}...")
            
            await service.close()
            
    except Exception as e:
        print(f"âŒ æ–‡ä»¶ç´¢å¼•æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸ¯ LlamaIndex RAG ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(demonstrate_llamaindex_rag())
    
    # æ¼”ç¤ºæ–‡ä»¶ç´¢å¼•
    asyncio.run(demonstrate_file_based_indexing())