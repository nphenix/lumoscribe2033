# LangChain 1.0 配置示例
# 将此文件复制为 .env 并填入您的实际配置

# === 基础 API 密钥配置 ===
OPENAI_API_KEY=your_openai_api_key_here
GROQ_API_KEY=your_groq_api_key_here
XAI_API_KEY=your_xai_api_key_here
OLLAMA_BASE_URL=http://localhost:11434

# === 模型参数配置 ===
# 环境变量前缀（默认：LC_MODEL_）
LANGCHAIN_MODEL_PREFIX=LC_MODEL_

# openai-gpt4 参数
LC_MODEL_OPENAI-GPT4_TEMPERATURE=0.1
LC_MODEL_OPENAI-GPT4_MAX_TOKENS=4000
LC_MODEL_OPENAI-GPT4_TIMEOUT=30
LC_MODEL_OPENAI-GPT4_ENABLED=true
LC_MODEL_OPENAI-GPT4_PRIORITY=100

# openai-gpt35-turbo 参数
LC_MODEL_OPENAI-GPT35-TURBO_TEMPERATURE=0.1
LC_MODEL_OPENAI-GPT35-TURBO_MAX_TOKENS=4000
LC_MODEL_OPENAI-GPT35-TURBO_TIMEOUT=30
LC_MODEL_OPENAI-GPT35-TURBO_ENABLED=true
LC_MODEL_OPENAI-GPT35-TURBO_PRIORITY=90

# ollama-mistral 参数
LC_MODEL_OLLAMA-MISTRAL_TEMPERATURE=0.7
LC_MODEL_OLLAMA-MISTRAL_MAX_TOKENS=4000
LC_MODEL_OLLAMA-MISTRAL_TIMEOUT=30
LC_MODEL_OLLAMA-MISTRAL_ENABLED=true
LC_MODEL_OLLAMA-MISTRAL_PRIORITY=80

# ollama-llama2 参数
LC_MODEL_OLLAMA-LLAMA2_TEMPERATURE=0.7
LC_MODEL_OLLAMA-LLAMA2_MAX_TOKENS=4000
LC_MODEL_OLLAMA-LLAMA2_TIMEOUT=30
LC_MODEL_OLLAMA-LLAMA2_ENABLED=true
LC_MODEL_OLLAMA-LLAMA2_PRIORITY=70

# groq-llama3 参数
LC_MODEL_GROQ-LLAMA3_TEMPERATURE=0.7
LC_MODEL_GROQ-LLAMA3_MAX_TOKENS=4000
LC_MODEL_GROQ-LLAMA3_TIMEOUT=30
LC_MODEL_GROQ-LLAMA3_ENABLED=true
LC_MODEL_GROQ-LLAMA3_PRIORITY=60

# xai-grok 参数
LC_MODEL_XAI-GROK_TEMPERATURE=0.7
LC_MODEL_XAI-GROK_MAX_TOKENS=4000
LC_MODEL_XAI-GROK_TIMEOUT=30
LC_MODEL_XAI-GROK_ENABLED=true
LC_MODEL_XAI-GROK_PRIORITY=50

# 自定义参数示例（JSON 格式）
# LC_MODEL_OPENAI-GPT4_PARAMS={"response_format": "json", "stream": false}

# === 路由配置 ===
LANGCHAIN_ROUTING_ENABLE_PERFORMANCE=true
LANGCHAIN_ROUTING_ENABLE_COST_OPTIMIZATION=true
LANGCHAIN_ROUTING_CONFIDENCE_THRESHOLD=0.7
LANGCHAIN_ROUTING_MAX_RETRIES=2

# === 监控配置 ===
LANGCHAIN_MONITORING_COLLECT_METRICS=true
LANGCHAIN_MONITORING_LOG_ROUTING=true
LANGCHAIN_MONITORING_METRIC_WINDOW_SIZE=100