name: Enhanced CI Pipeline (LangChain 1.0)

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: "3.12"
  CONDA_ENV_NAME: "lumoscribe2033"

jobs:
  # å¢å¼ºçš„å…ƒæ•°æ®éªŒè¯
  enhanced-metadata-verification:
    name: ğŸ” Enhanced Metadata Verification
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-version: latest
          activate-environment: ${{ env.CONDA_ENV_NAME }}
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        shell: bash -l {0}
        run: |
          pip install -r requirements.txt
          pip install langchain==1.0.* langchain-agents pydantic
      
      - name: Enhanced Metadata Verification
        run: |
          echo "ğŸ” Running Enhanced Metadata Verification..."
          
          # éªŒè¯æ‰€æœ‰ç”Ÿæˆæ–‡ä»¶çš„å¢å¼ºå…ƒæ•°æ®
          python -c "
          import sys
          from pathlib import Path
          from src.framework.shared.metadata_injector_enhanced import verify_enhanced_metadata
          
          files_to_check = []
          files_to_check.extend(Path('docs').rglob('*.md'))
          files_to_check.extend(Path('specs').rglob('*.md'))
          files_to_check.extend(Path('data').rglob('*.md'))
          
          failed_files = []
          enhanced_files = []
          basic_files = []
          
          for file_path in files_to_check:
              if file_path.is_file():
                  result = verify_enhanced_metadata(str(file_path))
                  
                  if not result.get('valid'):
                      failed_files.append(f'{file_path}: {result.get(\"error\", \"Unknown error\")}')
                  elif result.get('langchain_enhanced'):
                      enhanced_files.append(str(file_path))
                  else:
                      basic_files.append(str(file_path))
          
          print(f'ğŸ“Š Metadata Verification Summary:')
          print(f'   Total files checked: {len(files_to_check)}')
          print(f'   Enhanced metadata files: {len(enhanced_files)}')
          print(f'   Basic metadata files: {len(basic_files)}')
          print(f'   Failed verification: {len(failed_files)}')
          
          if failed_files:
              print('\\nâŒ Failed files:')
              for file in failed_files:
                  print(f'   - {file}')
              sys.exit(1)
          
          if enhanced_files:
              print('\\nâœ… Enhanced metadata files found:')
              for file in enhanced_files:
                  print(f'   - {file}')
          
          print('\\nâœ… All metadata verification passed!')
          "
      
      - name: Check LangChain Features Usage
        run: |
          echo "ğŸ”§ Checking LangChain Features Usage..."
          
          python -c "
          import json
          from pathlib import Path
          
          # æ”¶é›†å¢å¼ºå…ƒæ•°æ®æ–‡ä»¶çš„ LangChain ç‰¹æ€§ä½¿ç”¨æƒ…å†µ
          enhanced_files = list(Path('docs').rglob('*.md')) + list(Path('specs').rglob('*.md'))
          
          total_files = 0
          langchain_features = {'summary': 0, 'quality': 0, 'issue_detection': 0}
          
          for file_path in enhanced_files:
              if file_path.is_file():
                  with open(file_path, 'r', encoding='utf-8') as f:
                      content = f.read()
                  
                  if '<!-- langchain-enhanced: true -->' in content:
                      total_files += 1
                      
                      # æå–å¤„ç†å…ƒæ•°æ®
                      import re
                      metadata_match = re.search(r'processing_metadata.*?content_length', content)
                      if metadata_match:
                          # ç®€å•ç»Ÿè®¡ç‰¹æ€§ä½¿ç”¨
                          if '<!-- content-summary:' in content:
                              langchain_features['summary'] += 1
                          if '<!-- quality-score:' in content:
                              langchain_features['quality'] += 1
                          if 'detected-issues:' in content and 'detected-issues: 0' not in content:
                              langchain_features['issue_detection'] += 1
          
          print(f'ğŸ“ˆ LangChain Features Usage:')
          print(f'   Total enhanced files: {total_files}')
          for feature, count in langchain_features.items():
              percentage = (count / max(total_files, 1)) * 100
              print(f'   {feature}: {count} files ({percentage:.1f}%)')
          "
  
  # å¢å¼ºçš„æ–‡æ¡£åˆ†ç±»éªŒè¯
  enhanced-doc-classification:
    name: ğŸ“‹ Enhanced Document Classification Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-version: latest
          activate-environment: ${{ env.CONDA_ENV_NAME }}
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        shell: bash -l {0}
        run: |
          pip install -r requirements.txt
          pip install langchain==1.0.* langchain-agents pydantic
      
      - name: Enhanced Document Classification Check
        run: |
          echo "ğŸ“‹ Running Enhanced Document Classification Check..."
          
          python -c "
          import json
          from pathlib import Path
          
          # æ£€æŸ¥æ–‡æ¡£åˆ†ç±»çš„ä¸€è‡´æ€§
          classification_stats = {'agent': 0, 'developer': 0, 'external': 0}
          token_issues = []
          
          docs_files = list(Path('docs').rglob('*.md')) + list(Path('specs').rglob('*.md'))
          
          for file_path in docs_files:
              if file_path.is_file():
                  with open(file_path, 'r', encoding='utf-8') as f:
                      content = f.read()
                  
                  # æ£€æŸ¥æ˜¯å¦åŒ…å«å¢å¼ºå…ƒæ•°æ®
                  if '<!-- content-summary:' in content:
                      # æå–åˆ†ç±»ä¿¡æ¯
                      if 'agent' in content.lower() and 'prompt' in content.lower():
                          classification_stats['agent'] += 1
                      elif 'api' in content.lower() or 'interface' in content.lower():
                          classification_stats['developer'] += 1
                      elif 'user' in content.lower() or 'guide' in content.lower():
                          classification_stats['external'] += 1
                  
                  # æ£€æŸ¥ Token é™åˆ¶
                  if '<!-- quality-score:' in content:
                      # ç®€å•çš„ Token æ£€æŸ¥
                      content_length = len(content)
                      if content_length > 50000:  # ç²—ç•¥ä¼°è®¡
                          token_issues.append(f'{file_path}: å¯èƒ½è¶…å‡º Token é™åˆ¶')
          
          print(f'ğŸ“Š Document Classification Summary:')
          for doc_type, count in classification_stats.items():
              print(f'   {doc_type}: {count} files')
          
          if token_issues:
              print('\\nâš ï¸ Token Issues Found:')
              for issue in token_issues:
                  print(f'   - {issue}')
              print('\\nğŸ’¡ Recommendation: Consider splitting large documents or optimizing content.')
          
          print('\\nâœ… Enhanced document classification check completed!')
          "
  
  # å¢å¼ºçš„ OpenAPI å¥‘çº¦æµ‹è¯•
  enhanced-openapi-contract:
    name: ğŸ“œ Enhanced OpenAPI Contract Test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-version: latest
          activate-environment: ${{ env.CONDA_ENV_NAME }}
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        shell: bash -l {0}
        run: |
          pip install -r requirements.txt
          pip install langchain==1.0.* langchain-agents pydantic
      
      - name: Enhanced OpenAPI Contract Test
        run: |
          echo "ğŸ“œ Running Enhanced OpenAPI Contract Test..."
          
          python -c "
          import json
          import yaml
          from pathlib import Path
          from src.framework.shared.metadata_injector_enhanced import EnhancedMetadataGenerator
          
          # è¿è¡ŒåŸºç¡€å¥‘çº¦æµ‹è¯•
          try:
              from src.tests.contract.test_openapi_contract import validate_openapi_contract
              result = validate_openapi_contract()
              
              print(f'ğŸ“Š Basic Contract Test Result:')
              print(f'   Status: {\"âœ… PASSED\" if result[\"status\"] == \"passed\" else \"âŒ FAILED\"}')
              print(f'   Total endpoints: {result.get(\"total_endpoints\", 0)}')
              print(f'   Valid endpoints: {result.get(\"valid_endpoints\", 0)}')
              print(f'   Failed endpoints: {result.get(\"failed_endpoints\", 0)}')
              
              if result.get('differences'):
                  print('\\nğŸ” Contract Differences:')
                  for diff in result['differences'][:5]:  # æ˜¾ç¤ºå‰5ä¸ªå·®å¼‚
                      print(f'   - {diff}')
                  if len(result['differences']) > 5:
                      print(f'   ... and {len(result[\"differences\"]) - 5} more')
              
          except Exception as e:
              print(f'âŒ Contract test failed: {e}')
          
          # LangChain å¢å¼ºåˆ†æ
          generator = EnhancedMetadataGenerator()
          if generator.summary_agent:
              print('\\nğŸ¤– LangChain Enhanced Analysis:')
              print('   âœ… Summary generation agent available')
          if generator.quality_agent:
              print('   âœ… Quality assessment agent available')
          if generator.issue_detector_agent:
              print('   âœ… Issue detection agent available')
          
          print('\\nâœ… Enhanced OpenAPI contract test completed!')
          "
  
  # å¢å¼ºçš„æŒ‡æ ‡æ”¶é›†
  enhanced-metrics-collection:
    name: ğŸ“ˆ Enhanced Metrics Collection
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-version: latest
          activate-environment: ${{ env.CONDA_ENV_NAME }}
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        shell: bash -l {0}
        run: |
          pip install -r requirements.txt
          pip install langchain==1.0.* langchain-agents pydantic
      
      - name: Enhanced Metrics Collection
        run: |
          echo "ğŸ“ˆ Running Enhanced Metrics Collection..."
          
          python -c "
          import json
          import hashlib
          from datetime import datetime
          from pathlib import Path
          
          # åŸºç¡€æŒ‡æ ‡æ”¶é›†
          try:
              from src.cli.metrics_collector import collect_metrics
              metrics = collect_metrics()
              
              print('ğŸ“Š Basic Metrics:')
              print(f'   Total files analyzed: {metrics.get(\"total_files\", 0)}')
              print(f'   Documents processed: {metrics.get(\"documents_processed\", 0)}')
              print(f'   Compliance checks: {metrics.get(\"compliance_checks\", 0)}')
              print(f'   Average processing time: {metrics.get(\"avg_processing_time\", 0):.2f}s')
              
          except Exception as e:
              print(f'âš ï¸ Basic metrics collection failed: {e}')
              metrics = {}
          
          # LangChain å¢å¼ºæŒ‡æ ‡
          print('\\nğŸ¤– LangChain Enhancement Metrics:')
          
          # ç»Ÿè®¡å¢å¼ºå…ƒæ•°æ®æ–‡ä»¶
          enhanced_files = []
          for pattern in ['docs/**/*.md', 'specs/**/*.md', 'data/**/*.md']:
              enhanced_files.extend(Path('.').rglob(pattern))
          
          enhanced_count = 0
          total_content_length = 0
          
          for file_path in enhanced_files:
              if file_path.is_file():
                  with open(file_path, 'r', encoding='utf-8') as f:
                      content = f.read()
                  
                  if '<!-- langchain-enhanced: true -->' in content:
                      enhanced_count += 1
                      total_content_length += len(content)
          
          print(f'   Enhanced files: {enhanced_count}')
          print(f'   Total enhanced content length: {total_content_length:,} characters')
          
          # æ–‡æ¡£è´¨é‡æŒ‡æ ‡
          quality_files = list(Path('docs').rglob('*.md')) + list(Path('specs').rglob('*.md'))
          total_quality_score = 0
          quality_file_count = 0
          
          for file_path in quality_files:
              if file_path.is_file():
                  with open(file_path, 'r', encoding='utf-8') as f:
                      content = f.read()
                  
                  if '<!-- quality-score:' in content:
                      import re
                      score_match = re.search(r'<!-- quality-score: (\d+) -->', content)
                      if score_match:
                          score = int(score_match.group(1))
                          total_quality_score += score
                          quality_file_count += 1
          
          avg_quality_score = total_quality_score / max(quality_file_count, 1)
          print(f'   Average quality score: {avg_quality_score:.1f}/100')
          print(f'   Quality-assessed files: {quality_file_count}')
          
          # ç”Ÿæˆå¢å¼ºçš„æŒ‡æ ‡æŠ¥å‘Š
          enhanced_metrics = {
              'timestamp': datetime.now().isoformat(),
              'enhancement_version': '2.0.0 (LangChain Enhanced)',
              'enhanced_files_count': enhanced_count,
              'total_content_length': total_content_length,
              'average_quality_score': avg_quality_score,
              'quality_assessed_files': quality_file_count,
              'langchain_features': {
                  'summary_generation': enhanced_count > 0,
                  'quality_assessment': quality_file_count > 0,
                  'issue_detection': True,  # å‡è®¾å·²å¯ç”¨
                  'metadata_enhancement': enhanced_count > 0
              },
              'basic_metrics': metrics
          }
          
          # ä¿å­˜å¢å¼ºæŒ‡æ ‡
          metrics_dir = Path('docs/internal')
          metrics_dir.mkdir(parents=True, exist_ok=True)
          
          metrics_file = metrics_dir / f'enhanced_metrics_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'
          with open(metrics_file, 'w', encoding='utf-8') as f:
              json.dump(enhanced_metrics, f, indent=2, ensure_ascii=False)
          
          print(f'\\nğŸ’¾ Enhanced metrics saved to: {metrics_file}')
          print('\\nâœ… Enhanced metrics collection completed!')
          "
  
  # å¢å¼ºçš„åˆè§„æ£€æŸ¥
  enhanced-compliance-check:
    name: ğŸ”’ Enhanced Compliance Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-version: latest
          activate-environment: ${{ env.CONDA_ENV_NAME }}
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        shell: bash -l {0}
        run: |
          pip install -r requirements.txt
          pip install langchain==1.0.* langchain-agents pydantic
      
      - name: Enhanced Compliance Check
        run: |
          echo "ğŸ”’ Running Enhanced Compliance Check..."
          
          python -c "
          import json
          from pathlib import Path
          
          # æ£€æŸ¥åˆè§„æ€§è¦æ±‚
          compliance_issues = []
          compliance_passed = []
          
          # 1. æ£€æŸ¥ç”Ÿæˆæ–‡ä»¶å¿…é¡»æœ‰å…ƒæ•°æ®
          generated_patterns = ['docs/**/*.md', 'specs/**/*.md', 'data/**/*.md']
          for pattern in generated_patterns:
              files = list(Path('.').rglob(pattern))
              
              for file_path in files:
                  if file_path.is_file():
                      with open(file_path, 'r', encoding='utf-8') as f:
                          content = f.read()
                      
                      if '<!-- generated:' not in content:
                          compliance_issues.append(f'{file_path}: ç¼ºå°‘ç”Ÿæˆå…ƒæ•°æ®')
                      else:
                          compliance_passed.append(f'{file_path}: å…ƒæ•°æ®æ£€æŸ¥é€šè¿‡')
          
          # 2. æ£€æŸ¥å¢å¼ºå…ƒæ•°æ®è´¨é‡
          enhanced_files = list(Path('docs').rglob('*.md')) + list(Path('specs').rglob('*.md'))
          for file_path in enhanced_files:
              if file_path.is_file():
                  with open(file_path, 'r', encoding='utf-8') as f:
                      content = f.read()
                  
                  if '<!-- langchain-enhanced: true -->' in content:
                      # æ£€æŸ¥å¢å¼ºå­—æ®µå®Œæ•´æ€§
                      required_fields = ['content-summary', 'content-type', 'quality-score']
                      missing_fields = []
                      
                      for field in required_fields:
                          if f'<!-- {field}:' not in content:
                              missing_fields.append(field)
                      
                      if missing_fields:
                          compliance_issues.append(f'{file_path}: å¢å¼ºå…ƒæ•°æ®ç¼ºå°‘å­—æ®µ: {missing_fields}')
                      else:
                          compliance_passed.append(f'{file_path}: å¢å¼ºå…ƒæ•°æ®å®Œæ•´')
          
          # 3. æ£€æŸ¥æ–‡æ¡£åˆ†ç±»ä¸€è‡´æ€§
          for file_path in enhanced_files:
              if file_path.is_file():
                  with open(file_path, 'r', encoding='utf-8') as f:
                      content = f.read()
                  
                  if '<!-- content-summary:' in content:
                      # æ£€æŸ¥åˆ†ç±»ä¸å†…å®¹çš„ä¸€è‡´æ€§
                      summary_line = re.search(r'<!-- content-summary: (.+?) -->', content)
                      if summary_line:
                          summary = summary_line.group(1)
                          
                          # ç®€å•çš„ä¸€è‡´æ€§æ£€æŸ¥
                          if 'agent' in file_path.name.lower() and 'agent' not in summary.lower():
                              compliance_issues.append(f'{file_path}: æ–‡ä»¶åä¸å†…å®¹æ‘˜è¦ä¸ä¸€è‡´')
                          elif 'spec' in file_path.name.lower() and 'specification' not in summary.lower():
                              compliance_issues.append(f'{file_path}: æ–‡ä»¶åä¸å†…å®¹æ‘˜è¦ä¸ä¸€è‡´')
          
          # è¾“å‡ºç»“æœ
          print('ğŸ“Š Compliance Check Summary:')
          print(f'   Passed checks: {len(compliance_passed)}')
          print(f'   Failed checks: {len(compliance_issues)}')
          
          if compliance_passed:
              print('\\nâœ… Passed Checks:')
              for check in compliance_passed[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                  print(f'   - {check}')
              if len(compliance_passed) > 10:
                  print(f'   ... and {len(compliance_passed) - 10} more')
          
          if compliance_issues:
              print('\\nâŒ Failed Checks:')
              for issue in compliance_issues[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                  print(f'   - {issue}')
              if len(compliance_issues) > 10:
                  print(f'   ... and {len(compliance_issues) - 10} more')
              
              print('\\nğŸ’¥ Compliance check failed! Please fix the issues above.')
              exit(1)
          
          print('\\nâœ… All compliance checks passed!')
          "
  
  # æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ
  enhanced-report-generation:
    name: ğŸ“‹ Enhanced Report Generation
    runs-on: ubuntu-latest
    needs: [enhanced-metadata-verification, enhanced-doc-classification, enhanced-openapi-contract, enhanced-metrics-collection, enhanced-compliance-check]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniforge-version: latest
          activate-environment: ${{ env.CONDA_ENV_NAME }}
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Enhanced Report Generation
        run: |
          echo "ğŸ“‹ Generating Enhanced CI Report..."
          
          python -c "
          import json
          from datetime import datetime
          from pathlib import Path
          
          # æ”¶é›†æ‰€æœ‰ä½œä¸šç»“æœ
          jobs_status = {
              'enhanced-metadata-verification': '${{ needs.enhanced-metadata-verification.result }}',
              'enhanced-doc-classification': '${{ needs.enhanced-doc-classification.result }}',
              'enhanced-openapi-contract': '${{ needs.enhanced-openapi-contract.result }}',
              'enhanced-metrics-collection': '${{ needs.enhanced-metrics-collection.result }}',
              'enhanced-compliance-check': '${{ needs.enhanced-compliance-check.result }}'
          }
          
          # ç”Ÿæˆå¢å¼ºçš„ CI æŠ¥å‘Š
          enhanced_report = {
              'report_type': 'enhanced_ci_report',
              'timestamp': datetime.now().isoformat(),
              'git_info': {
                  'commit': '${{ github.sha }}',
                  'branch': '${{ github.ref_name }}',
                  'author': '${{ github.actor }}'
              },
              'jobs_status': jobs_status,
              'enhancement_features': {
                  'langchain_integration': True,
                  'enhanced_metadata': True,
                  'automated_quality_check': True,
                  'compliance_validation': True,
                  'metrics_collection': True
              },
              'summary': {
                  'total_jobs': len(jobs_status),
                  'successful_jobs': len([status for status in jobs_status.values() if status == 'success']),
                  'failed_jobs': len([status for status in jobs_status.values() if status == 'failure']),
                  'enhanced_features_used': sum(jobs_status.values()).count('success')
              },
              'recommendations': []
          }
          
          # æ·»åŠ å»ºè®®
          if any(status == 'failure' for status in jobs_status.values()):
              enhanced_report['recommendations'].append('ä¿®å¤å¤±è´¥çš„ CI æ£€æŸ¥')
          
          enhanced_report['recommendations'].extend([
              'ç»§ç»­ä½¿ç”¨ LangChain 1.0 å¢å¼ºåŠŸèƒ½',
              'å®šæœŸæ›´æ–°å¢å¼ºå…ƒæ•°æ®',
              'ç›‘æ§æ–‡æ¡£è´¨é‡æŒ‡æ ‡'
          ])
          
          # ä¿å­˜æŠ¥å‘Š
          reports_dir = Path('docs/internal')
          reports_dir.mkdir(parents=True, exist_ok=True)
          
          report_file = reports_dir / f'enhanced_ci_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'
          with open(report_file, 'w', encoding='utf-8') as f:
              json.dump(enhanced_report, f, indent=2, ensure_ascii=False)
          
          # ç”Ÿæˆ Markdown æ‘˜è¦
          markdown_summary = f'''# Enhanced CI Report

**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Commit**: ${{{{ github.sha }}}}
**Branch**: ${{{{ github.ref_name }}}}
**Author**: ${{{{ github.actor }}}}

## Job Status

| Job | Status |
|-----|--------|
| Enhanced Metadata Verification | {'âœ… Success' if jobs_status['enhanced-metadata-verification'] == 'success' else 'âŒ Failed'} |
| Enhanced Document Classification | {'âœ… Success' if jobs_status['enhanced-doc-classification'] == 'success' else 'âŒ Failed'} |
| Enhanced OpenAPI Contract Test | {'âœ… Success' if jobs_status['enhanced-openapi-contract'] == 'success' else 'âŒ Failed'} |
| Enhanced Metrics Collection | {'âœ… Success' if jobs_status['enhanced-metrics-collection'] == 'success' else 'âŒ Failed'} |
| Enhanced Compliance Check | {'âœ… Success' if jobs_status['enhanced-compliance-check'] == 'success' else 'âŒ Failed'} |

## Summary

- **Total Jobs**: {enhanced_report['summary']['total_jobs']}
- **Successful Jobs**: {enhanced_report['summary']['successful_jobs']}
- **Failed Jobs**: {enhanced_report['summary']['failed_jobs']}
- **Enhanced Features Used**: {enhanced_report['summary']['enhanced_features_used']}

## Enhancement Features

âœ… LangChain 1.0 Integration
âœ… Enhanced Metadata Generation
âœ… Automated Quality Check
âœ… Compliance Validation
âœ… Metrics Collection

## Recommendations

'''
          
          for rec in enhanced_report['recommendations']:
              markdown_summary += f'- {rec}\\n'
          
          markdown_file = reports_dir / f'enhanced_ci_summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.md'
          with open(markdown_file, 'w', encoding='utf-8') as f:
              f.write(markdown_summary)
          
          print(f'ğŸ“Š Enhanced CI report saved to: {report_file}')
          print(f'ğŸ“ Markdown summary saved to: {markdown_file}')
          print('\\nâœ… Enhanced CI report generation completed!')
          "