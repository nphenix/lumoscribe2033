"""
Metrics ç›¸å…³çš„ Arq ä»»åŠ¡

é›†æˆ LangChain 1.0 å’Œ OpenTelemetry æœ€ä½³å®è·µçš„æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿ
"""

import asyncio
import time
from datetime import datetime
from typing import Any

from src.framework.shared.logging import get_logger
from src.framework.shared.monitoring import get_enhanced_metrics_collector
from src.framework.shared.redis_cache import get_cache_manager

logger = get_logger(__name__)


async def collect_comprehensive_metrics(
    ctx: dict[str, Any],
    request_data: dict[str, Any],
) -> dict[str, Any]:
    """
    æ”¶é›†ç»¼åˆç³»ç»ŸæŒ‡æ ‡ä»»åŠ¡

    Args:
        ctx: Arq ä¸Šä¸‹æ–‡
        request_data: è¯·æ±‚æ•°æ®

    Returns:
        ä»»åŠ¡æ‰§è¡Œç»“æœ
    """
    start_time = time.time()

    try:
        # è·å–å¢å¼ºæŒ‡æ ‡æ”¶é›†å™¨
        metrics_collector = await get_enhanced_metrics_collector()
        cache_manager = await get_cache_manager()

        # æ”¶é›†ç»¼åˆæŒ‡æ ‡
        comprehensive_metrics = await metrics_collector.collect_comprehensive_metrics()

        # æ”¶é›†ç¼“å­˜å¥åº·çŠ¶æ€
        cache_health = {}
        if cache_manager:
            cache_health = await cache_manager.health_check()

        # æ”¶é›†ç³»ç»Ÿèµ„æºæŒ‡æ ‡
        system_metrics = _collect_system_resources()

        # æ”¶é›†åº”ç”¨æ€§èƒ½æŒ‡æ ‡
        app_metrics = _collect_application_performance()

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "execution_time": time.time() - start_time,
            "metrics": {
                "comprehensive": comprehensive_metrics,
                "cache_health": cache_health,
                "system_resources": system_metrics,
                "application_performance": app_metrics,
                "langchain_integration": _get_langchain_metrics(),
                "opentelemetry_status": _get_opentelemetry_status()
            },
            "message": "ç»¼åˆæŒ‡æ ‡æ”¶é›†å®Œæˆ"
        }

        # ç¼“å­˜æŠ¥å‘Šç»“æœ
        if cache_manager:
            await cache_manager.set(
                f"metrics_report_{int(time.time())}",
                report,
                ttl=1800  # 30åˆ†é’Ÿç¼“å­˜
            )

        logger.info("ğŸ“Š ç»¼åˆæŒ‡æ ‡æ”¶é›†å®Œæˆ")
        return report

    except Exception as e:
        logger.error(f"âŒ ç»¼åˆæŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "execution_time": time.time() - start_time,
            "message": "æŒ‡æ ‡æ”¶é›†å¤±è´¥"
        }


async def collect_real_time_metrics(
    ctx: dict[str, Any],
    request_data: dict[str, Any],
) -> dict[str, Any]:
    """
    å®æ—¶æŒ‡æ ‡æ”¶é›†ä»»åŠ¡

    Args:
        ctx: Arq ä¸Šä¸‹æ–‡
        request_data: è¯·æ±‚æ•°æ®

    Returns:
        å®æ—¶æŒ‡æ ‡æ•°æ®
    """
    start_time = time.time()

    try:
        # metrics_collector = await get_enhanced_metrics_collector()
        # cache_manager = await get_cache_manager()

        # è·å–å®æ—¶æŒ‡æ ‡
        real_time_metrics = {
            "timestamp": datetime.now().isoformat(),
            "system_load": _get_current_system_load(),
            "cache_performance": _get_cache_performance(),
            "application_health": _get_application_health(),
            "active_alerts": _get_active_alerts()
        }

        return {
            "success": True,
            "metrics": real_time_metrics,
            "execution_time": time.time() - start_time,
            "message": "å®æ—¶æŒ‡æ ‡æ”¶é›†å®Œæˆ"
        }

    except Exception as e:
        logger.error(f"âŒ å®æ—¶æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "execution_time": time.time() - start_time,
            "message": "å®æ—¶æŒ‡æ ‡æ”¶é›†å¤±è´¥"
        }


async def generate_performance_report(
    ctx: dict[str, Any],
    request_data: dict[str, Any],
) -> dict[str, Any]:
    """
    ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šä»»åŠ¡

    Args:
        ctx: Arq ä¸Šä¸‹æ–‡
        request_data: è¯·æ±‚æ•°æ®

    Returns:
        æ€§èƒ½æŠ¥å‘Š
    """
    start_time = time.time()

    try:
        metrics_collector = await get_enhanced_metrics_collector()

        # è·å–å†å²æ•°æ®
        alert_history = metrics_collector.get_alert_history(hours=24)
        performance_trends = metrics_collector._calculate_performance_trends()
        resource_utilization = metrics_collector._calculate_resource_utilization()

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        report = {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "execution_time": time.time() - start_time,
            "report": {
                "summary": {
                    "period": "24å°æ—¶",
                    "total_alerts": len(alert_history),
                    "critical_alerts": len([a for a in alert_history if a.get("level") == "critical"]),
                    "warning_alerts": len([a for a in alert_history if a.get("level") == "warning"]),
                    "overall_health": "healthy" if len(alert_history) == 0 else "degraded"
                },
                "performance_trends": performance_trends,
                "resource_utilization": resource_utilization,
                "recommendations": _generate_performance_recommendations(resource_utilization, alert_history)
            },
            "message": "æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå®Œæˆ"
        }

        logger.info("ğŸ“ˆ æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return report

    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "execution_time": time.time() - start_time,
            "message": "æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå¤±è´¥"
        }


def _collect_system_resources() -> dict[str, Any]:
    """æ”¶é›†ç³»ç»Ÿèµ„æºæŒ‡æ ‡"""
    try:
        import psutil

        # CPU ä¿¡æ¯
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_count = psutil.cpu_count()
        cpu_freq = psutil.cpu_freq()

        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        swap = psutil.swap_memory()

        # ç£ç›˜ä¿¡æ¯
        disk = psutil.disk_usage('/')
        disk_io = psutil.disk_io_counters()

        # ç½‘ç»œä¿¡æ¯
        network = psutil.net_io_counters()

        # è¿›ç¨‹ä¿¡æ¯
        processes = len(psutil.pids())

        return {
            "cpu": {
                "usage_percent": cpu_percent,
                "count": cpu_count,
                "frequency_current": cpu_freq.current if cpu_freq else 0,
                "frequency_min": cpu_freq.min if cpu_freq else 0,
                "frequency_max": cpu_freq.max if cpu_freq else 0
            },
            "memory": {
                "total": memory.total,
                "available": memory.available,
                "used": memory.used,
                "percent": memory.percent,
                "swap_total": swap.total,
                "swap_used": swap.used,
                "swap_percent": swap.percent
            },
            "disk": {
                "total": disk.total,
                "used": disk.used,
                "free": disk.free,
                "percent": disk.percent,
                "read_bytes": disk_io.read_bytes if disk_io else 0,
                "write_bytes": disk_io.write_bytes if disk_io else 0
            },
            "network": {
                "bytes_sent": network.bytes_sent if network else 0,
                "bytes_recv": network.bytes_recv if network else 0,
                "packets_sent": network.packets_sent if network else 0,
                "packets_recv": network.packets_recv if network else 0
            },
            "processes": {
                "count": processes,
                "running": len([p for p in psutil.process_iter() if p.status() == 'running'])
            }
        }

    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿèµ„æºæ”¶é›†å¤±è´¥: {e}")
        return {}


def _collect_application_performance() -> dict[str, Any]:
    """æ”¶é›†åº”ç”¨æ€§èƒ½æŒ‡æ ‡"""
    try:
        from src.framework.shared.monitoring import metrics_collector

        # è·å–ä»»åŠ¡æŒ‡æ ‡
        task_summary = metrics_collector.get_task_summary(hours=1)

        # è·å– API æŒ‡æ ‡
        api_summary = metrics_collector.get_api_summary(hours=1)

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_requests = sum(
            summary.get("total_requests", 0)
            for summary in api_summary.values()
        )
        successful_requests = sum(
            summary.get("success_rate", 0) * summary.get("total_requests", 0) / 100
            for summary in api_summary.values()
        )
        return {
            "requests": {
                "total": total_requests,
                "successful": successful_requests,
                "failed": total_requests - successful_requests,
                "success_rate": (successful_requests / total_requests * 100) if total_requests > 0 else 0
            },
            "response_time": {
                "avg": sum(
                    summary.get("avg_response_time", 0)
                    for summary in api_summary.values()
                ) / max(len(api_summary), 1),
                "min": min(
                    summary.get("min_response_time", 0)
                    for summary in api_summary.values()
                ) if any(summary.get("min_response_time", 0) for summary in api_summary.values()) else 0,
                "max": max(
                    summary.get("max_response_time", 0)
                    for summary in api_summary.values()
                ) if any(summary.get("max_response_time", 0) for summary in api_summary.values()) else 0
            },
            "tasks": {
                "total": sum(
                    summary.get("total_count", 0)
                    for summary in task_summary.values()
                ),
                "successful": sum(
                    summary.get("success_count", 0)
                    for summary in task_summary.values()
                ),
                "failed": sum(
                    summary.get("failed_count", 0)
                    for summary in task_summary.values()
                )
            }
        }


    except Exception as e:
        logger.error(f"âŒ åº”ç”¨æ€§èƒ½æ”¶é›†å¤±è´¥: {e}")
        return {}


def _get_langchain_metrics() -> dict[str, Any]:
    """è·å– LangChain é›†æˆæŒ‡æ ‡"""
    try:
        from src.framework.orchestrators.langchain_runner import get_global_runner

        runner = get_global_runner()
        if not runner:
            return {"status": "not_initialized"}

        # è·å–è·¯ç”±ç»Ÿè®¡
        routing_stats = getattr(runner, 'routing_stats', {})

        # è·å–æ¨¡å‹å¥åº·çŠ¶æ€
        health_stats = getattr(runner, 'health_stats', {})

        return {
            "status": "active",
            "routing": {
                "total_requests": routing_stats.get("total_requests", 0),
                "successful_routings": routing_stats.get("successful_routings", 0),
                "fallback_count": routing_stats.get("fallback_count", 0),
                "avg_routing_time": routing_stats.get("avg_routing_time", 0)
            },
            "health_checks": {
                "total_checks": health_stats.get("total_checks", 0),
                "healthy_models": health_stats.get("healthy_models", 0),
                "unhealthy_models": health_stats.get("unhealthy_models", 0)
            }
        }

    except Exception as e:
        logger.error(f"âŒ LangChain æŒ‡æ ‡è·å–å¤±è´¥: {e}")
        return {"status": "error", "error": str(e)}


def _get_opentelemetry_status() -> dict[str, Any]:
    """è·å– OpenTelemetry çŠ¶æ€"""
    try:
        from src.framework.shared.telemetry import get_telemetry_metrics

        # telemetry_metrics = get_telemetry_metrics()

        return {
            "status": "active",
            "tracing_enabled": True,
            "metrics_enabled": True,
            "exporters": {
                "span_exporter": "console",
                "metric_exporter": "console"
            },
            "instrumentation": {
                "auto_instrumentation": True,
                "libraries": ["fastapi", "requests", "sqlite3"]
            }
        }

    except Exception as e:
        logger.error(f"âŒ OpenTelemetry çŠ¶æ€è·å–å¤±è´¥: {e}")
        return {"status": "error", "error": str(e)}


def _get_current_system_load() -> dict[str, float]:
    """è·å–å½“å‰ç³»ç»Ÿè´Ÿè½½"""
    try:
        import psutil

        # 1åˆ†é’Ÿå¹³å‡è´Ÿè½½
        load_avg = psutil.getloadavg()

        return {
            "load_1min": load_avg[0] if len(load_avg) > 0 else 0,
            "load_5min": load_avg[1] if len(load_avg) > 1 else 0,
            "load_15min": load_avg[2] if len(load_avg) > 2 else 0
        }

    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿè´Ÿè½½è·å–å¤±è´¥: {e}")
        return {}


def _get_cache_performance() -> dict[str, Any]:
    """è·å–ç¼“å­˜æ€§èƒ½æŒ‡æ ‡"""
    try:
        cache_manager = asyncio.run(get_cache_manager())

        if not cache_manager:
            return {"status": "not_available"}

        cache_metrics = cache_manager.get_metrics()
        cache_health = asyncio.run(cache_manager.health_check())

        return {
            "status": "active",
            "hit_rate": cache_metrics.get("cache_metrics", {}).get("hit_rate", 0),
            "operations_per_second": cache_metrics.get("operations", {}).get("per_second", 0),
            "local_cache_utilization": cache_metrics.get("local_cache", {}).get("utilization", 0),
            "active_locks": cache_metrics.get("locks", {}).get("active_count", 0),
            "health_status": cache_health.get("status", "unknown")
        }

    except Exception as e:
        logger.error(f"âŒ ç¼“å­˜æ€§èƒ½è·å–å¤±è´¥: {e}")
        return {"status": "error", "error": str(e)}


def _get_application_health() -> dict[str, Any]:
    """è·å–åº”ç”¨å¥åº·çŠ¶æ€"""
    try:
        from src.framework.shared.monitoring import metrics_collector

        health_status = metrics_collector.get_health_status()

        return {
            "overall": health_status.get("overall_health", "unknown"),
            "system": health_status.get("system_health", "unknown"),
            "tasks": health_status.get("task_health", "unknown"),
            "timestamp": health_status.get("timestamp", datetime.now().isoformat())
        }

    except Exception as e:
        logger.error(f"âŒ åº”ç”¨å¥åº·çŠ¶æ€è·å–å¤±è´¥: {e}")
        return {"overall": "error", "error": str(e)}


def _get_active_alerts() -> list[dict[str, Any]]:
    """è·å–æ´»è·ƒè­¦æŠ¥"""
    try:
        from src.framework.shared.monitoring import get_enhanced_metrics_collector

        metrics_collector = asyncio.run(get_enhanced_metrics_collector())
        alert_history = metrics_collector.get_alert_history(hours=1)

        # åªè¿”å›æœ€è¿‘1å°æ—¶çš„è­¦æŠ¥
        return alert_history

    except Exception as e:
        logger.error(f"âŒ æ´»è·ƒè­¦æŠ¥è·å–å¤±è´¥: {e}")
        return []


def _generate_performance_recommendations(
    resource_utilization: dict[str, Any],
    alert_history: list[dict[str, Any]]
) -> list[str]:
    """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
    recommendations = []

    try:
        # CPU ä½¿ç”¨ç‡å»ºè®®
        cpu_util = resource_utilization.get("cpu", {})
        if cpu_util.get("level") in ["warning", "critical"]:
            recommendations.append("è€ƒè™‘ä¼˜åŒ–CPUå¯†é›†å‹ä»»åŠ¡æˆ–å¢åŠ CPUèµ„æº")
            recommendations.append("æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸è¿›ç¨‹æ¶ˆè€—CPUèµ„æº")

        # å†…å­˜ä½¿ç”¨ç‡å»ºè®®
        memory_util = resource_utilization.get("memory", {})
        if memory_util.get("level") in ["warning", "critical"]:
            recommendations.append("ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œæ£€æŸ¥å†…å­˜æ³„æ¼")
            recommendations.append("è€ƒè™‘å¢åŠ ç‰©ç†å†…å­˜æˆ–ä¼˜åŒ–åº”ç”¨å†…å­˜ç®¡ç†")

        # è­¦æŠ¥é¢‘ç‡å»ºè®®
        critical_alerts = [a for a in alert_history if a.get("level") == "critical"]
        if len(critical_alerts) > 5:  # 24å°æ—¶å†…è¶…è¿‡5ä¸ªä¸¥é‡è­¦æŠ¥
            recommendations.append("ç³»ç»Ÿå­˜åœ¨ä¸¥é‡æ€§èƒ½é—®é¢˜ï¼Œå»ºè®®ç«‹å³æ£€æŸ¥")

        # ç¼“å­˜æ€§èƒ½å»ºè®®
        if len([a for a in alert_history if a.get("type") == "cache"]) > 3:
            recommendations.append("ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼Œè€ƒè™‘è°ƒæ•´ç¼“å­˜å¤§å°æˆ–TTLè®¾ç½®")

        return recommendations if recommendations else ["ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œæ— ç‰¹æ®Šå»ºè®®"]

    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
        return ["æ— æ³•ç”Ÿæˆæ€§èƒ½å»ºè®®"]


# ä¿æŒå‘åå…¼å®¹çš„åŸå§‹å‡½æ•°
async def collect_metrics(
    ctx: dict[str, Any],
    request_data: dict[str, Any],
) -> dict[str, Any]:
    """
    æ”¶é›†ç³»ç»ŸæŒ‡æ ‡ä»»åŠ¡ï¼ˆå‘åå…¼å®¹ç‰ˆæœ¬ï¼‰
    """
    return await collect_comprehensive_metrics(ctx, request_data)
