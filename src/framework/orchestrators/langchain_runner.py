"""
LangChain 1.0 RouterChain + RunnableSequence å¤šæ¨¡å‹è·¯ç”±æ‰§è¡Œå™¨

åŸºäº LangChain 1.0 çš„ RouterChain å’Œ RunnableSequence å®ç°æ™ºèƒ½å¤šæ¨¡å‹è·¯ç”±ï¼Œ
æ”¯æŒåŠ¨æ€æ¨¡å‹é€‰æ‹©ã€é“¾å¼æ‰§è¡Œå’Œæ€§èƒ½ç›‘æ§ã€‚
"""

import asyncio
import time
from collections.abc import Sequence
from dataclasses import dataclass
from functools import lru_cache
from typing import Annotated, Any, Literal, Optional, TypedDict, Union

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable, RunnableLambda, RunnableSequence
from pydantic import BaseModel, Field

from ..shared.config import config_manager
from ..shared.logging import get_logger
from ..shared.telemetry import trace_method

logger = get_logger(__name__)


class RouteDecision(BaseModel):
    """è·¯ç”±å†³ç­–çš„ç»“æ„åŒ–è¾“å‡º"""
    model_name: str = Field(
        ...,
        description="é€‰æ‹©çš„æ¨¡å‹åç§°",
        json_schema_extra={"example": "openai-gpt4"}
    )
    reason: str = Field(
        ...,
        description="é€‰æ‹©è¯¥æ¨¡å‹çš„åŸå› ",
        json_schema_extra={"example": "å¤æ‚æ¨ç†ä»»åŠ¡éœ€è¦é«˜è´¨é‡æ¨¡å‹"}
    )
    confidence: float = Field(
        ...,
        description="è·¯ç”±å†³ç­–çš„ç½®ä¿¡åº¦ (0-1)",
        json_schema_extra={"example": 0.95}
    )


class ExecutionState(TypedDict):
    """æ‰§è¡ŒçŠ¶æ€"""
    input: str
    route_decision: dict[str, Any] | None
    selected_model: str | None
    response: str | None
    execution_time: float | None
    success: bool
    error_message: str | None
    usage_metadata: dict[str, Any] | None
    performance_metrics: dict[str, Any] | None


class ModelConfig(TypedDict):
    """æ¨¡å‹é…ç½®"""
    name: str
    model: BaseChatModel
    cost_per_token: float
    avg_response_time: float
    success_rate: float
    capabilities: list[str]


class LangChainRunner:
    """LangChain 1.0 å¤šæ¨¡å‹è·¯ç”±æ‰§è¡Œå™¨"""

    def __init__(self, models: dict[str, BaseChatModel] | None = None):
        """
        åˆå§‹åŒ– LangChain æ‰§è¡Œå™¨

        Args:
            models: å¯é€‰çš„æ¨¡å‹å­—å…¸ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é…ç½®ç®¡ç†å™¨ä¸­çš„å¯ç”¨æ¨¡å‹
        """
        # å¦‚æœæä¾›äº†æ¨¡å‹ï¼Œä½¿ç”¨æä¾›çš„æ¨¡å‹ï¼›å¦åˆ™ä½¿ç”¨é…ç½®ç®¡ç†å™¨
        if models is not None:
            self.models = models
        else:
            # ä»é…ç½®ç®¡ç†å™¨è·å–å¯ç”¨çš„æ¨¡å‹é…ç½®ï¼Œä½†éœ€è¦å¤–éƒ¨æä¾›æ¨¡å‹å®ä¾‹
            enabled_configs = config_manager.get_enabled_models()
            self.models = {}  # éœ€è¦å¤–éƒ¨æ³¨å…¥æ¨¡å‹å®ä¾‹
            logger.info(f"LangChainRunner initialized with config from ConfigManager: {list(enabled_configs.keys())}")

        self.model_configs = self._create_model_configs()
        self.performance_metrics = self._initialize_metrics()
        self.routing_chain = self._create_routing_chain()
        self.execution_chain = self._create_execution_chain()

        # åˆå§‹åŒ–å¥åº·æ£€æŸ¥å™¨
        self.health_checker = None
        self._enable_health_checking = config_manager.get_setting("enable_health_checking", True)

        # åˆå§‹åŒ–é«˜çº§è·¯ç”±ç¼“å­˜ï¼ˆåŸºäºLangChainæœ€ä½³å®è·µï¼‰
        self._routing_cache = {}
        self._cache_stats = {"hits": 0, "misses": 0, "evictions": 0}
        self._cache_ttl = config_manager.get_setting("routing_cache_ttl", 300)  # 5åˆ†é’Ÿé»˜è®¤ç¼“å­˜
        self._cache_cleanup_interval = config_manager.get_setting("routing_cache_cleanup_interval", 3600)  # 1å°æ—¶æ¸…ç†
        self._cache_max_size = config_manager.get_setting("routing_cache_max_size", 1000)  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        self._last_cleanup_time = time.time()

        logger.info(f"LangChainRunner initialized with models: {list(self.models.keys())}")

    @classmethod
    def from_config_manager(cls, model_instances: dict[str, BaseChatModel]) -> 'LangChainRunner':
        """
        åŸºäºé…ç½®ç®¡ç†å™¨åˆ›å»º LangChainRunner

        Args:
            model_instances: æ¨¡å‹å®ä¾‹å­—å…¸ï¼Œkey ä¸ºæ¨¡å‹åç§°ï¼Œvalue ä¸ºæ¨¡å‹å®ä¾‹

        Returns:
            LangChainRunner å®ä¾‹
        """
        # éªŒè¯é…ç½®
        errors = config_manager.validate_config()
        if errors:
            logger.warning(f"Configuration validation errors: {errors}")

        # è·å–å¯ç”¨çš„æ¨¡å‹é…ç½®
        enabled_configs = config_manager.get_enabled_models()
        logger.info(f"Enabled models from config: {list(enabled_configs.keys())}")

        # è¿‡æ»¤å‡ºæœ‰å®ä¾‹çš„æ¨¡å‹
        available_models = {}
        for model_name, config in enabled_configs.items():
            if model_name in model_instances:
                available_models[model_name] = model_instances[model_name]
            else:
                logger.warning(f"Model {model_name} is enabled in config but no instance provided")

        if not available_models:
            raise ValueError("No models available - check configuration and model instances")

        return cls(available_models)

    def initialize_health_checker(self):
        """åˆå§‹åŒ–å¥åº·æ£€æŸ¥å™¨"""
        if not self._enable_health_checking:
            logger.info("Health checking is disabled")
            return

        if self.health_checker is None and self.models:
            from .health_checker import HealthCheckConfig, LangChainHealthChecker

            config = HealthCheckConfig(
                timeout=config_manager.get_setting("health_check_timeout", 30.0),
                max_retries=config_manager.get_setting("health_check_retries", 3),
                check_interval=config_manager.get_setting("health_check_interval", 60.0),
                failure_threshold=config_manager.get_setting("health_failure_threshold", 5),
                enable_auto_recovery=config_manager.get_setting("enable_auto_recovery", True),
                enable_tracing=config_manager.get_setting("enable_health_tracing", True)
            )

            self.health_checker = LangChainHealthChecker(self.models, config)
            logger.info("LangChain health checker initialized")

    async def start_health_monitoring(self):
        """å¯åŠ¨å¥åº·ç›‘æ§"""
        if self.health_checker:
            await self.health_checker.start_monitoring()
            logger.info("Health monitoring started")

    async def stop_health_monitoring(self):
        """åœæ­¢å¥åº·ç›‘æ§"""
        if self.health_checker:
            await self.health_checker.stop_monitoring()
            logger.info("Health monitoring stopped")

    def _create_model_configs(self) -> dict[str, ModelConfig]:
        """åˆ›å»ºæ¨¡å‹é…ç½®"""
        configs = {}
        for name, model in self.models.items():
            # ä»é…ç½®ç®¡ç†å™¨è·å–é…ç½®
            config_manager_model = config_manager.get_model_by_name(name)
            if config_manager_model:
                # ä½¿ç”¨é…ç½®ç®¡ç†å™¨ä¸­çš„é…ç½®
                configs[name] = ModelConfig(
                    name=name,
                    model=model,
                    cost_per_token=config_manager_model.cost_per_token,
                    avg_response_time=0.0,
                    success_rate=1.0,
                    capabilities=[cap.value for cap in config_manager_model.capabilities]
                )
            else:
                # ä½¿ç”¨é»˜è®¤é…ç½®
                configs[name] = ModelConfig(
                    name=name,
                    model=model,
                    cost_per_token=self._get_default_cost(name),
                    avg_response_time=0.0,
                    success_rate=1.0,
                    capabilities=self._get_model_capabilities(name)
                )
        return configs

    def _get_default_cost(self, model_name: str) -> float:
        """è·å–æ¨¡å‹çš„é»˜è®¤æˆæœ¬"""
        cost_mapping = {
            "openai-gpt4": 0.03,
            "openai-gpt35": 0.005,
            "ollama-llama2": 0.001,
            "ollama-mistral": 0.002,
            "claude-3-opus": 0.015,
            "claude-3-sonnet": 0.003
        }
        return cost_mapping.get(model_name, 0.01)

    def _get_model_capabilities(self, model_name: str) -> list[str]:
        """è·å–æ¨¡å‹èƒ½åŠ›"""
        capabilities_mapping = {
            "openai-gpt4": ["complex_reasoning", "creative_writing", "code_analysis", "high_quality"],
            "openai-gpt35": ["general_conversation", "text_processing", "moderate_quality"],
            "ollama-llama2": ["simple_queries", "fast_response", "low_cost"],
            "ollama-mistral": ["code_analysis", "technical_tasks", "balanced_performance"],
            "claude-3-opus": ["complex_reasoning", "creative_writing", "high_quality"],
            "claude-3-sonnet": ["general_conversation", "code_analysis", "moderate_quality"]
        }
        return capabilities_mapping.get(model_name, ["general_purpose"])

    def _initialize_metrics(self) -> dict[str, dict[str, float]]:
        """åˆå§‹åŒ–æ€§èƒ½æŒ‡æ ‡"""
        return {
            name: {
                "success_rate": 1.0,
                "avg_response_time": 0.0,
                "cost_per_token": self._get_default_cost(name),
                "total_requests": 0,
                "total_cost": 0.0
            }
            for name in self.models
        }

    def _create_routing_chain(self) -> Runnable:
        """åˆ›å»ºåŸºäº LangChain 1.0 æœ€ä½³å®è·µçš„è·¯ç”±é“¾"""
        # åŸºäº LangChain æœ€ä½³å®è·µçš„è·¯ç”±æç¤ºè¯æ¨¡æ¿
        routing_prompt = ChatPromptTemplate.from_messages([
            ("system", """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ LLM è·¯ç”±å™¨ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·è¯·æ±‚çš„å†…å®¹ã€å¤æ‚åº¦ã€æˆæœ¬é¢„ç®—ã€ç³»ç»ŸçŠ¶æ€å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹ã€‚

## è·¯ç”±åŸåˆ™ï¼ˆåŸºäº LangChain 1.0 æœ€ä½³å®è·µï¼‰
1. **åŠ¨æ€ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šæ ¹æ®è¿è¡Œæ—¶çŠ¶æ€è°ƒæ•´è·¯ç”±ç­–ç•¥
2. **å¤šç»´åº¦è¯„ä¼°**ï¼šå†…å®¹ç±»å‹ã€æ€§èƒ½æŒ‡æ ‡ã€æˆæœ¬æ•ˆç›Šã€ç³»ç»Ÿè´Ÿè½½
3. **æ•…éšœè½¬ç§»æœºåˆ¶**ï¼šè‡ªåŠ¨é™çº§åˆ°å¯ç”¨æ¨¡å‹
4. **è´Ÿè½½å‡è¡¡**ï¼šé¿å…è¿‡åº¦ä½¿ç”¨å•ä¸€æ¨¡å‹

## å¯ç”¨æ¨¡å‹åŠå…¶ç‰¹ç‚¹ï¼š
{model_descriptions}

## å½“å‰ç³»ç»ŸçŠ¶æ€ï¼š
{system_context}

## è·¯ç”±å†³ç­–æ¡†æ¶ï¼š
### å†…å®¹ç±»å‹æ˜ å°„ï¼š
- **å¤æ‚æ¨ç†**ï¼šæ•°å­¦è®¡ç®—ã€é€»è¾‘åˆ†æã€ç®—æ³•è®¾è®¡ â†’ openai-gpt4, claude-3-opus
- **åˆ›æ„å†™ä½œ**ï¼šæ•…äº‹ç”Ÿæˆã€æ–‡æ¡ˆåˆ›ä½œã€å†…å®¹ç”Ÿæˆ â†’ openai-gpt4, claude-3-opus
- **ä»£ç åˆ†æ**ï¼šç¼–ç¨‹é—®é¢˜ã€æŠ€æœ¯è°ƒè¯•ã€ä»£ç å®¡æŸ¥ â†’ ollama-mistral, openai-gpt4
- **æ–‡æœ¬å¤„ç†**ï¼šæ–‡æ¡£åˆ†æã€æ€»ç»“ç¿»è¯‘ã€ä¿¡æ¯æå– â†’ openai-gpt35, claude-3-sonnet
- **ç®€å•æŸ¥è¯¢**ï¼šå¿«é€Ÿé—®ç­”ã€åŸºæœ¬ä¿¡æ¯ã€çŠ¶æ€æŸ¥è¯¢ â†’ ollama-llama2, openai-gpt35

### æ€§èƒ½æƒé‡å› å­ï¼š
- **æˆåŠŸç‡**ï¼š>90% (+20%), 80-90% (+10%), <80% (-30%)
- **å“åº”æ—¶é—´**ï¼š>10s (-25%), 5-10s (-10%), <5s (+15%)
- **è´Ÿè½½çŠ¶æ€**ï¼šé«˜è´Ÿè½½ (-20%), ä¸­ç­‰ (-5%), ä½è´Ÿè½½ (+10%)

### æˆæœ¬ä¼˜åŒ–ç­–ç•¥ï¼š
- **é¢„ç®—å……è¶³**ï¼šä¼˜å…ˆè´¨é‡ï¼Œé€‰æ‹©é«˜æ€§èƒ½æ¨¡å‹
- **é¢„ç®—ä¸­ç­‰**ï¼šå¹³è¡¡æ€§èƒ½ä¸æˆæœ¬
- **é¢„ç®—æœ‰é™**ï¼šä¼˜å…ˆæˆæœ¬ï¼Œé€‰æ‹©ç»æµæ¨¡å‹

## ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{routing_context}

## åˆ†ææµç¨‹ï¼š
1. **å†…å®¹è¯†åˆ«**ï¼šåˆ†æè¯·æ±‚ç±»å‹ã€å¤æ‚åº¦ã€ç´§æ€¥ç¨‹åº¦
2. **æ¨¡å‹åŒ¹é…**ï¼šæ ¹æ®èƒ½åŠ›æ˜ å°„æ‰¾åˆ°å€™é€‰æ¨¡å‹
3. **çŠ¶æ€è¯„ä¼°**ï¼šæ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡
4. **æƒé‡è®¡ç®—**ï¼šç»¼åˆæ€§èƒ½ã€æˆæœ¬ã€è´Ÿè½½å› ç´ 
5. **æœ€ç»ˆé€‰æ‹©**ï¼šé€‰æ‹©ç»¼åˆå¾—åˆ†æœ€é«˜çš„å¯ç”¨æ¨¡å‹

## è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š

{{"model_name": "æ¨¡å‹åç§°","reason": "è¯¦ç»†å†³ç­–ç†ç”±","confidence": 0.85}}
"""),
            ("user", "ç”¨æˆ·è¯·æ±‚ï¼š{input}")
        ])

        # ä½¿ç”¨æœ€å¼ºçš„æ¨¡å‹ä½œä¸ºè·¯ç”±æ¨¡å‹
        routing_model = self._get_routing_model()

        # åˆ›å»ºç»“æ„åŒ–è¾“å‡ºè§£æå™¨ï¼ˆåŸºäº LangChain 1.0 æœ€ä½³å®è·µï¼‰
        structured_llm = routing_model.with_structured_output(RouteDecision)

        # åˆ›å»ºè·¯ç”±é“¾ï¼ˆä½¿ç”¨ LangChain 1.0 æ¨èçš„é“¾å¼ç»„åˆï¼‰
        routing_chain = routing_prompt | structured_llm

        return routing_chain

    def _get_routing_model(self) -> BaseChatModel:
        """è·å–ç”¨äºè·¯ç”±çš„æ¨¡å‹"""
        # ä¼˜å…ˆä½¿ç”¨é«˜è´¨é‡æ¨¡å‹è¿›è¡Œè·¯ç”±å†³ç­–
        high_quality_models = ["openai-gpt4", "claude-3-opus"]
        for model_name in high_quality_models:
            if model_name in self.models:
                return self.models[model_name]

        # é™çº§åˆ°å…¶ä»–å¯ç”¨æ¨¡å‹
        for model_name, model in self.models.items():
            if "gpt" in model_name or "claude" in model_name:
                return model

        # æœ€åä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        return list(self.models.values())[0]

    def _create_execution_chain(self) -> RunnableSequence:
        """åˆ›å»ºæ‰§è¡Œé“¾"""
        # åˆ›å»ºæ¨¡å‹æ‰§è¡Œé“¾
        model_chains = {}

        for model_name, model in self.models.items():
            # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºæ‰§è¡Œé“¾
            model_chain = (
                RunnableLambda(self._prepare_model_input) |
                model |
                RunnableLambda(self._extract_response)
            )
            model_chains[model_name] = model_chain

        # åˆ›å»ºå¤šè·¯ç”±æ‰§è¡Œå™¨
        # åœ¨ LangChain v1.0 ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å®ç°è·¯ç”±é€»è¾‘
        return self._create_manual_router(model_chains)

    @trace_method
    async def route_request(self, request: str, context: dict[str, Any] | None = None) -> dict[str, Any]:
        """
        è·¯ç”±è¯·æ±‚åˆ°åˆé€‚çš„æ¨¡å‹

        Args:
            request: ç”¨æˆ·è¯·æ±‚
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            è·¯ç”±å†³ç­–ç»“æœ
        """
        start_time = time.time()

        try:
            # å‡†å¤‡è·¯ç”±ä¸Šä¸‹æ–‡
            routing_context = self._prepare_routing_context(context)

            # æ‰§è¡Œè·¯ç”±å†³ç­–
            decision = await self.routing_chain.ainvoke({
                "input": request,
                **routing_context
            })

            execution_time = time.time() - start_time

            result = {
                "model_name": decision.model_name,
                "reason": decision.reason,
                "confidence": decision.confidence,
                "execution_time": execution_time,
                "success": True
            }

            logger.info(f"Routing decision: {decision.model_name} (confidence: {decision.confidence})")
            return result

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Routing failed: {str(e)}")

            # è¿”å›é»˜è®¤æ¨¡å‹
            default_model = self._get_default_model()
            return {
                "model_name": default_model,
                "reason": f"è·¯ç”±å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: {str(e)}",
                "confidence": 0.5,
                "execution_time": execution_time,
                "success": False,
                "error": str(e)
            }

    def _prepare_routing_context(self, context: dict[str, Any] | None) -> dict[str, Any]:
        """å‡†å¤‡å¢å¼ºçš„è·¯ç”±ä¸Šä¸‹æ–‡"""
        # ç”Ÿæˆæ¨¡å‹æè¿°
        model_descriptions = []
        for name, config in self.model_configs.items():
            capabilities_str = ", ".join(config["capabilities"])
            cost_str = f"${config['cost_per_token']:.4f}/token"
            latency_str = f"{config.get('avg_response_time', 0):.2f}s"
            model_descriptions.append(
                f"- {name}: {capabilities_str} (æˆæœ¬: {cost_str}, å»¶è¿Ÿ: {latency_str})"
            )

        # ç”Ÿæˆç³»ç»Ÿä¸Šä¸‹æ–‡
        system_context = self._generate_system_context()

        # ç”Ÿæˆè·¯ç”±ä¸Šä¸‹æ–‡
        routing_context = self._generate_routing_context(context)

        return {
            "model_descriptions": "\n".join(model_descriptions),
            "system_context": system_context,
            "routing_context": routing_context
        }

    def _generate_system_context(self) -> str:
        """ç”Ÿæˆç³»ç»ŸçŠ¶æ€ä¸Šä¸‹æ–‡"""
        system_info = []

        # æ¨¡å‹æ€§èƒ½çŠ¶æ€
        model_statuses = []
        for name, metrics in self.performance_metrics.items():
            status = self._assess_model_status(name, metrics)
            model_statuses.append(f"- {name}: {status}")

        system_info.append(f"**æ¨¡å‹çŠ¶æ€**: {'; '.join(model_statuses)}")

        # ç³»ç»Ÿè´Ÿè½½æƒ…å†µ
        load_info = self._assess_system_load()
        system_info.append(f"**ç³»ç»Ÿè´Ÿè½½**: {load_info}")

        # æˆæœ¬æ¦‚è§ˆ
        cost_info = self._generate_cost_summary()
        system_info.append(f"**æˆæœ¬æ¦‚è§ˆ**: {cost_info}")

        return "\n".join(system_info)

    def _assess_model_status(self, model_name: str, metrics: dict[str, Any]) -> str:
        """è¯„ä¼°æ¨¡å‹çŠ¶æ€"""
        success_rate = metrics.get('success_rate', 0)
        response_time = metrics.get('avg_response_time', 0)
        total_requests = metrics.get('total_requests', 0)

        # çŠ¶æ€è¯„ä¼°é€»è¾‘
        if success_rate < 0.8:
            status = "âš ï¸ æ•…éšœ"
        elif success_rate < 0.9:
            status = "ğŸŸ¡ ä¸ç¨³å®š"
        elif response_time > 10:
            status = "ğŸŒ å“åº”æ…¢"
        elif total_requests == 0:
            status = "âšª æœªä½¿ç”¨"
        else:
            status = "âœ… æ­£å¸¸"

        return f"{status} (æˆåŠŸç‡:{success_rate:.1%}, å“åº”:{response_time:.1f}s, è¯·æ±‚:{total_requests})"

    def _assess_system_load(self) -> str:
        """è¯„ä¼°ç³»ç»Ÿè´Ÿè½½"""
        total_requests = sum(m.get('total_requests', 0) for m in self.performance_metrics.values())
        avg_response_time = sum(m.get('avg_response_time', 0) for m in self.performance_metrics.values()) / len(self.performance_metrics)

        if total_requests > 1000:
            load_level = "é«˜è´Ÿè½½"
        elif total_requests > 100:
            load_level = "ä¸­ç­‰è´Ÿè½½"
        else:
            load_level = "ä½è´Ÿè½½"

        return f"{load_level} (æ€»è¯·æ±‚:{total_requests}, å¹³å‡å“åº”:{avg_response_time:.1f}s)"

    def _generate_cost_summary(self) -> str:
        """ç”Ÿæˆæˆæœ¬æ¦‚è§ˆ"""
        total_cost = sum(m.get('total_cost', 0) for m in self.performance_metrics.values())
        avg_cost_per_request = total_cost / max(sum(m.get('total_requests', 1) for m in self.performance_metrics.values()), 1)

        return f"æ€»æˆæœ¬:${total_cost:.2f}, å¹³å‡æˆæœ¬:${avg_cost_per_request:.4f}/è¯·æ±‚"

    def _generate_routing_context(self, context: dict[str, Any] | None) -> str:
        """ç”Ÿæˆè·¯ç”±ä¸Šä¸‹æ–‡"""
        context_info = []

        if context:
            # é¢„ç®—æç¤º
            if budget_hint := context.get('budget_hint'):
                context_info.append(f"**é¢„ç®—æç¤º**: {budget_hint}")

            # ä¼˜å…ˆçº§
            if priority := context.get('priority'):
                context_info.append(f"**ä¼˜å…ˆçº§**: {priority}")

            # å“åº”æ—¶é—´è¦æ±‚
            if max_response_time := context.get('max_response_time'):
                context_info.append(f"**æœ€å¤§å“åº”æ—¶é—´**: {max_response_time}s")

        # æ·»åŠ é»˜è®¤ä¸Šä¸‹æ–‡
        if not context_info:
            context_info.append("**é»˜è®¤è·¯ç”±**: æ— ç‰¹æ®Šè¦æ±‚ï¼Œä½¿ç”¨æ™ºèƒ½è·¯ç”±")

        return "\n".join(context_info)

    def _get_default_model(self) -> str:
        """è·å–é»˜è®¤æ¨¡å‹"""
        # ä½¿ç”¨é…ç½®ç®¡ç†å™¨çš„é»˜è®¤æ¨¡å‹é€‰æ‹©é€»è¾‘
        default_model_name = config_manager.get_default_model()
        if default_model_name and default_model_name in self.models:
            return default_model_name

        # é™çº§åˆ°ä¼˜å…ˆçº§é€‰æ‹©
        preferred_models = ["openai-gpt35-turbo", "ollama-mistral", "groq-llama3"]
        for model_name in preferred_models:
            if model_name in self.models:
                return model_name

        # æœ€åä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        return list(self.models.keys())[0]

    @trace_method
    async def execute_request(self, request: str, **kwargs) -> dict[str, Any]:
        """
        æ‰§è¡Œè¯·æ±‚å¹¶è¿”å›ç»“æœ

        Args:
            request: ç”¨æˆ·è¯·æ±‚
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            æ‰§è¡Œç»“æœ
        """
        start_time = time.time()

        try:
            # è·¯ç”±å†³ç­–
            routing_result = await self.route_request(request)

            if not routing_result["success"]:
                raise Exception(f"è·¯ç”±å¤±è´¥: {routing_result.get('error', 'Unknown error')}")

            model_name = routing_result["model_name"]
            selected_model = self.models[model_name]

            # æ‰§è¡Œè¯·æ±‚
            response = await selected_model.ainvoke([
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI åŠ©æ‰‹ï¼Œè¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"),
                HumanMessage(content=request)
            ], **kwargs)

            execution_time = time.time() - start_time

            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            await self._update_metrics(model_name, success=True, response_time=execution_time)

            result = {
                "model": model_name,
                "response": response.content if hasattr(response, 'content') else str(response),
                "usage": getattr(response, 'usage_metadata', {}),
                "execution_time": execution_time,
                "success": True,
                "routing_info": routing_result
            }

            logger.info(f"Request executed successfully with {model_name} in {execution_time:.2f}s")
            return result

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Request execution failed: {str(e)}")

            # æ›´æ–°å¤±è´¥æŒ‡æ ‡
            if "model_name" in locals():
                await self._update_metrics(model_name, success=False, response_time=execution_time)

            raise e

    @trace_method
    async def execute_chain(self, request: str, chain_config: dict[str, Any]) -> dict[str, Any]:
        """
        æ‰§è¡Œé“¾å¼è¯·æ±‚

        Args:
            request: åˆå§‹è¯·æ±‚
            chain_config: é“¾é…ç½®

        Returns:
            é“¾æ‰§è¡Œç»“æœ
        """
        start_time = time.time()

        try:
            # æ‰§è¡Œè·¯ç”±å†³ç­–
            routing_result = await self.route_request(request)

            if not routing_result["success"]:
                raise Exception(f"è·¯ç”±å¤±è´¥: {routing_result.get('error', 'Unknown error')}")

            model_name = routing_result["model_name"]
            selected_model = self.models[model_name]

            # æ„å»ºæ‰§è¡Œé“¾
            execution_chain = self._build_chain(selected_model, chain_config)

            # æ‰§è¡Œé“¾
            result = await execution_chain.ainvoke({"input": request})

            execution_time = time.time() - start_time

            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            await self._update_metrics(model_name, success=True, response_time=execution_time)

            return {
                "model": model_name,
                "result": result,
                "execution_time": execution_time,
                "success": True,
                "routing_info": routing_result
            }

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Chain execution failed: {str(e)}")
            raise e

    def _build_chain(self, model: BaseChatModel, chain_config: dict[str, Any]) -> RunnableSequence:
        """æ„å»ºæ‰§è¡Œé“¾"""
        chain_steps = []

        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if "system_prompt" in chain_config:
            chain_steps.append(RunnableLambda(
                lambda x: [SystemMessage(content=chain_config["system_prompt"])] + x
            ))

        # æ·»åŠ æ¨¡å‹
        chain_steps.append(model)

        # æ·»åŠ è¾“å‡ºè§£æå™¨
        if chain_config.get("parse_output", True):
            chain_steps.append(StrOutputParser())

        return RunnableSequence(*chain_steps)

    async def _update_metrics(self, model_name: str, success: bool, response_time: float):
        """æ›´æ–°æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
        metrics = self.performance_metrics[model_name]

        # æ›´æ–°æˆåŠŸç‡ (æŒ‡æ•°åŠ æƒå¹³å‡)
        metrics["success_rate"] = (
            0.9 * metrics["success_rate"] + 0.1 * (1.0 if success else 0.0)
        )

        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´ (æŒ‡æ•°åŠ æƒå¹³å‡)
        if metrics["avg_response_time"] == 0:
            metrics["avg_response_time"] = response_time
        else:
            metrics["avg_response_time"] = (
                0.9 * metrics["avg_response_time"] + 0.1 * response_time
            )

        # æ›´æ–°æ€»è¯·æ±‚æ¬¡æ•°
        metrics["total_requests"] += 1

        # æ›´æ–°æ€»æˆæœ¬
        if success:
            cost = self.model_configs[model_name]["cost_per_token"] * response_time
            metrics["total_cost"] += cost

    def get_performance_metrics(self) -> dict[str, dict[str, Any]]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return {
            name: {
                **metrics,
                "capabilities": self.model_configs[name]["capabilities"]
            }
            for name, metrics in self.performance_metrics.items()
        }

    def get_model_info(self) -> dict[str, dict[str, Any]]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            name: {
                "config": config,
                "metrics": self.performance_metrics[name]
            }
            for name, config in self.model_configs.items()
        }

    def _prepare_model_input(self, inputs: str | dict | list) -> list[BaseMessage]:
        """å‡†å¤‡æ¨¡å‹è¾“å…¥"""
        if isinstance(inputs, str):
            return [HumanMessage(content=inputs)]
        elif isinstance(inputs, dict):
            input_text = inputs.get("input", "")
            return [HumanMessage(content=input_text)]
        elif isinstance(inputs, list):
            # å¦‚æœå·²ç»æ˜¯æ¶ˆæ¯åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
            if all(isinstance(msg, BaseMessage) for msg in inputs):
                return inputs
            else:
                # å°†å­—ç¬¦ä¸²åˆ—è¡¨è½¬æ¢ä¸ºæ¶ˆæ¯åˆ—è¡¨
                return [HumanMessage(content=str(msg)) for msg in inputs]
        else:
            return [HumanMessage(content=str(inputs))]

    def _extract_response(self, response: AIMessage | dict | str) -> str | dict:
        """æå–æ¨¡å‹å“åº”"""
        if isinstance(response, AIMessage):
            return response.content
        elif isinstance(response, dict):
            # å¦‚æœæ˜¯å­—å…¸ï¼Œä¿æŒåŸæ ·
            return response
        else:
            # å…¶ä»–æƒ…å†µè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            return str(response)

    def _create_manual_router(self, model_chains: dict[str, Runnable]) -> Runnable:
        """æ‰‹åŠ¨åˆ›å»ºæ™ºèƒ½è·¯ç”±å™¨"""
        async def route_and_execute(inputs):
            """æ™ºèƒ½è·¯ç”±å’Œæ‰§è¡Œé€»è¾‘"""
            try:
                # è·å–è¾“å…¥å†…å®¹
                if isinstance(inputs, dict):
                    input_text = inputs.get("input", "")
                elif isinstance(inputs, str):
                    input_text = inputs
                else:
                    input_text = str(inputs)

                # æ™ºèƒ½è·¯ç”±å†³ç­–
                routing_decision = await self._intelligent_route(input_text)

                if not routing_decision["success"]:
                    # è·¯ç”±å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤æ¨¡å‹
                    model_name = self._get_default_model()
                    logger.warning(f"æ™ºèƒ½è·¯ç”±å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: {model_name}")
                else:
                    model_name = routing_decision["model_name"]
                    logger.info(f"æ™ºèƒ½è·¯ç”±é€‰æ‹©æ¨¡å‹: {model_name} (ç½®ä¿¡åº¦: {routing_decision['confidence']})")

                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
                if model_name not in model_chains:
                    raise ValueError(f"æ¨¡å‹ {model_name} ä¸å¯ç”¨")

                # æ‰§è¡Œæ¨¡å‹é“¾
                chain = model_chains[model_name]

                # æ·»åŠ è·¯ç”±ä¸Šä¸‹æ–‡
                if isinstance(inputs, dict):
                    inputs["routing_context"] = {
                        "selected_model": model_name,
                        "routing_reason": routing_decision.get("reason", ""),
                        "confidence": routing_decision.get("confidence", 0.5)
                    }

                # æ‰§è¡Œå¹¶è¿”å›ç»“æœ
                result = await chain.ainvoke(inputs)

                # è¿”å›å¢å¼ºçš„ç»“æœ
                return {
                    "result": result,
                    "model": model_name,
                    "routing_info": routing_decision,
                    "success": True
                }

            except Exception as e:
                logger.error(f"è·¯ç”±æ‰§è¡Œå¤±è´¥: {str(e)}")
                raise ValueError(f"è·¯ç”±æ‰§è¡Œå¤±è´¥: {str(e)}")

        return RunnableLambda(route_and_execute)

    async def _intelligent_route(self, input_text: str) -> dict[str, Any]:
        """åŸºäº LangChain 1.0 æœ€ä½³å®è·µçš„æ™ºèƒ½è·¯ç”±å†³ç­–"""
        try:
            # 1. åŠ¨æ€å†…å®¹åˆ†æï¼ˆä½¿ç”¨ LangChain æ¨èçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼‰
            content_analysis = await self._dynamic_content_analysis(input_text)

            # 2. è·å–æ¨¡å‹å¯ç”¨æ€§çŠ¶æ€
            model_availability = await self._get_model_availability()

            # 3. æ¨¡å‹è¯„åˆ†ï¼ˆè€ƒè™‘å¯ç”¨æ€§ï¼‰
            model_scores = self._score_models(content_analysis, model_availability)

            # 4. æ€§èƒ½åŠ æƒï¼ˆåŸºäºå®æ—¶æŒ‡æ ‡ï¼‰
            weighted_scores = self._apply_performance_weights(model_scores)

            # 5. åŠ¨æ€å¥åº·æƒé‡è°ƒæ•´
            health_weighted_scores = self._apply_dynamic_health_weights(weighted_scores)

            # 6. æˆæœ¬ä¼˜åŒ–åˆ†æ
            cost_optimized_scores = self._apply_cost_optimization(health_weighted_scores, content_analysis)

            # 7. è´Ÿè½½å‡è¡¡è°ƒæ•´
            balanced_scores = self._apply_load_balancing(cost_optimized_scores)

            # 8. æ™ºèƒ½æ•…éšœè½¬ç§»
            final_model = await self._intelligent_fallback_routing(
                input_text, content_analysis, balanced_scores, model_availability
            )
            model_name, final_score = final_model

            # 9. è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(balanced_scores, final_score)

            # 10. ç”Ÿæˆè¯¦ç»†ç†ç”±
            detailed_reason = self._generate_detailed_reason(
                content_analysis, model_name, confidence, balanced_scores
            )

            return {
                "model_name": model_name,
                "reason": detailed_reason,
                "confidence": confidence,
                "success": True,
                "scores": balanced_scores,
                "availability": model_availability,
                "analysis": content_analysis,
                "final_score": final_score
            }

        except Exception as e:
            logger.error(f"æ™ºèƒ½è·¯ç”±å†³ç­–å¤±è´¥: {str(e)}")
            # ä½¿ç”¨é™çº§ç­–ç•¥
            fallback_model = await self._get_fallback_model()
            return {
                "model_name": fallback_model,
                "reason": f"è·¯ç”±ç³»ç»Ÿå¼‚å¸¸ï¼Œä½¿ç”¨é™çº§æ¨¡å‹: {str(e)}",
                "confidence": 0.3,
                "success": False
            }

    async def _get_model_availability(self) -> dict[str, bool]:
        """è·å–æ¨¡å‹å¯ç”¨æ€§çŠ¶æ€"""
        availability = {}

        for model_name in self.models.keys():
            # é»˜è®¤å¯ç”¨
            is_available = True

            # æ£€æŸ¥å¥åº·çŠ¶æ€
            if self.health_checker and model_name in self.health_checker.health_status:
                health = self.health_checker.health_status[model_name]
                is_available = health.get("is_healthy", True)

                # æ£€æŸ¥è¿ç»­å¤±è´¥æ¬¡æ•°
                consecutive_failures = health.get("metadata", {}).get("consecutive_failures", 0)
                if consecutive_failures > 3:
                    is_available = False
                    logger.warning(f"Model {model_name} marked as unavailable due to {consecutive_failures} consecutive failures")

            availability[model_name] = is_available

        logger.debug(f"Model availability: {availability}")
        return availability

    def _get_routing_cache_key(self, input_text: str, context: dict[str, Any] | None) -> str:
        """ç”Ÿæˆè·¯ç”±ç¼“å­˜é”®ï¼ˆåŸºäºLangChainæœ€ä½³å®è·µï¼‰"""
        import hashlib
        import json

        # åˆ›å»ºç¼“å­˜é”®çš„ç»„ä»¶ï¼ˆåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
        key_components = {
            "input_text": input_text,
            "context": context,
            "model_configs": {name: {
                "capabilities": config["capabilities"],
                "cost_per_token": config["cost_per_token"]
            } for name, config in self.model_configs.items()},
            "performance_metrics": {name: {
                "success_rate": metrics["success_rate"],
                "avg_response_time": metrics["avg_response_time"]
            } for name, metrics in self.performance_metrics.items()}
        }

        # åºåˆ—åŒ–å¹¶ç”Ÿæˆå“ˆå¸Œ
        key_string = json.dumps(key_components, sort_keys=True, default=str)
        return hashlib.md5(key_string.encode()).hexdigest()

    def _is_cache_valid(self, cache_time: float, model_health: dict[str, Any]) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«æ¨¡å‹å¥åº·çŠ¶æ€æ£€æŸ¥ï¼‰"""
        # æ£€æŸ¥TTLè¿‡æœŸ
        if (time.time() - cache_time) > self._cache_ttl:
            return False

        # æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€å˜åŒ–ï¼ˆå¦‚æœæœ‰ç¼“å­˜çš„å¥åº·çŠ¶æ€ä¿¡æ¯ï¼‰
        cached_health = model_health.get("health_snapshot", {})
        if self.health_checker:
            current_health = {}
            for model_name in self.models.keys():
                if model_name in self.health_checker.health_status:
                    health = self.health_checker.health_status[model_name]
                    current_health[model_name] = {
                        "is_healthy": health.get("is_healthy", True),
                        "success_rate": health.get("success_rate", 1.0),
                        "consecutive_failures": health.get("metadata", {}).get("consecutive_failures", 0)
                    }

            # å¦‚æœå¥åº·çŠ¶æ€å‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œä½¿ç¼“å­˜å¤±æ•ˆ
            for model_name, current in current_health.items():
                cached = cached_health.get(model_name, {})
                if (current.get("is_healthy", True) != cached.get("is_healthy", True) or
                    abs(current.get("success_rate", 1.0) - cached.get("success_rate", 1.0)) > 0.2):
                    return False

        return True

    def _cleanup_expired_cache(self) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒLRUå’Œå®¹é‡ç®¡ç†ï¼‰"""
        current_time = time.time()
        cleaned_count = 0

        if current_time - self._last_cleanup_time > self._cache_cleanup_interval:
            expired_keys = []
            healthy_models = set()

            # è·å–å½“å‰å¥åº·æ¨¡å‹åˆ—è¡¨
            if self.health_checker:
                healthy_models = {
                    name for name, health in self.health_checker.health_status.items()
                    if health.get("is_healthy", True)
                }

            # æ£€æŸ¥è¿‡æœŸå’Œæ— æ•ˆç¼“å­˜é¡¹
            for key, (result, cache_time, model_health) in self._routing_cache.items():
                # æ£€æŸ¥TTLè¿‡æœŸ
                if not self._is_cache_valid(cache_time, model_health):
                    expired_keys.append(key)
                # æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€ï¼ˆå¦‚æœé€‰æ‹©äº†ä¸å¥åº·çš„æ¨¡å‹ï¼‰
                elif result.get("model_name") not in healthy_models:
                    expired_keys.append(key)

            # æ¸…ç†è¿‡æœŸé¡¹
            for key in expired_keys:
                del self._routing_cache[key]
                cleaned_count += 1

            # å¦‚æœç¼“å­˜å¤§å°è¶…è¿‡é™åˆ¶ï¼Œæ‰§è¡ŒLRUæ¸…ç†
            while len(self._routing_cache) > self._cache_max_size:
                # æ‰¾åˆ°æœ€è€çš„ç¼“å­˜é¡¹ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºéšæœºåˆ é™¤ï¼Œå®é™…å¯ä»¥ç»´æŠ¤LRUé˜Ÿåˆ—ï¼‰
                oldest_key = next(iter(self._routing_cache))
                del self._routing_cache[oldest_key]
                cleaned_count += 1
                self._cache_stats["evictions"] += 1

            self._last_cleanup_time = current_time
            if cleaned_count > 0:
                logger.info(f"æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸ/æ— æ•ˆç¼“å­˜é¡¹ï¼Œå½“å‰ç¼“å­˜å¤§å°: {len(self._routing_cache)}")

        return cleaned_count

    def _get_cached_routing_result(self, cache_key: str) -> dict[str, Any] | None:
        """è·å–ç¼“å­˜çš„è·¯ç”±ç»“æœï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if cache_key in self._routing_cache:
            result, cache_time, model_health = self._routing_cache[cache_key]

            # æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§
            if self._is_cache_valid(cache_time, model_health):
                self._cache_stats["hits"] += 1
                logger.debug(f"è·¯ç”±ç¼“å­˜å‘½ä¸­: {cache_key[:8]}...")

                # è®°å½•ç¼“å­˜å‘½ä¸­æ—¶çš„å¥åº·çŠ¶æ€å˜åŒ–
                if self.health_checker:
                    current_health = {}
                    for model_name in self.models.keys():
                        if model_name in self.health_checker.health_status:
                            health = self.health_checker.health_status[model_name]
                            current_health[model_name] = {
                                "is_healthy": health.get("is_healthy", True),
                                "success_rate": health.get("success_rate", 1.0)
                            }

                    # æ›´æ–°ç¼“å­˜é¡¹çš„å¥åº·çŠ¶æ€å¿«ç…§
                    self._routing_cache[cache_key] = (result, cache_time, {"health_snapshot": current_health})

                return result
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self._routing_cache[cache_key]
                self._cache_stats["misses"] += 1
                logger.debug(f"è·¯ç”±ç¼“å­˜è¿‡æœŸ: {cache_key[:8]}...")

        return None

    def _cache_routing_result(self, cache_key: str, result: dict[str, Any]) -> None:
        """ç¼“å­˜è·¯ç”±ç»“æœï¼ˆå¢å¼ºç‰ˆï¼‰"""
        # è·å–å½“å‰æ¨¡å‹å¥åº·çŠ¶æ€å¿«ç…§
        model_health = {}
        if self.health_checker:
            for model_name in self.models.keys():
                if model_name in self.health_checker.health_status:
                    health = self.health_checker.health_status[model_name]
                    model_health[model_name] = {
                        "is_healthy": health.get("is_healthy", True),
                        "success_rate": health.get("success_rate", 1.0),
                        "consecutive_failures": health.get("metadata", {}).get("consecutive_failures", 0)
                    }

        health_snapshot = {"health_snapshot": model_health}

        # å¦‚æœå·²å­˜åœ¨ç›¸åŒé”®ï¼Œå…ˆåˆ é™¤ï¼ˆä¿æŒLRUé¡ºåºï¼‰
        if cache_key in self._routing_cache:
            del self._routing_cache[cache_key]

        # æ£€æŸ¥ç¼“å­˜å®¹é‡é™åˆ¶
        while len(self._routing_cache) >= self._cache_max_size:
            # ç®€åŒ–çš„LRUï¼šåˆ é™¤æœ€è€çš„é¡¹
            oldest_key = next(iter(self._routing_cache))
            del self._routing_cache[oldest_key]
            self._cache_stats["evictions"] += 1

        # å­˜å‚¨æ–°ç¼“å­˜é¡¹
        self._routing_cache[cache_key] = (result, time.time(), health_snapshot)

        # å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
        self._cleanup_expired_cache()

    @lru_cache(maxsize=1000)
    def _cached_content_analysis(self, input_text: str) -> dict[str, Any]:
        """ç¼“å­˜å†…å®¹åˆ†æç»“æœï¼ˆåŸºäºLangChainæœ€ä½³å®è·µï¼‰"""
        return self._analyze_content(input_text)

    def get_cache_stats(self) -> dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŸºäºLangChainæœ€ä½³å®è·µï¼‰"""
        total_requests = self._cache_stats["hits"] + self._cache_stats["misses"]
        hit_rate = self._cache_stats["hits"] / total_requests if total_requests > 0 else 0

        # è·å–ç¼“å­˜å†…å­˜ä½¿ç”¨æƒ…å†µ
        cache_size_bytes = len(str(self._routing_cache))
        memory_usage_mb = cache_size_bytes / (1024 * 1024)

        stats = {
            "routing_cache": {
                "total_requests": total_requests,
                "cache_hits": self._cache_stats["hits"],
                "cache_misses": self._cache_stats["misses"],
                "hit_rate": hit_rate,
                "cache_size": len(self._routing_cache),
                "max_size": self._cache_max_size,
                "evictions": self._cache_stats["evictions"],
                "memory_usage_mb": round(memory_usage_mb, 2),
                "is_healthy": hit_rate > 0.3,  # ç¼“å­˜å‘½ä¸­ç‡>30%è®¤ä¸ºå¥åº·
                "recommendation": self._get_cache_optimization_recommendation(hit_rate)
            }
        }

        return stats

    def _get_cache_optimization_recommendation(self, hit_rate: float) -> str:
        """è·å–ç¼“å­˜ä¼˜åŒ–å»ºè®®"""
        if hit_rate > 0.7:
            return "ç¼“å­˜æ•ˆæœä¼˜ç§€ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ ç¼“å­˜TTL"
        elif hit_rate > 0.5:
            return "ç¼“å­˜æ•ˆæœè‰¯å¥½ï¼Œä¿æŒå½“å‰é…ç½®"
        elif hit_rate > 0.3:
            return "ç¼“å­˜æ•ˆæœä¸€èˆ¬ï¼Œå»ºè®®ä¼˜åŒ–ç¼“å­˜é”®ç”Ÿæˆç­–ç•¥"
        else:
            return "ç¼“å­˜æ•ˆæœè¾ƒå·®ï¼Œå»ºè®®æ£€æŸ¥ç¼“å­˜é”®ç”Ÿæˆæˆ–å¢åŠ ç¼“å­˜å®¹é‡"

    def clear_cache(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        cache_size = len(self._routing_cache)
        self._routing_cache.clear()
        logger.info(f"å·²æ¸…ç©ºè·¯ç”±ç¼“å­˜ï¼Œå…±æ¸…ç† {cache_size} ä¸ªç¼“å­˜é¡¹")

    async def warmup_cache(self, warmup_data: list[tuple[str, dict[str, Any] | None]]) -> int:
        """é¢„çƒ­ç¼“å­˜ï¼ˆåŸºäºLangChainæœ€ä½³å®è·µï¼‰"""
        warmed_count = 0

        for input_text, context in warmup_data:
            cache_key = self._get_routing_cache_key(input_text, context)

            # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
            if cache_key not in self._routing_cache:
                try:
                    # æ‰§è¡Œè·¯ç”±å†³ç­–å¹¶ç¼“å­˜
                    routing_result = await self._intelligent_route(input_text)
                    self._cache_routing_result(cache_key, routing_result)
                    warmed_count += 1
                    logger.debug(f"ç¼“å­˜é¢„çƒ­æˆåŠŸ: {cache_key[:8]}...")
                except Exception as e:
                    logger.warning(f"ç¼“å­˜é¢„çƒ­å¤±è´¥: {e}")

        logger.info(f"ç¼“å­˜é¢„çƒ­å®Œæˆï¼Œå…±é¢„çƒ­ {warmed_count} ä¸ªç¼“å­˜é¡¹")
        return warmed_count

    async def _dynamic_content_analysis(self, input_text: str) -> dict[str, Any]:
        """åŠ¨æ€å†…å®¹åˆ†æï¼ˆåŸºäº LangChain æœ€ä½³å®è·µï¼‰"""
        # ä½¿ç”¨ç¼“å­˜çš„å†…å®¹åˆ†æ
        basic_analysis = self._cached_content_analysis(input_text)

        # æ·±åº¦åˆ†æ
        depth_analysis = self._analyze_complexity_depth(input_text)

        # ç´§æ€¥ç¨‹åº¦è¯„ä¼°
        urgency = self._assess_urgency(input_text)

        # ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æ
        context_analysis = await self._context_aware_analysis(input_text)

        return {
            **basic_analysis,
            **depth_analysis,
            "urgency": urgency,
            **context_analysis,
            "analysis_timestamp": time.time(),
            "analysis_version": "1.0"
        }

    def _analyze_complexity_depth(self, input_text: str) -> dict[str, Any]:
        """åˆ†æå¤æ‚åº¦æ·±åº¦"""
        word_count = len(input_text.split())
        line_count = len(input_text.split('\n'))
        technical_terms = self._extract_technical_terms(input_text)

        # å¤æ‚åº¦è®¡ç®—ï¼ˆåŸºäºå¤šç»´åº¦ï¼‰
        complexity_score = min(1.0, (
            word_count * 0.001 +  # é•¿åº¦å› ç´ 
            line_count * 0.05 +    # ç»“æ„å¤æ‚åº¦
            len(technical_terms) * 0.1 +  # æŠ€æœ¯æœ¯è¯­
            (1 if '?' in input_text else 0) * 0.2  # é—®é¢˜å¤æ‚åº¦
        ))

        # æ·±åº¦çº§åˆ«
        if complexity_score > 0.8:
            depth_level = "very_high"
        elif complexity_score > 0.6:
            depth_level = "high"
        elif complexity_score > 0.4:
            depth_level = "medium"
        elif complexity_score > 0.2:
            depth_level = "low"
        else:
            depth_level = "very_low"

        return {
            "complexity_score": complexity_score,
            "depth_level": depth_level,
            "word_count": word_count,
            "line_count": line_count,
            "technical_terms_count": len(technical_terms),
            "technical_terms": technical_terms
        }

    def _assess_urgency(self, input_text: str) -> dict[str, Any]:
        """è¯„ä¼°ç´§æ€¥ç¨‹åº¦"""
        urgency_keywords = [
            "ç´§æ€¥", "urgent", "ç«‹å³", "é©¬ä¸Š", "ç°åœ¨", " ASAP ",
            "crucial", "critical", "important", "å¿…é¡»", "éœ€è¦"
        ]

        urgency_score = sum(1 for keyword in urgency_keywords
                          if keyword.lower() in input_text.lower())

        if urgency_score >= 3:
            urgency_level = "critical"
            urgency_weight = 1.5
        elif urgency_score >= 2:
            urgency_level = "high"
            urgency_weight = 1.3
        elif urgency_score >= 1:
            urgency_level = "medium"
            urgency_weight = 1.1
        else:
            urgency_level = "low"
            urgency_weight = 1.0

        return {
            "urgency_score": urgency_score,
            "urgency_level": urgency_level,
            "urgency_weight": urgency_weight
        }

    async def _context_aware_analysis(self, input_text: str) -> dict[str, Any]:
        """ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æ"""
        # åˆ†ææ˜¯å¦éœ€è¦å¤šæ­¥éª¤å¤„ç†
        multi_step_indicators = [
            "æ­¥éª¤", "step", "é¦–å…ˆ", "ç„¶å", "æœ€å", "æµç¨‹",
            "è¿‡ç¨‹", "procedure", "method", "approach"
        ]

        is_multi_step = any(indicator in input_text.lower()
                          for indicator in multi_step_indicators)

        # åˆ†ææ˜¯å¦éœ€è¦ç²¾ç¡®æ€§
        precision_indicators = [
            "ç²¾ç¡®", "å‡†ç¡®", "è¯¦ç»†", "å…·ä½“", "exact", "precise",
            "è¯¦ç»†è¯´æ˜", "å…·ä½“æ­¥éª¤", "å‡†ç¡®ç­”æ¡ˆ"
        ]

        requires_precision = any(indicator in input_text.lower()
                               for indicator in precision_indicators)

        return {
            "requires_multi_step": is_multi_step,
            "requires_precision": requires_precision,
            "context_flags": {
                "multi_step": is_multi_step,
                "precision": requires_precision
            }
        }

    def _extract_technical_terms(self, text: str) -> list[str]:
        """æå–æŠ€æœ¯æœ¯è¯­"""
        tech_keywords = [
            "API", "database", "algorithm", "function", "class",
            "method", "variable", "parameter", "query", "request",
            "response", "error", "exception", "debug", "test",
            "deploy", "server", "client", "network", "security"
        ]

        found_terms = []
        text_lower = text.lower()
        for term in tech_keywords:
            if term.lower() in text_lower:
                found_terms.append(term)

        return list(set(found_terms))  # å»é‡

    def _apply_dynamic_health_weights(self, model_scores: dict[str, float]) -> dict[str, float]:
        """åº”ç”¨åŠ¨æ€å¥åº·æƒé‡"""
        weighted_scores = {}

        for model_name, score in model_scores.items():
            # è·å–å®æ—¶å¥åº·çŠ¶æ€
            health_multiplier = self._calculate_dynamic_health_multiplier(model_name)

            # åŠ¨æ€è°ƒæ•´æƒé‡
            weighted_score = score * health_multiplier
            weighted_scores[model_name] = weighted_score

            logger.debug(f"Model {model_name}: dynamic_health_multiplier={health_multiplier:.3f}, weighted_score={weighted_score:.3f}")

        return weighted_scores

    def _calculate_dynamic_health_multiplier(self, model_name: str) -> float:
        """è®¡ç®—åŠ¨æ€å¥åº·ä¹˜æ•°"""
        if not self.health_checker:
            return 1.0

        health = self.health_checker.health_status.get(model_name)
        if not health:
            return 1.0

        # åŸºäºå¤šä¸ªå¥åº·æŒ‡æ ‡è®¡ç®—åŠ¨æ€ä¹˜æ•°
        success_rate = health.get("success_rate", 1.0)
        response_time = health.get("response_time", 0.0)
        consecutive_failures = health.get("metadata", {}).get("consecutive_failures", 0)
        total_checks = health.get("metadata", {}).get("check_count", 1)

        # æˆåŠŸç‡æƒé‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
        success_weight = success_rate

        # å“åº”æ—¶é—´æƒé‡ï¼ˆåå‘æŒ‡æ•°ï¼‰
        response_weight = max(0.1, 1.0 - (response_time / 15.0))

        # è¿ç»­å¤±è´¥æƒ©ç½š
        failure_penalty = 0.1 ** consecutive_failures if consecutive_failures > 0 else 1.0

        # æ£€æŸ¥æ¬¡æ•°ç½®ä¿¡åº¦
        confidence_factor = min(1.0, total_checks / 5.0)

        # ç»¼åˆå¥åº·ä¹˜æ•°
        health_multiplier = (
            success_weight * 0.4 +
            response_weight * 0.3 +
            confidence_factor * 0.2
        ) * failure_penalty

        # å¦‚æœæ¨¡å‹ä¸å¥åº·ï¼Œåº”ç”¨é¢å¤–æƒ©ç½š
        if not health.get("is_healthy", True):
            health_multiplier *= 0.05

        return health_multiplier

    def _apply_cost_optimization(self, model_scores: dict[str, float], content_analysis: dict[str, Any]) -> dict[str, float]:
        """åº”ç”¨æˆæœ¬ä¼˜åŒ–"""
        optimized_scores = {}

        # è·å–å†…å®¹çš„é¢„ç®—æ•æ„Ÿåº¦
        budget_sensitivity = self._assess_budget_sensitivity(content_analysis)

        for model_name, score in model_scores.items():
            # è·å–æ¨¡å‹æˆæœ¬
            cost_per_token = self.model_configs[model_name].get("cost_per_token", 0.01)

            # æˆæœ¬æƒé‡è°ƒæ•´
            if budget_sensitivity == "high":
                # é«˜é¢„ç®—æ•æ„Ÿåº¦ï¼Œå¤§å¹…é™ä½é«˜æˆæœ¬æ¨¡å‹åˆ†æ•°
                cost_weight = max(0.1, 1.0 - (cost_per_token / 0.05))
            elif budget_sensitivity == "medium":
                # ä¸­ç­‰é¢„ç®—æ•æ„Ÿåº¦ï¼Œé€‚åº¦è°ƒæ•´
                cost_weight = max(0.3, 1.0 - (cost_per_token / 0.1))
            else:
                # ä½é¢„ç®—æ•æ„Ÿåº¦ï¼Œè½»å¾®è°ƒæ•´
                cost_weight = max(0.7, 1.0 - (cost_per_token / 0.2))

            # åº”ç”¨æˆæœ¬ä¼˜åŒ–
            optimized_score = score * cost_weight
            optimized_scores[model_name] = optimized_score

            logger.debug(f"Model {model_name}: cost_weight={cost_weight:.3f}, optimized_score={optimized_score:.3f}")

        return optimized_scores

    def _assess_budget_sensitivity(self, content_analysis: dict[str, Any]) -> str:
        """è¯„ä¼°é¢„ç®—æ•æ„Ÿåº¦"""
        content_type = content_analysis.get("content_type", "")
        urgency_level = content_analysis.get("urgency", {}).get("urgency_level", "low")

        # ç®€å•æŸ¥è¯¢é€šå¸¸æ˜¯é¢„ç®—æ•æ„Ÿçš„
        if content_type == "simple_query":
            return "high"

        # é«˜ç´§æ€¥ç¨‹åº¦é™ä½é¢„ç®—æ•æ„Ÿåº¦
        if urgency_level in ["critical", "high"]:
            return "low"

        # ä¸€èˆ¬å†…å®¹ä¸ºä¸­ç­‰æ•æ„Ÿåº¦
        return "medium"

    def _apply_load_balancing(self, model_scores: dict[str, float]) -> dict[str, float]:
        """åº”ç”¨è´Ÿè½½å‡è¡¡"""
        balanced_scores = {}

        # è®¡ç®—æ¨¡å‹ä½¿ç”¨ç‡
        model_usage = self._calculate_model_usage()

        for model_name, score in model_scores.items():
            # è·å–æ¨¡å‹ä½¿ç”¨ç‡
            usage_rate = model_usage.get(model_name, 0.0)

            # è´Ÿè½½å‡è¡¡æƒé‡ï¼ˆä½¿ç”¨ç‡è¶Šé«˜ï¼Œæƒé‡è¶Šä½ï¼‰
            if usage_rate > 0.8:
                load_weight = 0.6  # é«˜è´Ÿè½½ï¼Œå¤§å¹…é™ä½åˆ†æ•°
            elif usage_rate > 0.6:
                load_weight = 0.8  # ä¸­ç­‰è´Ÿè½½ï¼Œé€‚åº¦é™ä½
            elif usage_rate > 0.4:
                load_weight = 0.9  # ä½è´Ÿè½½ï¼Œè½»å¾®é™ä½
            else:
                load_weight = 1.0  # æœªä½¿ç”¨ï¼Œä¿æŒåŸåˆ†

            # åº”ç”¨è´Ÿè½½å‡è¡¡
            balanced_score = score * load_weight
            balanced_scores[model_name] = balanced_score

            logger.debug(f"Model {model_name}: usage_rate={usage_rate:.3f}, load_weight={load_weight:.3f}, balanced_score={balanced_score:.3f}")

        return balanced_scores

    def _calculate_model_usage(self) -> dict[str, float]:
        """è®¡ç®—æ¨¡å‹ä½¿ç”¨ç‡"""
        usage_rates = {}
        total_requests = sum(m.get("total_requests", 0) for m in self.performance_metrics.values())

        if total_requests == 0:
            # å¦‚æœæ²¡æœ‰è¯·æ±‚ï¼Œæ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç‡ä¸º0
            for model_name in self.models:
                usage_rates[model_name] = 0.0
        else:
            for model_name, metrics in self.performance_metrics.items():
                request_count = metrics.get("total_requests", 0)
                usage_rates[model_name] = request_count / total_requests

        return usage_rates

    async def _intelligent_fallback_routing(
        self,
        input_text: str,
        content_analysis: dict[str, Any],
        current_scores: dict[str, float],
        model_availability: dict[str, bool]
    ) -> tuple[str, float]:
        """æ™ºèƒ½æ•…éšœè½¬ç§»è·¯ç”±"""
        logger.warning("ä¸»è·¯ç”±å¤±è´¥ï¼Œå¯ç”¨æ™ºèƒ½æ•…éšœè½¬ç§»")

        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        available_models = [name for name, available in model_availability.items() if available]

        if not available_models:
            # æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨ï¼Œè¿”å›é”™è¯¯
            raise Exception("æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨")

        # æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©å¤‡ç”¨ç­–ç•¥
        content_type = content_analysis.get("content_type", "general_conversation")

        # å®šä¹‰é™çº§ç­–ç•¥
        fallback_strategies = {
            "complex_reasoning": ["openai-gpt35", "claude-3-sonnet", "ollama-mistral"],
            "creative_writing": ["openai-gpt35", "claude-3-sonnet", "ollama-mistral"],
            "code_analysis": ["ollama-mistral", "openai-gpt35", "claude-3-sonnet"],
            "text_processing": ["openai-gpt35", "ollama-llama2", "ollama-mistral"],
            "simple_query": ["ollama-llama2", "openai-gpt35", "ollama-mistral"],
            "general_conversation": ["openai-gpt35", "ollama-llama2", "claude-3-sonnet"]
        }

        # è·å–é™çº§ç­–ç•¥
        strategy = fallback_strategies.get(content_type, fallback_strategies["general_conversation"])

        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„é™çº§æ¨¡å‹
        for model_name in strategy:
            if model_name in available_models:
                logger.info(f"æ•…éšœè½¬ç§»é€‰æ‹©æ¨¡å‹: {model_name}")
                return (model_name, current_scores.get(model_name, 0.5))

        # å¦‚æœç­–ç•¥ä¸­æ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œé€‰æ‹©åˆ†æ•°æœ€é«˜çš„å¯ç”¨æ¨¡å‹
        available_scores = {name: score for name, score in current_scores.items() if name in available_models}
        if available_scores:
            best_fallback = max(available_scores.items(), key=lambda x: x[1])
            logger.info(f"ç­–ç•¥å¤–æ•…éšœè½¬ç§»é€‰æ‹©æ¨¡å‹: {best_fallback[0]}")
            return (best_fallback[0], best_fallback[1])

        # æœ€åçš„é™çº§é€‰æ‹©
        fallback_model = available_models[0]
        logger.warning(f"æœ€åé™çº§é€‰æ‹©æ¨¡å‹: {fallback_model}")
        return (fallback_model, 0.3)

    async def _get_fallback_model(self) -> str:
        """è·å–é™çº§æ¨¡å‹"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å¥åº·æ£€æŸ¥å™¨
        if self.health_checker:
            healthy_models = [
                name for name, health in self.health_checker.health_status.items()
                if health.get("is_healthy", True)
            ]
            if healthy_models:
                # è¿”å›æœ€ç®€å•çš„æ¨¡å‹
                simple_models = ["ollama-llama2", "openai-gpt35", "ollama-mistral"]
                for model in simple_models:
                    if model in healthy_models:
                        return model
                return healthy_models[0]

        # æ²¡æœ‰å¥åº·æ£€æŸ¥å™¨æ—¶çš„é™çº§é€»è¾‘
        preferred_models = ["openai-gpt35", "ollama-llama2", "ollama-mistral"]
        for model in preferred_models:
            if model in self.models:
                return model

        # æœ€åçš„é™çº§
        return list(self.models.keys())[0]

    def _generate_detailed_reason(
        self,
        content_analysis: dict[str, Any],
        selected_model: str,
        confidence: float,
        all_scores: dict[str, float]
    ) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„å†³ç­–ç†ç”±"""
        content_type = content_analysis.get("content_type", "unknown")
        complexity = content_analysis.get("complexity_score", 0)
        urgency = content_analysis.get("urgency", {}).get("urgency_level", "low")

        # è·å–æ¨¡å‹ä¿¡æ¯
        model_config = self.model_configs.get(selected_model, {})
        capabilities = model_config.get("capabilities", [])

        # åˆ†æä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ¨¡å‹
        reasons = []

        # å†…å®¹åŒ¹é…åŸå› 
        if content_type in capabilities:
            reasons.append(f"å†…å®¹ç±»å‹åŒ¹é… ({content_type})")
        else:
            reasons.append(f"å†…å®¹ç±»å‹é€‚é… ({content_type} â†’ {capabilities})")

        # å¤æ‚åº¦é€‚é…åŸå› 
        if complexity > 0.7:
            reasons.append("é«˜å¤æ‚åº¦ä»»åŠ¡")
        elif complexity < 0.3:
            reasons.append("ç®€å•æŸ¥è¯¢ä¼˜åŒ–")
        else:
            reasons.append("ä¸­ç­‰å¤æ‚åº¦")

        # ç´§æ€¥ç¨‹åº¦åŸå› 
        if urgency in ["critical", "high"]:
            reasons.append(f"é«˜ç´§æ€¥åº¦ ({urgency})")

        # æ€§èƒ½åŸå› 
        if self.health_checker:
            health = self.health_checker.health_status.get(selected_model, {})
            success_rate = health.get("success_rate", 0)
            if success_rate > 0.9:
                reasons.append("é«˜æˆåŠŸç‡")
            elif success_rate > 0.8:
                reasons.append("ç¨³å®šæ€§èƒ½")

        # æˆæœ¬åŸå› 
        cost_per_token = model_config.get("cost_per_token", 0)
        if cost_per_token < 0.01:
            reasons.append("æˆæœ¬ä¼˜åŒ–")
        elif cost_per_token > 0.02:
            reasons.append("é«˜è´¨é‡ä¼˜å…ˆ")

        # ç»¼åˆç†ç”±
        detailed_reason = f"é€‰æ‹© {selected_model} æ¨¡å‹ï¼Œå› ä¸ºï¼š{', '.join(reasons)}ã€‚"
        detailed_reason += f" å†…å®¹ç±»å‹ï¼š{content_type}ï¼Œå¤æ‚åº¦ï¼š{complexity:.2f}ï¼Œç´§æ€¥åº¦ï¼š{urgency}ã€‚"
        detailed_reason += f" æ¨¡å‹ç½®ä¿¡åº¦ï¼š{confidence:.2%}"

        return detailed_reason

    def _analyze_content(self, input_text: str) -> dict[str, Any]:
        """å†…å®¹åˆ†æ"""
        input_lower = input_text.lower()

        # æ£€æµ‹å†…å®¹ç±»å‹ - æŒ‰ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥ï¼Œé¿å…è¯¯åˆ¤
        # 1. é¦–å…ˆæ£€æŸ¥æ˜ç¡®çš„ä»£ç ç›¸å…³å…³é”®è¯ï¼ˆéœ€è¦æ›´ç²¾ç¡®çš„åŒ¹é…ï¼‰
        code_keywords = ["ä»£ç ", "code", "ç¼–ç¨‹", "program", "ç¼–ç¨‹", "debug", "è°ƒè¯•", "å¼€å‘", "development"]
        if any(keyword in input_lower for keyword in code_keywords):
            # é¢å¤–æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯ä»£ç ç›¸å…³ï¼ˆé¿å…"ä»€ä¹ˆæ˜¯Python?"è¢«è¯¯åˆ¤ï¼‰
            # æ‰©å±•ä»£ç ç›¸å…³æ£€æµ‹è¯æ±‡ï¼ŒåŒ…æ‹¬"åˆ†æ"ã€"è¿™æ®µ"ç­‰ä¸Šä¸‹æ–‡è¯æ±‡
            code_context_terms = [
                "ç¼–ç¨‹è¯­è¨€", "programming language", "å†™ä»£ç ", "å¼€å‘", "ç¼–ç¨‹", "coding",
                "åˆ†æ", "è¿™æ®µ", "å‡½æ•°", "function", "ç±»", "class", "æ–¹æ³•", "method",
                "å˜é‡", "variable", "ç®—æ³•", "algorithm", "æ•°æ®ç»“æ„", "data structure"
            ]
            if any(term in input_lower for term in code_context_terms):
                content_type = "code_analysis"
                reason = "æ£€æµ‹åˆ°ä»£ç ç›¸å…³å†…å®¹ï¼Œé€‰æ‹©ä»£ç åˆ†ææ¨¡å‹"
            else:
                # å¯èƒ½æ˜¯ç®€å•çš„æ¦‚å¿µè¯¢é—®ï¼Œé™çº§ä¸ºç®€å•æŸ¥è¯¢
                content_type = "simple_query"
                reason = "æ£€æµ‹åˆ°æŠ€æœ¯æ¦‚å¿µè¯¢é—®ï¼Œé€‰æ‹©ç®€å•æŸ¥è¯¢æ¨¡å‹"
        # 2. æ£€æŸ¥å¤æ‚æ¨ç†ç›¸å…³å†…å®¹
        elif any(keyword in input_lower for keyword in ["æ•°å­¦", "math", "è®¡ç®—", "calculation", "é€»è¾‘", "logic", "æ¨ç†", "reasoning", "ç®—æ³•", "algorithm", "è¯æ˜", "proof"]):
            content_type = "complex_reasoning"
            reason = "æ£€æµ‹åˆ°å¤æ‚æ¨ç†å†…å®¹ï¼Œé€‰æ‹©é«˜è´¨é‡æ¨¡å‹"
        # 3. æ£€æŸ¥åˆ›æ„å†™ä½œç›¸å…³å†…å®¹
        elif any(keyword in input_lower for keyword in ["å†™ä½œ", "write", "åˆ›ä½œ", "creative", "æ•…äº‹", "story", "æ–‡æ¡ˆ", "copywriting", "å†™è¯—", "ä½œæ›²"]):
            content_type = "creative_writing"
            reason = "æ£€æµ‹åˆ°åˆ›æ„å†™ä½œå†…å®¹ï¼Œé€‰æ‹©åˆ›æ„èƒ½åŠ›å¼ºçš„æ¨¡å‹"
        # 4. æ£€æŸ¥æ–‡æœ¬å¤„ç†ç›¸å…³å†…å®¹
        elif any(keyword in input_lower for keyword in ["æ–‡æ¡£", "document", "text", "åˆ†æ", "analyze", "æ€»ç»“", "summarize", "ç¿»è¯‘", "translate", "æå–", "extract"]):
            content_type = "text_processing"
            reason = "æ£€æµ‹åˆ°æ–‡æœ¬å¤„ç†å†…å®¹ï¼Œé€‰æ‹©æ–‡æœ¬å¤„ç†æ¨¡å‹"
        # 5. æ£€æŸ¥ç®€å•æŸ¥è¯¢ç›¸å…³å†…å®¹
        elif any(keyword in input_lower for keyword in ["ç®€å•", "simple", "å¿«é€Ÿ", "quick", "åŸºæœ¬", "basic", "ä»€ä¹ˆæ˜¯", "ä»€ä¹ˆæ˜¯", "what is", "how to", "å¦‚ä½•", "æ€ä¹ˆ"]):
            content_type = "simple_query"
            reason = "æ£€æµ‹åˆ°ç®€å•æŸ¥è¯¢ï¼Œé€‰æ‹©å¿«é€Ÿå“åº”æ¨¡å‹"
        else:
            content_type = "general_conversation"
            reason = "é€šç”¨å¯¹è¯å†…å®¹ï¼Œé€‰æ‹©å¹³è¡¡æ€§èƒ½æ¨¡å‹"

        return {
            "content_type": content_type,
            "reason": reason,
            "input_length": len(input_text),
            "complexity_score": self._estimate_complexity(input_text)
        }

    def _estimate_complexity(self, input_text: str) -> float:
        """ä¼°ç®—å†…å®¹å¤æ‚åº¦"""
        length = len(input_text)
        lines = len(input_text.split('\n'))
        technical_words = len([w for w in input_text.split() if any(c in w for c in ['api', 'database', 'algorithm', 'function'])])

        # å¤æ‚åº¦è¯„åˆ† (0-1)
        complexity = min(1.0, (length * 0.001 + lines * 0.1 + technical_words * 0.05))
        return complexity

    def _score_models(self, content_analysis: dict[str, Any], model_availability: dict[str, bool] | None = None) -> dict[str, float]:
        """ä¸ºæ¨¡å‹è¯„åˆ†ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒæ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥ï¼‰"""
        content_type = content_analysis["content_type"]
        complexity = content_analysis["complexity_score"]

        scores = {}

        for model_name, config in self.model_configs.items():
            # æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
            if model_availability and not model_availability.get(model_name, True):
                scores[model_name] = 0.0  # ä¸å¯ç”¨çš„æ¨¡å‹å¾—åˆ†ä¸º0
                continue

            model_capabilities = set(config["capabilities"])

            # åŸºç¡€èƒ½åŠ›åŒ¹é…åˆ†
            capability_score = 0.0

            if content_type == "code_analysis":
                if "code_analysis" in model_capabilities:
                    capability_score = 0.9
                elif "technical_tasks" in model_capabilities:
                    capability_score = 0.7
                else:
                    capability_score = 0.3

            elif content_type == "complex_reasoning":
                if "complex_reasoning" in model_capabilities:
                    capability_score = 0.9
                elif "high_quality" in model_capabilities:
                    capability_score = 0.8
                else:
                    capability_score = 0.4

            elif content_type == "creative_writing":
                if "creative_writing" in model_capabilities:
                    capability_score = 0.9
                elif "high_quality" in model_capabilities:
                    capability_score = 0.7
                else:
                    capability_score = 0.3

            elif content_type == "text_processing":
                if "text_processing" in model_capabilities:
                    capability_score = 0.8
                elif "general_conversation" in model_capabilities:
                    capability_score = 0.6
                else:
                    capability_score = 0.4

            elif content_type == "simple_query":
                if "fast_response" in model_capabilities:
                    capability_score = 0.9
                elif "low_cost" in model_capabilities:
                    capability_score = 0.8
                else:
                    capability_score = 0.5

            else:  # general_conversation
                if "general_conversation" in model_capabilities:
                    capability_score = 0.8
                elif "balanced_performance" in model_capabilities:
                    capability_score = 0.7
                else:
                    capability_score = 0.5

            # å¤æ‚åº¦è°ƒæ•´
            if complexity > 0.7:  # é«˜å¤æ‚åº¦
                if "high_quality" in model_capabilities or "complex_reasoning" in model_capabilities:
                    capability_score *= 1.2
            elif complexity < 0.3:  # ä½å¤æ‚åº¦
                if "fast_response" in model_capabilities or "low_cost" in model_capabilities:
                    capability_score *= 1.1

            # å¥åº·çŠ¶æ€è°ƒæ•´ï¼ˆå¦‚æœæœ‰å¥åº·æ£€æŸ¥å™¨ï¼‰
            if self.health_checker and model_name in self.health_checker.health_status:
                health = self.health_checker.health_status[model_name]
                health_multiplier = health.get("success_rate", 1.0)
                capability_score *= health_multiplier

            scores[model_name] = capability_score

        return scores

    def _apply_performance_weights(self, model_scores: dict[str, float]) -> dict[str, float]:
        """åº”ç”¨æ€§èƒ½æƒé‡"""
        weighted_scores = {}

        for model_name, base_score in model_scores.items():
            metrics = self.performance_metrics[model_name]

            # æ€§èƒ½æƒé‡
            success_rate_weight = metrics["success_rate"]
            response_time_weight = max(0.1, 1.0 - (metrics["avg_response_time"] / 10.0))  # å“åº”æ—¶é—´è¶Šé•¿æƒé‡è¶Šä½
            cost_weight = max(0.1, 1.0 - (self.model_configs[model_name]["cost_per_token"] / 0.05))  # æˆæœ¬è¶Šé«˜è´¨é‡æƒè¶Šä½

            # ç»¼åˆæƒé‡
            performance_weight = (success_rate_weight * 0.5 + response_time_weight * 0.3 + cost_weight * 0.2)

            # åº”ç”¨æƒé‡
            weighted_score = base_score * performance_weight
            weighted_scores[model_name] = weighted_score

            logger.debug(f"Model {model_name}: base_score={base_score:.3f}, performance_weight={performance_weight:.3f}, weighted_score={weighted_score:.3f}")

        return weighted_scores

    def _calculate_confidence(self, weighted_scores: dict[str, float], best_score: float) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        if len(weighted_scores) == 1:
            return 0.9

        # è·å–ç¬¬äºŒé«˜çš„åˆ†æ•°
        sorted_scores = sorted(weighted_scores.values(), reverse=True)
        if len(sorted_scores) > 1:
            second_best_score = sorted_scores[1]
            # ç½®ä¿¡åº¦åŸºäºåˆ†æ•°å·®è·
            score_gap = best_score - second_best_score
            confidence = min(0.95, 0.5 + score_gap)
        else:
            confidence = 0.7

        return confidence

