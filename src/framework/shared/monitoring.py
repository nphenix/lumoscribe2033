"""
ç³»ç»Ÿç›‘æ§å’Œå¯è§‚æµ‹æ€§

æä¾›è¯¦ç»†çš„æ€§èƒ½ç›‘æ§ã€æŒ‡æ ‡æ”¶é›†å’Œå¯è§‚æµ‹æ€§åŠŸèƒ½
"""

import asyncio
import json
import os
import threading
import time
from collections import defaultdict, deque
from collections.abc import Callable
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from typing import Any, Optional

import psutil

from src.framework.shared.logging import get_logger

logger = get_logger(__name__)


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: str
    metric_name: str
    value: float
    unit: str
    tags: dict[str, str]
    metadata: dict[str, Any]


@dataclass
class SystemResourceMetric:
    """ç³»ç»Ÿèµ„æºæŒ‡æ ‡"""
    timestamp: str
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    disk_usage_percent: float
    network_sent_bytes: int
    network_recv_bytes: int


@dataclass
class TaskMetric:
    """ä»»åŠ¡æ‰§è¡ŒæŒ‡æ ‡"""
    timestamp: str
    task_name: str
    execution_time: float
    status: str  # success, failed, running
    queue_time: float
    worker_id: str
    retries: int


@dataclass
class ApiMetric:
    """API è¯·æ±‚æŒ‡æ ‡"""
    timestamp: str
    endpoint: str
    method: str
    status_code: int
    response_time: float
    request_size: int
    response_size: int
    client_ip: str


class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self, max_metrics: int = 10000):
        self.max_metrics = max_metrics
        self.performance_metrics: deque = deque(maxlen=max_metrics)
        self.system_metrics: deque = deque(maxlen=1000)
        self.task_metrics: deque = deque(maxlen=max_metrics)
        self.api_metrics: deque = deque(maxlen=max_metrics)

        self.system_stats = SystemStats()
        self.task_stats = TaskStats()
        self.api_stats = ApiStats()

        self._collectors: list[Callable] = []
        self._is_running = False
        self._monitoring_thread: threading.Thread | None = None

    def start(self, interval: int = 60):
        """å¯åŠ¨ç›‘æ§æ”¶é›†"""
        if self._is_running:
            return

        self._is_running = True
        self._monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            args=(interval,),
            daemon=True
        )
        self._monitoring_thread.start()
        logger.info("ğŸ“Š æŒ‡æ ‡æ”¶é›†å™¨å·²å¯åŠ¨")

    def stop(self):
        """åœæ­¢ç›‘æ§æ”¶é›†"""
        self._is_running = False
        if self._monitoring_thread:
            self._monitoring_thread.join(timeout=5)
        logger.info("ğŸ“Š æŒ‡æ ‡æ”¶é›†å™¨å·²åœæ­¢")

    def _monitoring_loop(self, interval: int):
        """ç›‘æ§å¾ªç¯"""
        while self._is_running:
            try:
                self._collect_system_metrics()
                self._collect_custom_metrics()
                time.sleep(interval)
            except Exception as e:
                logger.error(f"ç›‘æ§æ”¶é›†é”™è¯¯: {e}")
                time.sleep(5)  # é”™è¯¯åçŸ­æš‚ç­‰å¾…

    def _collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        try:
            # CPU å’Œå†…å­˜ä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()

            # ç£ç›˜ä½¿ç”¨ç‡
            disk = psutil.disk_usage('/')

            # ç½‘ç»œç»Ÿè®¡
            network = psutil.net_io_counters()

            metric = SystemResourceMetric(
                timestamp=datetime.now().isoformat(),
                cpu_percent=cpu_percent,
                memory_percent=memory.percent,
                memory_used_mb=memory.used / (1024 * 1024),
                disk_usage_percent=disk.percent,
                network_sent_bytes=network.bytes_sent,
                network_recv_bytes=network.bytes_recv
            )

            self.system_metrics.append(metric)
            self.system_stats.update(metric)

        except Exception as e:
            logger.error(f"ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")

    def _collect_custom_metrics(self):
        """æ”¶é›†è‡ªå®šä¹‰æŒ‡æ ‡"""
        for collector in self._collectors:
            try:
                collector(self)
            except Exception as e:
                logger.error(f"è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†å™¨é”™è¯¯: {e}")

    def add_collector(self, collector: Callable):
        """æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†å™¨"""
        self._collectors.append(collector)

    def record_task_metric(
        self,
        task_name: str,
        execution_time: float,
        status: str,
        queue_time: float = 0,
        worker_id: str = "",
        retries: int = 0
    ):
        """è®°å½•ä»»åŠ¡æ‰§è¡ŒæŒ‡æ ‡"""
        metric = TaskMetric(
            timestamp=datetime.now().isoformat(),
            task_name=task_name,
            execution_time=execution_time,
            status=status,
            queue_time=queue_time,
            worker_id=worker_id,
            retries=retries
        )

        self.task_metrics.append(metric)
        self.task_stats.update(metric)

    def record_api_metric(
        self,
        endpoint: str,
        method: str,
        status_code: int,
        response_time: float,
        request_size: int = 0,
        response_size: int = 0,
        client_ip: str = ""
    ):
        """è®°å½• API è¯·æ±‚æŒ‡æ ‡"""
        metric = ApiMetric(
            timestamp=datetime.now().isoformat(),
            endpoint=endpoint,
            method=method,
            status_code=status_code,
            response_time=response_time,
            request_size=request_size,
            response_size=response_size,
            client_ip=client_ip
        )

        self.api_metrics.append(metric)
        self.api_stats.update(metric)

    def record_performance_metric(
        self,
        metric_name: str,
        value: float,
        unit: str = "",
        tags: dict[str, str] = None,
        metadata: dict[str, Any] = None
    ):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        metric = PerformanceMetric(
            timestamp=datetime.now().isoformat(),
            metric_name=metric_name,
            value=value,
            unit=unit,
            tags=tags or {},
            metadata=metadata or {}
        )

        self.performance_metrics.append(metric)

    def get_system_summary(self, hours: int = 24) -> dict[str, Any]:
        """è·å–ç³»ç»Ÿèµ„æºæ‘˜è¦"""
        cutoff = datetime.now() - timedelta(hours=hours)
        recent_metrics = [
            m for m in self.system_metrics
            if datetime.fromisoformat(m.timestamp) > cutoff
        ]

        if not recent_metrics:
            return {}

        cpu_values = [m.cpu_percent for m in recent_metrics]
        memory_values = [m.memory_percent for m in recent_metrics]

        return {
            "time_range": f"æœ€è¿‘ {hours} å°æ—¶",
            "system_stats": {
                "avg_cpu": sum(cpu_values) / len(cpu_values),
                "max_cpu": max(cpu_values),
                "avg_memory": sum(memory_values) / len(memory_values),
                "max_memory": max(memory_values),
                "total_samples": len(recent_metrics)
            },
            "current_resources": self.system_stats.get_current_stats()
        }

    def get_task_summary(self, hours: int = 24) -> dict[str, Any]:
        """è·å–ä»»åŠ¡æ‰§è¡Œæ‘˜è¦"""
        cutoff = datetime.now() - timedelta(hours=hours)
        recent_metrics = [
            m for m in self.task_metrics
            if datetime.fromisoformat(m.timestamp) > cutoff
        ]

        return self.task_stats.get_summary(recent_metrics)

    def get_api_summary(self, hours: int = 24) -> dict[str, Any]:
        """è·å– API è¯·æ±‚æ‘˜è¦"""
        cutoff = datetime.now() - timedelta(hours=hours)
        recent_metrics = [
            m for m in self.api_metrics
            if datetime.fromisoformat(m.timestamp) > cutoff
        ]

        return self.api_stats.get_summary(recent_metrics)

    def get_health_status(self) -> dict[str, Any]:
        """è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        current_time = datetime.now()

        # æ£€æŸ¥æœ€è¿‘ 5 åˆ†é’Ÿçš„ç³»ç»ŸæŒ‡æ ‡
        recent_system_metrics = [
            m for m in self.system_metrics
            if datetime.fromisoformat(m.timestamp) > current_time - timedelta(minutes=5)
        ]

        # æ£€æŸ¥æœ€è¿‘ 5 åˆ†é’Ÿçš„ä»»åŠ¡æŒ‡æ ‡
        recent_task_metrics = [
            m for m in self.task_metrics
            if datetime.fromisoformat(m.timestamp) > current_time - timedelta(minutes=5)
        ]

        health_status = {
            "timestamp": current_time.isoformat(),
            "system_health": "healthy",
            "task_health": "healthy",
            "overall_health": "healthy"
        }

        # ç³»ç»Ÿå¥åº·æ£€æŸ¥
        if recent_system_metrics:
            latest_system = recent_system_metrics[-1]
            if latest_system.cpu_percent > 90:
                health_status["system_health"] = "critical"
            elif latest_system.memory_percent > 80:
                health_status["system_health"] = "warning"

        # ä»»åŠ¡å¥åº·æ£€æŸ¥
        if recent_task_metrics:
            failed_tasks = [m for m in recent_task_metrics if m.status == "failed"]
            if failed_tasks:
                failure_rate = len(failed_tasks) / len(recent_task_metrics)
                if failure_rate > 0.1:  # å¤±è´¥ç‡è¶…è¿‡ 10%
                    health_status["task_health"] = "critical"
                elif failure_rate > 0.05:  # å¤±è´¥ç‡è¶…è¿‡ 5%
                    health_status["task_health"] = "warning"

        # ç»¼åˆå¥åº·çŠ¶æ€
        if health_status["system_health"] == "critical" or health_status["task_health"] == "critical":
            health_status["overall_health"] = "critical"
        elif health_status["system_health"] == "warning" or health_status["task_health"] == "warning":
            health_status["overall_health"] = "warning"

        return health_status

    def export_metrics(self, output_dir: str = "logs/metrics") -> dict[str, str]:
        """å¯¼å‡ºæŒ‡æ ‡æ•°æ®åˆ°æ–‡ä»¶"""
        os.makedirs(output_dir, exist_ok=True)
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")

        exported_files = {}

        # å¯¼å‡ºç³»ç»ŸæŒ‡æ ‡
        if self.system_metrics:
            system_data = [asdict(m) for m in self.system_metrics]
            system_file = os.path.join(output_dir, f"system_metrics_{current_time}.json")
            with open(system_file, "w", encoding="utf-8") as f:
                json.dump(system_data, f, ensure_ascii=False, indent=2)
            exported_files["system"] = system_file

        # å¯¼å‡ºä»»åŠ¡æŒ‡æ ‡
        if self.task_metrics:
            task_data = [asdict(m) for m in self.task_metrics]
            task_file = os.path.join(output_dir, f"task_metrics_{current_time}.json")
            with open(task_file, "w", encoding="utf-8") as f:
                json.dump(task_data, f, ensure_ascii=False, indent=2)
            exported_files["task"] = task_file

        # å¯¼å‡º API æŒ‡æ ‡
        if self.api_metrics:
            api_data = [asdict(m) for m in self.api_metrics]
            api_file = os.path.join(output_dir, f"api_metrics_{current_time}.json")
            with open(api_file, "w", encoding="utf-8") as f:
                json.dump(api_data, f, ensure_ascii=False, indent=2)
            exported_files["api"] = api_file

        logger.info(f"ğŸ“Š æŒ‡æ ‡æ•°æ®å·²å¯¼å‡º: {exported_files}")
        return exported_files


class SystemStats:
    """ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""

    def __init__(self):
        self._cpu_history = deque(maxlen=100)
        self._memory_history = deque(maxlen=100)

    def update(self, metric: SystemResourceMetric):
        """æ›´æ–°ç³»ç»Ÿç»Ÿè®¡"""
        self._cpu_history.append(metric.cpu_percent)
        self._memory_history.append(metric.memory_percent)

    def get_current_stats(self) -> dict[str, float]:
        """è·å–å½“å‰ç³»ç»Ÿç»Ÿè®¡"""
        return {
            "cpu_percent": self._cpu_history[-1] if self._cpu_history else 0,
            "memory_percent": self._memory_history[-1] if self._memory_history else 0,
            "cpu_avg_10": sum(list(self._cpu_history)[-10:]) / min(10, len(self._cpu_history)),
            "memory_avg_10": sum(list(self._memory_history)[-10:]) / min(10, len(self._memory_history))
        }


class TaskStats:
    """ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯"""

    def __init__(self):
        self._task_counts = defaultdict(int)
        self._task_times = defaultdict(list)
        self._task_errors = defaultdict(list)

    def update(self, metric: TaskMetric):
        """æ›´æ–°ä»»åŠ¡ç»Ÿè®¡"""
        self._task_counts[f"{metric.task_name}_{metric.status}"] += 1
        self._task_times[metric.task_name].append(metric.execution_time)
        if metric.status == "failed":
            self._task_errors[metric.task_name].append(metric)

    def get_summary(self, metrics: list[TaskMetric]) -> dict[str, Any]:
        """è·å–ä»»åŠ¡æ‘˜è¦"""
        if not metrics:
            return {}

        # æŒ‰ä»»åŠ¡åç§°åˆ†ç»„
        task_groups = defaultdict(list)
        for metric in metrics:
            task_groups[metric.task_name].append(metric)

        summary = {}
        for task_name, task_metrics in task_groups.items():
            total_count = len(task_metrics)
            success_count = len([m for m in task_metrics if m.status == "success"])
            failed_count = len([m for m in task_metrics if m.status == "failed"])

            execution_times = [m.execution_time for m in task_metrics]

            summary[task_name] = {
                "total_count": total_count,
                "success_count": success_count,
                "failed_count": failed_count,
                "success_rate": (success_count / total_count * 100) if total_count > 0 else 0,
                "avg_execution_time": sum(execution_times) / len(execution_times) if execution_times else 0,
                "min_execution_time": min(execution_times) if execution_times else 0,
                "max_execution_time": max(execution_times) if execution_times else 0,
                "total_execution_time": sum(execution_times)
            }

        return summary


class ApiStats:
    """API ç»Ÿè®¡ä¿¡æ¯"""

    def __init__(self):
        self._endpoint_counts = defaultdict(int)
        self._status_counts = defaultdict(int)
        self._response_times = defaultdict(list)

    def update(self, metric: ApiMetric):
        """æ›´æ–° API ç»Ÿè®¡"""
        endpoint_key = f"{metric.method} {metric.endpoint}"
        self._endpoint_counts[endpoint_key] += 1
        self._status_counts[str(metric.status_code)] += 1
        self._response_times[endpoint_key].append(metric.response_time)

    def get_summary(self, metrics: list[ApiMetric]) -> dict[str, Any]:
        """è·å– API æ‘˜è¦"""
        if not metrics:
            return {}

        # æŒ‰ç«¯ç‚¹åˆ†ç»„
        endpoint_groups = defaultdict(list)
        for metric in metrics:
            endpoint_key = f"{metric.method} {metric.endpoint}"
            endpoint_groups[endpoint_key].append(metric)

        summary = {}
        for endpoint, endpoint_metrics in endpoint_groups.items():
            total_count = len(endpoint_metrics)
            status_codes = [m.status_code for m in endpoint_metrics]
            response_times = [m.response_time for m in endpoint_metrics]

            # çŠ¶æ€ç ç»Ÿè®¡
            status_counts = defaultdict(int)
            for code in status_codes:
                status_counts[str(code)] += 1

            summary[endpoint] = {
                "total_requests": total_count,
                "status_codes": dict(status_counts),
                "success_rate": (len([c for c in status_codes if 200 <= c < 300]) / total_count * 100) if total_count > 0 else 0,
                "avg_response_time": sum(response_times) / len(response_times) if response_times else 0,
                "min_response_time": min(response_times) if response_times else 0,
                "max_response_time": max(response_times) if response_times else 0,
                "total_response_time": sum(response_times)
            }

        return summary


# å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹
metrics_collector = MetricsCollector()


def get_metrics_collector() -> MetricsCollector:
    """è·å–å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹"""
    return metrics_collector


class EnhancedMetricsCollector:
    """å¢å¼ºçš„æŒ‡æ ‡æ”¶é›†å™¨ï¼Œé›†æˆ Redis ç¼“å­˜å’Œæ€§èƒ½ç›‘æ§"""

    def __init__(self):
        self.base_collector = metrics_collector
        self.cache_manager = None
        self.performance_metrics = defaultdict(list)
        self.alert_thresholds = {
            "cpu_warning": 70.0,
            "cpu_critical": 90.0,
            "memory_warning": 75.0,
            "memory_critical": 85.0,
            "disk_warning": 80.0,
            "disk_critical": 90.0,
            "response_time_warning": 2.0,
            "response_time_critical": 5.0,
            "error_rate_warning": 5.0,
            "error_rate_critical": 10.0
        }
        self.alert_history = deque(maxlen=1000)

    async def initialize(self):
        """åˆå§‹åŒ–å¢å¼ºæŒ‡æ ‡æ”¶é›†å™¨"""
        try:
            from src.framework.shared.redis_cache import get_cache_manager
            self.cache_manager = await get_cache_manager()
            logger.info("âœ… å¢å¼ºæŒ‡æ ‡æ”¶é›†å™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºæŒ‡æ ‡æ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

    async def collect_comprehensive_metrics(self) -> dict[str, Any]:
        """æ”¶é›†ç»¼åˆæŒ‡æ ‡"""
        try:
            # åŸºç¡€ç³»ç»ŸæŒ‡æ ‡
            system_metrics = self.base_collector.get_system_summary(hours=1)

            # ç¼“å­˜æŒ‡æ ‡
            cache_metrics = {}
            if self.cache_manager:
                cache_metrics = self.cache_manager.get_metrics()

            # åº”ç”¨æ€§èƒ½æŒ‡æ ‡
            app_metrics = self._collect_application_metrics()

            # å¥åº·çŠ¶æ€
            health_status = self.base_collector.get_health_status()
            cache_health = {}
            if self.cache_manager:
                cache_health = await self.cache_manager.health_check()

            # è­¦æŠ¥æ£€æŸ¥
            alerts = self._check_alerts(system_metrics, cache_metrics)

            # ç»¼åˆæŒ‡æ ‡
            comprehensive_metrics = {
                "timestamp": datetime.now().isoformat(),
                "system_metrics": system_metrics,
                "cache_metrics": cache_metrics,
                "application_metrics": app_metrics,
                "health_status": {
                    "overall": health_status["overall_health"],
                    "system": health_status["system_health"],
                    "tasks": health_status["task_health"],
                    "cache": cache_health.get("status", "unknown")
                },
                "alerts": alerts,
                "performance_trends": self._calculate_performance_trends(),
                "resource_utilization": self._calculate_resource_utilization()
            }

            # å­˜å‚¨åˆ° Redis ç¼“å­˜
            if self.cache_manager:
                await self.cache_manager.set(
                    "comprehensive_metrics",
                    comprehensive_metrics,
                    ttl=300  # 5åˆ†é’Ÿç¼“å­˜
                )

            return comprehensive_metrics

        except Exception as e:
            logger.error(f"âŒ ç»¼åˆæŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            return {"error": str(e), "timestamp": datetime.now().isoformat()}

    def _collect_application_metrics(self) -> dict[str, Any]:
        """æ”¶é›†åº”ç”¨çº§æŒ‡æ ‡"""
        try:
            # ä»»åŠ¡æ‰§è¡ŒæŒ‡æ ‡
            task_summary = self.base_collector.get_task_summary(hours=1)

            # API è¯·æ±‚æŒ‡æ ‡
            api_summary = self.base_collector.get_api_summary(hours=1)

            # æ€§èƒ½è®¡æ•°å™¨
            performance_counters = {
                "total_requests": sum(
                    summary.get("total_requests", 0)
                    for summary in api_summary.values()
                ),
                "successful_requests": sum(
                    summary.get("success_rate", 0) * summary.get("total_requests", 0) / 100
                    for summary in api_summary.values()
                ),
                "failed_requests": sum(
                    summary.get("total_requests", 0) * (100 - summary.get("success_rate", 0)) / 100
                    for summary in api_summary.values()
                ),
                "average_response_time": sum(
                    summary.get("avg_response_time", 0)
                    for summary in api_summary.values()
                ) / max(len(api_summary), 1),
                "total_tasks": sum(
                    summary.get("total_count", 0)
                    for summary in task_summary.values()
                ),
                "successful_tasks": sum(
                    summary.get("success_count", 0)
                    for summary in task_summary.values()
                ),
                "failed_tasks": sum(
                    summary.get("failed_count", 0)
                    for summary in task_summary.values()
                )
            }

            return {
                "task_summary": task_summary,
                "api_summary": api_summary,
                "performance_counters": performance_counters
            }

        except Exception as e:
            logger.error(f"âŒ åº”ç”¨æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            return {}

    def _check_alerts(self, system_metrics: dict, cache_metrics: dict) -> list[dict[str, Any]]:
        """æ£€æŸ¥è­¦æŠ¥æ¡ä»¶"""
        alerts = []
        current_time = datetime.now()

        try:
            # ç³»ç»Ÿ CPU è­¦æŠ¥
            system_stats = system_metrics.get("system_stats", {})
            if system_stats.get("avg_cpu", 0) > self.alert_thresholds["cpu_critical"]:
                alerts.append({
                    "type": "system",
                    "level": "critical",
                    "metric": "cpu_usage",
                    "value": system_stats.get("avg_cpu"),
                    "threshold": self.alert_thresholds["cpu_critical"],
                    "message": "CPU ä½¿ç”¨ç‡è¿‡é«˜",
                    "timestamp": current_time.isoformat()
                })
            elif system_stats.get("avg_cpu", 0) > self.alert_thresholds["cpu_warning"]:
                alerts.append({
                    "type": "system",
                    "level": "warning",
                    "metric": "cpu_usage",
                    "value": system_stats.get("avg_cpu"),
                    "threshold": self.alert_thresholds["cpu_warning"],
                    "message": "CPU ä½¿ç”¨ç‡è¾ƒé«˜",
                    "timestamp": current_time.isoformat()
                })

            # å†…å­˜è­¦æŠ¥
            if system_stats.get("avg_memory", 0) > self.alert_thresholds["memory_critical"]:
                alerts.append({
                    "type": "system",
                    "level": "critical",
                    "metric": "memory_usage",
                    "value": system_stats.get("avg_memory"),
                    "threshold": self.alert_thresholds["memory_critical"],
                    "message": "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜",
                    "timestamp": current_time.isoformat()
                })
            elif system_stats.get("avg_memory", 0) > self.alert_thresholds["memory_warning"]:
                alerts.append({
                    "type": "system",
                    "level": "warning",
                    "metric": "memory_usage",
                    "value": system_stats.get("avg_memory"),
                    "threshold": self.alert_thresholds["memory_warning"],
                    "message": "å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜",
                    "timestamp": current_time.isoformat()
                })

            # ç¼“å­˜è­¦æŠ¥
            cache_stats = cache_metrics.get("cache_metrics", {})
            if cache_stats.get("hit_rate", 0) < 50:
                alerts.append({
                    "type": "cache",
                    "level": "warning",
                    "metric": "hit_rate",
                    "value": cache_stats.get("hit_rate"),
                    "threshold": 50,
                    "message": "ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½",
                    "timestamp": current_time.isoformat()
                })

            # é”™è¯¯ç‡è­¦æŠ¥
            operations = cache_metrics.get("operations", {})
            if operations:
                total_ops = operations.get("total", 0)
                cache_errors = cache_stats.get("errors", 0)
                if total_ops > 0:
                    error_rate = (cache_errors / total_ops) * 100
                    if error_rate > self.alert_thresholds["error_rate_critical"]:
                        alerts.append({
                            "type": "cache",
                            "level": "critical",
                            "metric": "error_rate",
                            "value": error_rate,
                            "threshold": self.alert_thresholds["error_rate_critical"],
                            "message": "ç¼“å­˜é”™è¯¯ç‡è¿‡é«˜",
                            "timestamp": current_time.isoformat()
                        })
                    elif error_rate > self.alert_thresholds["error_rate_warning"]:
                        alerts.append({
                            "type": "cache",
                            "level": "warning",
                            "metric": "error_rate",
                            "value": error_rate,
                            "threshold": self.alert_thresholds["error_rate_warning"],
                            "message": "ç¼“å­˜é”™è¯¯ç‡è¾ƒé«˜",
                            "timestamp": current_time.isoformat()
                        })

            # è®°å½•è­¦æŠ¥å†å²
            for alert in alerts:
                self.alert_history.append(alert)

            return alerts

        except Exception as e:
            logger.error(f"âŒ è­¦æŠ¥æ£€æŸ¥å¤±è´¥: {e}")
            return []

    def _calculate_performance_trends(self) -> dict[str, Any]:
        """è®¡ç®—æ€§èƒ½è¶‹åŠ¿"""
        try:
            # è·å–æœ€è¿‘çš„æ€§èƒ½æ•°æ®
            recent_metrics = list(self.base_collector.performance_metrics)[-100:]  # æœ€è¿‘100ä¸ªæ•°æ®ç‚¹

            if len(recent_metrics) < 10:
                return {"trend": "insufficient_data"}

            # CPU è¶‹åŠ¿
            cpu_values = [m.value for m in recent_metrics if m.metric_name == "cpu_usage"]
            memory_values = [m.value for m in recent_metrics if m.metric_name == "memory_usage"]

            trends = {}

            # CPU è¶‹åŠ¿åˆ†æ
            if len(cpu_values) >= 10:
                recent_cpu = cpu_values[-10:]
                earlier_cpu = cpu_values[-20:-10] if len(cpu_values) >= 20 else cpu_values[:-10]

                if earlier_cpu:
                    recent_avg = sum(recent_cpu) / len(recent_cpu)
                    earlier_avg = sum(earlier_cpu) / len(earlier_cpu)

                    if recent_avg > earlier_avg * 1.1:  # 10% å¢é•¿
                        trends["cpu_trend"] = "increasing"
                    elif recent_avg < earlier_avg * 0.9:  # 10% å‡å°‘
                        trends["cpu_trend"] = "decreasing"
                    else:
                        trends["cpu_trend"] = "stable"
                else:
                    trends["cpu_trend"] = "insufficient_data"

            # å†…å­˜è¶‹åŠ¿åˆ†æ
            if len(memory_values) >= 10:
                recent_mem = memory_values[-10:]
                earlier_mem = memory_values[-20:-10] if len(memory_values) >= 20 else memory_values[:-10]

                if earlier_mem:
                    recent_avg = sum(recent_mem) / len(recent_mem)
                    earlier_avg = sum(earlier_mem) / len(earlier_mem)

                    if recent_avg > earlier_avg * 1.1:
                        trends["memory_trend"] = "increasing"
                    elif recent_avg < earlier_avg * 0.9:
                        trends["memory_trend"] = "decreasing"
                    else:
                        trends["memory_trend"] = "stable"
                else:
                    trends["memory_trend"] = "insufficient_data"

            return trends

        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½è¶‹åŠ¿è®¡ç®—å¤±è´¥: {e}")
            return {"trend": "calculation_error"}

    def _calculate_resource_utilization(self) -> dict[str, Any]:
        """è®¡ç®—èµ„æºåˆ©ç”¨ç‡"""
        try:
            system_summary = self.base_collector.get_system_summary(hours=24)
            system_stats = system_summary.get("system_stats", {})

            if not system_stats:
                return {"error": "no_system_data"}

            # èµ„æºåˆ©ç”¨ç‡åˆ†ç±»
            cpu_util = system_stats.get("avg_cpu", 0)
            memory_util = system_stats.get("avg_memory", 0)

            # åˆ©ç”¨ç‡ç­‰çº§
            def get_utilization_level(value, warning_threshold, critical_threshold):
                if value >= critical_threshold:
                    return "critical"
                elif value >= warning_threshold:
                    return "warning"
                elif value >= warning_threshold * 0.7:
                    return "moderate"
                else:
                    return "good"

            utilization = {
                "cpu": {
                    "current": cpu_util,
                    "level": get_utilization_level(
                        cpu_util,
                        self.alert_thresholds["cpu_warning"],
                        self.alert_thresholds["cpu_critical"]
                    ),
                    "status": "healthy" if cpu_util < self.alert_thresholds["cpu_warning"] else "degraded"
                },
                "memory": {
                    "current": memory_util,
                    "level": get_utilization_level(
                        memory_util,
                        self.alert_thresholds["memory_warning"],
                        self.alert_thresholds["memory_critical"]
                    ),
                    "status": "healthy" if memory_util < self.alert_thresholds["memory_warning"] else "degraded"
                }
            }

            # ç»¼åˆå¥åº·çŠ¶æ€
            critical_count = sum(
                1 for resource in utilization.values()
                if resource["level"] == "critical"
            )
            warning_count = sum(
                1 for resource in utilization.values()
                if resource["level"] == "warning"
            )

            if critical_count > 0:
                overall_status = "critical"
            elif warning_count > 0 or critical_count > 0:
                overall_status = "warning"
            else:
                overall_status = "healthy"

            utilization["overall"] = {
                "status": overall_status,
                "critical_resources": [name for name, resource in utilization.items() if resource["level"] == "critical"],
                "warning_resources": [name for name, resource in utilization.items() if resource["level"] == "warning"]
            }

            return utilization

        except Exception as e:
            logger.error(f"âŒ èµ„æºåˆ©ç”¨ç‡è®¡ç®—å¤±è´¥: {e}")
            return {"error": str(e)}

    def get_alert_history(self, hours: int = 24) -> list[dict[str, Any]]:
        """è·å–è­¦æŠ¥å†å²"""
        cutoff = datetime.now() - timedelta(hours=hours)
        return [
            alert for alert in self.alert_history
            if datetime.fromisoformat(alert["timestamp"]) > cutoff
        ]

    def update_alert_thresholds(self, **thresholds):
        """æ›´æ–°è­¦æŠ¥é˜ˆå€¼"""
        for key, value in thresholds.items():
            if key in self.alert_thresholds:
                self.alert_thresholds[key] = value
                logger.info(f"ğŸ”” è­¦æŠ¥é˜ˆå€¼å·²æ›´æ–°: {key} = {value}")


# å…¨å±€å¢å¼ºæŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹
enhanced_metrics_collector = EnhancedMetricsCollector()


async def get_enhanced_metrics_collector() -> EnhancedMetricsCollector:
    """è·å–å…¨å±€å¢å¼ºæŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹"""
    await enhanced_metrics_collector.initialize()
    return enhanced_metrics_collector
