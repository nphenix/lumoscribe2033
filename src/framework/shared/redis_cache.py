"""
Redis ç¼“å­˜ç®¡ç†å™¨

æä¾›é«˜æ€§èƒ½çš„ Redis ç¼“å­˜åŠŸèƒ½ï¼Œæ”¯æŒï¼š
- å¤šç§ç¼“å­˜ç­–ç•¥
- åˆ†å¸ƒå¼é”
- æ€§èƒ½æŒ‡æ ‡æ”¶é›†
- è¿æ¥æ± ç®¡ç†
- æ•…éšœè½¬ç§»
"""

import asyncio
import json
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Union

import redis.asyncio as redis
from redis.asyncio import ConnectionPool
from redis.exceptions import ConnectionError, RedisError

from src.framework.shared.logging import get_logger

logger = get_logger(__name__)


class CacheStrategy(str, Enum):
    """ç¼“å­˜ç­–ç•¥æšä¸¾"""
    LRU = "lru"  # æœ€è¿‘æœ€å°‘ä½¿ç”¨
    LFU = "lfu"  # æœ€å°‘ä½¿ç”¨é¢‘ç‡
    TTL = "ttl"   # ç”Ÿå­˜æ—¶é—´
    WRITE_THROUGH = "write_through"  # å†™é€
    WRITE_BACK = "write_back"  # å†™å›


@dataclass
class CacheMetrics:
    """ç¼“å­˜æŒ‡æ ‡"""
    hits: int = 0
    misses: int = 0
    sets: int = 0
    deletes: int = 0
    evictions: int = 0
    errors: int = 0
    total_get_time: float = 0.0
    total_set_time: float = 0.0

    @property
    def hit_rate(self) -> float:
        """ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.hits + self.misses
        return (self.hits / total * 100) if total > 0 else 0.0

    @property
    def avg_get_time(self) -> float:
        """å¹³å‡è·å–æ—¶é—´"""
        return self.total_get_time / self.hits if self.hits > 0 else 0.0

    @property
    def avg_set_time(self) -> float:
        """å¹³å‡è®¾ç½®æ—¶é—´"""
        return self.total_set_time / self.sets if self.sets > 0 else 0.0


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    value: Any
    created_at: float
    accessed_at: float
    access_count: int = 0
    ttl: int | None = None

    @property
    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        if self.ttl is None:
            return False
        return time.time() > self.created_at + self.ttl

    def touch(self):
        """æ›´æ–°è®¿é—®æ—¶é—´å’Œè®¡æ•°"""
        self.accessed_at = time.time()
        self.access_count += 1


class RedisCacheManager:
    """Redis ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(
        self,
        redis_url: str = "redis://localhost:6379/0",
        max_connections: int = 20,
        default_ttl: int = 3600,
        key_prefix: str = "lumoscribe:",
        enable_metrics: bool = True
    ):
        self.redis_url = redis_url
        self.max_connections = max_connections
        self.default_ttl = default_ttl
        self.key_prefix = key_prefix
        self.enable_metrics = enable_metrics

        # è¿æ¥æ± 
        self.connection_pool: ConnectionPool | None = None
        self.redis_client: redis.Redis | None = None

        # æŒ‡æ ‡
        self.metrics = CacheMetrics()

        # æœ¬åœ°ç¼“å­˜çƒ­ç‚¹æ•°æ®
        self._local_cache: dict[str, CacheEntry] = {}
        self._local_cache_size = 1000

        # åˆ†å¸ƒå¼é”
        self._locks: dict[str, asyncio.Lock] = {}

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            "total_operations": 0,
            "cache_efficiency": 0.0,
            "last_reset": time.time()
        }

    async def initialize(self) -> bool:
        """åˆå§‹åŒ– Redis è¿æ¥"""
        try:
            # åˆ›å»ºè¿æ¥æ± 
            self.connection_pool = ConnectionPool.from_url(
                self.redis_url,
                max_connections=self.max_connections,
                retry_on_timeout=True,
                socket_keepalive=True,
                socket_keepalive_options={},
                health_check_interval=30
            )

            # åˆ›å»º Redis å®¢æˆ·ç«¯
            self.redis_client = redis.Redis(
                connection_pool=self.connection_pool,
                decode_responses=True
            )

            # æµ‹è¯•è¿æ¥
            await self.redis_client.ping()

            logger.info(f"âœ… Redis ç¼“å­˜ç®¡ç†å™¨å·²åˆå§‹åŒ–: {self.redis_url}")
            return True

        except Exception as e:
            logger.error(f"âŒ Redis åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def close(self):
        """å…³é—­ Redis è¿æ¥"""
        try:
            if self.redis_client:
                await self.redis_client.aclose()  # ä½¿ç”¨ aclose() æ›¿ä»£ close()
            if self.connection_pool:
                await self.connection_pool.disconnect()
            logger.info("ğŸ”Œ Redis è¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.error(f"âŒ Redis å…³é—­å¤±è´¥: {e}")

    def _make_key(self, key: str) -> str:
        """ç”Ÿæˆå¸¦å‰ç¼€çš„é”®å"""
        return f"{self.key_prefix}{key}"

    async def get(self, key: str, default: Any = None) -> Any:
        """è·å–ç¼“å­˜å€¼"""
        start_time = time.time()

        try:
            # å…ˆæ£€æŸ¥æœ¬åœ°ç¼“å­˜
            local_key = self._make_key(key)
            if local_key in self._local_cache:
                entry = self._local_cache[local_key]
                if not entry.is_expired:
                    entry.touch()
                    self.metrics.hits += 1
                    self.metrics.total_get_time += time.time() - start_time
                    return entry.value
                else:
                    # è¿‡æœŸï¼Œä»æœ¬åœ°ç¼“å­˜åˆ é™¤
                    del self._local_cache[local_key]

            # ä» Redis è·å–
            if not self.redis_client:
                self.metrics.errors += 1
                logger.error("âŒ Redis å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
                return default

            redis_key = self._make_key(key)
            cached_data = await self.redis_client.get(redis_key)

            if cached_data is not None:
                try:
                    # å°è¯•è§£æ JSON
                    if cached_data.startswith('{') or cached_data.startswith('['):
                        value = json.loads(cached_data)
                    else:
                        value = cached_data

                    # æ›´æ–°æœ¬åœ°ç¼“å­˜
                    entry = CacheEntry(
                        value=value,
                        created_at=time.time(),
                        accessed_at=time.time(),
                        access_count=1
                    )
                    self._local_cache[local_key] = entry

                    # é™åˆ¶æœ¬åœ°ç¼“å­˜å¤§å°
                    if len(self._local_cache) > self._local_cache_size:
                        self._evict_lru_items()

                    self.metrics.hits += 1
                    self.metrics.total_get_time += time.time() - start_time
                    return value

                except json.JSONDecodeError:
                    # ä¸æ˜¯ JSONï¼Œç›´æ¥è¿”å›
                    entry = CacheEntry(
                        value=cached_data,
                        created_at=time.time(),
                        accessed_at=time.time(),
                        access_count=1
                    )
                    self._local_cache[local_key] = entry
                    self.metrics.hits += 1
                    self.metrics.total_get_time += time.time() - start_time
                    return cached_data

            # ç¼“å­˜æœªå‘½ä¸­
            self.metrics.misses += 1
            self.metrics.total_get_time += time.time() - start_time
            return default

        except Exception as e:
            self.metrics.errors += 1
            logger.error(f"âŒ ç¼“å­˜è·å–å¤±è´¥ [{key}]: {e}")
            return default

    async def set(
        self,
        key: str,
        value: Any,
        ttl: int | None = None,
        strategy: CacheStrategy = CacheStrategy.TTL
    ) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        start_time = time.time()

        try:
            if not self.redis_client:
                return False

            redis_key = self._make_key(key)
            ttl_value = ttl or self.default_ttl

            # åºåˆ—åŒ–å€¼
            if isinstance(value, (dict, list)):
                serialized_value = json.dumps(value, ensure_ascii=False)
            else:
                serialized_value = str(value)

            # è®¾ç½®åˆ° Redis
            success = await self.redis_client.setex(
                redis_key,
                ttl_value,
                serialized_value
            )

            if success:
                # æ›´æ–°æœ¬åœ°ç¼“å­˜
                entry = CacheEntry(
                    value=value,
                    created_at=time.time(),
                    accessed_at=time.time(),
                    access_count=1,
                    ttl=ttl_value
                )
                local_key = self._make_key(key)
                self._local_cache[local_key] = entry

                # é™åˆ¶æœ¬åœ°ç¼“å­˜å¤§å°
                if len(self._local_cache) > self._local_cache_size:
                    self._evict_lru_items()

                self.metrics.sets += 1
                self.metrics.total_set_time += time.time() - start_time
                return True
            else:
                self.metrics.errors += 1
                return False

        except Exception as e:
            self.metrics.errors += 1
            logger.error(f"âŒ ç¼“å­˜è®¾ç½®å¤±è´¥ [{key}]: {e}")
            return False

    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼"""
        try:
            if not self.redis_client:
                return False

            redis_key = self._make_key(key)

            # ä» Redis åˆ é™¤
            result = await self.redis_client.delete(redis_key)

            # ä»æœ¬åœ°ç¼“å­˜åˆ é™¤
            local_key = self._make_key(key)
            if local_key in self._local_cache:
                del self._local_cache[local_key]

            if result:
                self.metrics.deletes += 1
                return True
            return False

        except Exception as e:
            self.metrics.errors += 1
            logger.error(f"âŒ ç¼“å­˜åˆ é™¤å¤±è´¥ [{key}]: {e}")
            return False

    async def exists(self, key: str) -> bool:
        """æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨"""
        try:
            # å…ˆæ£€æŸ¥æœ¬åœ°ç¼“å­˜
            local_key = self._make_key(key)
            if local_key in self._local_cache:
                entry = self._local_cache[local_key]
                return not entry.is_expired

            # æ£€æŸ¥ Redis
            if not self.redis_client:
                return False

            redis_key = self._make_key(key)
            return bool(await self.redis_client.exists(redis_key))

        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜æ£€æŸ¥å¤±è´¥ [{key}]: {e}")
            return False

    async def clear(self, pattern: str | None = None) -> int:
        """æ¸…ç©ºç¼“å­˜"""
        try:
            if not self.redis_client:
                return 0

            if pattern:
                # æ¸…ç©ºåŒ¹é…æ¨¡å¼
                search_pattern = self._make_key(pattern)
                keys = await self.redis_client.keys(search_pattern)
                if keys:
                    deleted_count = await self.redis_client.delete(*keys)
                else:
                    deleted_count = 0
            else:
                # æ¸…ç©ºæ‰€æœ‰å¸¦å‰ç¼€çš„é”®
                search_pattern = self._make_key("*")
                keys = await self.redis_client.keys(search_pattern)
                if keys:
                    deleted_count = await self.redis_client.delete(*keys)
                else:
                    deleted_count = 0

            # æ¸…ç©ºæœ¬åœ°ç¼“å­˜
            if pattern:
                prefix = self._make_key("")
                self._local_cache = {
                    k: v for k, v in self._local_cache.items()
                    if not k.startswith(prefix) or not pattern or pattern in k
                }
            else:
                self._local_cache.clear()

            logger.info(f"ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©ºï¼Œåˆ é™¤äº† {deleted_count} ä¸ªé”®")
            return deleted_count

        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜æ¸…ç©ºå¤±è´¥: {e}")
            return 0

    async def get_ttl(self, key: str) -> int:
        """è·å–é”®çš„å‰©ä½™ç”Ÿå­˜æ—¶é—´"""
        try:
            if not self.redis_client:
                return -1

            redis_key = self._make_key(key)
            return await self.redis_client.ttl(redis_key)

        except Exception as e:
            logger.error(f"âŒ è·å– TTL å¤±è´¥ [{key}]: {e}")
            return -1

    async def acquire_lock(self, key: str, timeout: int = 10) -> bool:
        """è·å–åˆ†å¸ƒå¼é”"""
        try:
            if not self.redis_client:
                return False

            lock_key = self._make_key(f"lock:{key}")
            lock_value = f"{time.time()}:{id(asyncio.current_task())}"

            # ä½¿ç”¨ SET NX EX å®ç°åˆ†å¸ƒå¼é”
            result = await self.redis_client.set(
                lock_key,
                lock_value,
                ex=timeout,
                nx=True
            )

            if result:
                self._locks[key] = asyncio.Lock()
                logger.debug(f"ğŸ”’ è·å–é”æˆåŠŸ: {key}")
                return True
            else:
                logger.debug(f"âŒ è·å–é”å¤±è´¥: {key}")
                return False

        except Exception as e:
            logger.error(f"âŒ è·å–é”å¼‚å¸¸ [{key}]: {e}")
            return False

    async def release_lock(self, key: str) -> bool:
        """é‡Šæ”¾åˆ†å¸ƒå¼é”"""
        try:
            if not self.redis_client:
                return False

            lock_key = self._make_key(f"lock:{key}")

            # åªæœ‰é”çš„æŒæœ‰è€…æ‰èƒ½é‡Šæ”¾
            result = await self.redis_client.delete(lock_key)

            if key in self._locks:
                del self._locks[key]

            if result:
                logger.debug(f"ğŸ”“ é‡Šæ”¾é”æˆåŠŸ: {key}")
                return True
            else:
                logger.warning(f"âŒ é‡Šæ”¾é”å¤±è´¥: {key}")
                return False

        except Exception as e:
            logger.error(f"âŒ é‡Šæ”¾é”å¼‚å¸¸ [{key}]: {e}")
            return False

    def _evict_lru_items(self):
        """LRU æ·˜æ±°ç­–ç•¥"""
        if not self._local_cache:
            return

        # æŒ‰è®¿é—®æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„é¡¹
        sorted_items = sorted(
            self._local_cache.items(),
            key=lambda x: x[1].accessed_at
        )

        # åˆ é™¤æœ€æ—§çš„ 20% é¡¹
        evict_count = max(1, len(sorted_items) // 5)
        for i in range(evict_count):
            if i < len(sorted_items):
                key_to_remove = sorted_items[i][0]
                del self._local_cache[key_to_remove]
                self.metrics.evictions += 1

    async def get_info(self) -> dict[str, Any]:
        """è·å– Redis ä¿¡æ¯"""
        try:
            if not self.redis_client:
                return {}

            info = await self.redis_client.info()
            return {
                "redis_version": info.get("redis_version"),
                "used_memory": info.get("used_memory_human"),
                "connected_clients": info.get("connected_clients"),
                "total_commands_processed": info.get("total_commands_processed"),
                "keyspace_hits": info.get("keyspace_hits"),
                "keyspace_misses": info.get("keyspace_misses"),
                "uptime_in_seconds": info.get("uptime_in_seconds")
            }
        except Exception as e:
            logger.error(f"âŒ è·å– Redis ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def get_metrics(self) -> dict[str, Any]:
        """è·å–ç¼“å­˜æŒ‡æ ‡"""
        if not self.enable_metrics:
            return {}

        current_time = time.time()
        uptime = current_time - self._stats["last_reset"]

        return {
            "cache_metrics": {
                "hits": self.metrics.hits,
                "misses": self.metrics.misses,
                "sets": self.metrics.sets,
                "deletes": self.metrics.deletes,
                "evictions": self.metrics.evictions,
                "errors": self.metrics.errors,
                "hit_rate": self.metrics.hit_rate,
                "avg_get_time": self.metrics.avg_get_time,
                "avg_set_time": self.metrics.avg_set_time
            },
            "local_cache": {
                "size": len(self._local_cache),
                "max_size": self._local_cache_size,
                "utilization": len(self._local_cache) / self._local_cache_size * 100
            },
            "operations": {
                "total": self._stats["total_operations"],
                "per_second": self._stats["total_operations"] / uptime if uptime > 0 else 0,
                "uptime_seconds": uptime
            },
            "locks": {
                "active_count": len(self._locks),
                "active_keys": list(self._locks.keys())
            }
        }

    def reset_metrics(self):
        """é‡ç½®æŒ‡æ ‡"""
        self.metrics = CacheMetrics()
        self._stats = {
            "total_operations": 0,
            "cache_efficiency": 0.0,
            "last_reset": time.time()
        }
        logger.info("ğŸ“Š ç¼“å­˜æŒ‡æ ‡å·²é‡ç½®")

    async def health_check(self) -> dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        health_status = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "checks": {}
        }

        try:
            # æ£€æŸ¥ Redis è¿æ¥
            if self.redis_client:
                start_time = time.time()
                await self.redis_client.ping()
                response_time = time.time() - start_time
                health_status["checks"]["redis_connection"] = {
                    "status": "healthy",
                    "response_time": response_time
                }
            else:
                health_status["checks"]["redis_connection"] = {
                    "status": "unhealthy",
                    "error": "Redis å®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
                }
                health_status["status"] = "unhealthy"

            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            info = await self.get_info()
            if info.get("used_memory"):
                health_status["checks"]["memory_usage"] = {
                    "status": "healthy",
                    "used_memory": info["used_memory"]
                }

            # æ£€æŸ¥ç¼“å­˜æ•ˆç‡
            if self.metrics.hit_rate < 50:
                health_status["checks"]["cache_efficiency"] = {
                    "status": "warning",
                    "hit_rate": self.metrics.hit_rate,
                    "message": "ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½"
                }
                if health_status["status"] == "healthy":
                    health_status["status"] = "warning"
            else:
                health_status["checks"]["cache_efficiency"] = {
                    "status": "healthy",
                    "hit_rate": self.metrics.hit_rate
                }

            # æ£€æŸ¥é”™è¯¯ç‡
            total_ops = self.metrics.hits + self.metrics.misses + self.metrics.errors
            if total_ops > 0:
                error_rate = self.metrics.errors / total_ops * 100
                if error_rate > 5:
                    health_status["checks"]["error_rate"] = {
                        "status": "critical",
                        "error_rate": error_rate,
                        "message": "é”™è¯¯ç‡è¿‡é«˜"
                    }
                    health_status["status"] = "critical"
                elif error_rate > 1:
                    health_status["checks"]["error_rate"] = {
                        "status": "warning",
                        "error_rate": error_rate,
                        "message": "é”™è¯¯ç‡è¾ƒé«˜"
                    }
                    if health_status["status"] == "healthy":
                        health_status["status"] = "warning"
                else:
                    health_status["checks"]["error_rate"] = {
                        "status": "healthy",
                        "error_rate": error_rate
                    }

        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["checks"]["health_check_error"] = {
                "status": "error",
                "error": str(e)
            }

        return health_status


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
_cache_manager: RedisCacheManager | None = None


async def get_cache_manager() -> RedisCacheManager:
    """è·å–å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    global _cache_manager

    if _cache_manager is None:
        from src.framework.shared.config import settings
        _cache_manager = RedisCacheManager(
            redis_url=settings.ARQ_REDIS_URL,
            max_connections=20,
            default_ttl=3600,
            key_prefix="lumoscribe:cache:",
            enable_metrics=settings.METRICS_ENABLED
        )

        if not await _cache_manager.initialize():
            logger.error("âŒ ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥")

    return _cache_manager


async def close_cache_manager():
    """å…³é—­å…¨å±€ç¼“å­˜ç®¡ç†å™¨"""
    global _cache_manager
    if _cache_manager:
        await _cache_manager.close()
        _cache_manager = None
