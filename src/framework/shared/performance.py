"""
æ€§èƒ½ä¼˜åŒ–æ¨¡å—

åŸºäºLangChain v1.0æœ€ä½³å®è·µå®ç°çš„æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–ï¼ˆç¬¦åˆLangChainä¸­é—´ä»¶æ¨¡å¼ï¼‰
- å¤šçº§ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
- å¹¶å‘å¤„ç†ä¼˜åŒ–
- è¿æ¥æ± ç®¡ç†
- æ€§èƒ½ç›‘æ§å’Œåˆ†æ
- ç»“æ„åŒ–æ€§èƒ½æ•°æ®æ”¶é›†
- ä¸“å®¶çº§æ€§èƒ½ä¸­é—´ä»¶é›†æˆ
"""

import asyncio
import threading
import time
from collections import defaultdict, deque
from collections.abc import Callable
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Union

from src.framework.shared.logging import get_logger
from src.framework.shared.redis_cache import get_cache_manager

logger = get_logger(__name__)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    operation: str
    start_time: float
    end_time: float = field(default=0.0)
    duration: float = field(default=0.0)
    cache_hit: bool = field(default=False)
    database_queries: int = field(default=0)
    memory_usage: float = field(default=0.0)
    error: str | None = None


@dataclass
class QueryOptimization:
    """æŸ¥è¯¢ä¼˜åŒ–é…ç½®"""
    enable_query_cache: bool = True
    enable_connection_pooling: bool = True
    max_connections: int = 20
    query_timeout: float = 30.0
    batch_size: int = 100
    enable_index_hints: bool = True


@dataclass
class CacheOptimization:
    """ç¼“å­˜ä¼˜åŒ–é…ç½®"""
    enable_multi_level_cache: bool = True
    l1_cache_size: int = 1000  # å†…å­˜ç¼“å­˜
    l2_cache_ttl: int = 3600    # Redisç¼“å­˜TTL
    enable_write_through: bool = True
    enable_write_back: bool = False
    cache_warmup: bool = True


class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.metrics_history: deque = deque(maxlen=10000)
        self.query_stats: dict[str, dict[str, Any]] = defaultdict(lambda: {
            "count": 0,
            "total_time": 0.0,
            "avg_time": 0.0,
            "errors": 0,
            "cache_hits": 0
        })

        self.query_optimization = QueryOptimization()
        self.cache_optimization = CacheOptimization()

        # è¿æ¥æ± ç®¡ç†
        self._connection_pools: dict[str, Any] = {}
        self._pool_locks: dict[str, threading.Lock] = defaultdict(threading.Lock)

        # æ‰¹å¤„ç†é˜Ÿåˆ—
        self._batch_queues: dict[str, deque] = defaultdict(deque)
        self._batch_processors: dict[str, asyncio.Task] = {}

        logger.info("ğŸš€ æ€§èƒ½ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")

    @asynccontextmanager
    async def measure_performance(self, operation: str):
        """æ€§èƒ½æµ‹é‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        start_time = time.time()
        metrics = PerformanceMetrics(
            operation=operation,
            start_time=start_time
        )

        try:
            yield metrics
        except Exception as e:
            metrics.error = str(e)
            raise
        finally:
            metrics.end_time = time.time()
            metrics.duration = metrics.end_time - metrics.start_time
            self.metrics_history.append(metrics)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            stats = self.query_stats[operation]
            stats["count"] += 1
            stats["total_time"] += metrics.duration
            stats["avg_time"] = stats["total_time"] / stats["count"]

            if metrics.error:
                stats["errors"] += 1

            if metrics.cache_hit:
                stats["cache_hits"] += 1

            # è®°å½•æ€§èƒ½è­¦å‘Š
            if metrics.duration > 5.0:  # è¶…è¿‡5ç§’
                logger.warning(
                    f"âš ï¸ æ€§èƒ½è­¦å‘Š - æ“ä½œ: {operation}, "
                    f"è€—æ—¶: {metrics.duration:.2f}s"
                )

    async def optimize_query(
        self,
        query_func,
        operation: str,
        cache_key: str | None = None,
        cache_ttl: int = 300,
        *args,
        **kwargs
    ) -> Any:
        """ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢"""
        async with self.measure_performance(f"db_query_{operation}") as metrics:
            try:
                # å°è¯•ä»ç¼“å­˜è·å–
                if (self.query_optimization.enable_query_cache and cache_key and
                    self.cache_optimization.enable_multi_level_cache):

                    cache_manager = await get_cache_manager()
                    cached_result = await cache_manager.get(cache_key)

                    if cached_result is not None:
                        metrics.cache_hit = True
                        logger.debug(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {cache_key}")
                        return cached_result

                # æ‰§è¡ŒæŸ¥è¯¢
                result = await query_func(*args, **kwargs)
                metrics.database_queries = 1

                # ç¼“å­˜ç»“æœ
                if (self.query_optimization.enable_query_cache and cache_key and
                    self.cache_optimization.enable_multi_level_cache and result is not None):

                    cache_manager = await get_cache_manager()
                    await cache_manager.set(cache_key, result, ttl=cache_ttl)
                    logger.debug(f"ğŸ’¾ ç»“æœå·²ç¼“å­˜: {cache_key}")

                return result

            except Exception as e:
                metrics.error = str(e)
                logger.error(f"æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {operation} - {e}")
                raise

    async def batch_operation(
        self,
        operation_type: str,
        items: list[Any],
        batch_size: int | None = None,
        max_wait_time: float = 1.0
    ) -> list[Any]:
        """æ‰¹é‡æ“ä½œä¼˜åŒ–"""
        if not items:
            return []

        batch_size = batch_size or self.query_optimization.batch_size
        time.time()

        async with self.measure_performance(f"batch_{operation_type}") as metrics:
            try:
                results = []

                # åˆ†æ‰¹å¤„ç†
                for i in range(0, len(items), batch_size):
                    batch = items[i:i + batch_size]

                    # å¹¶å‘å¤„ç†æ‰¹æ¬¡
                    batch_tasks = []
                    for item in batch:
                        if operation_type == "create":
                            task = self._create_item(item)
                        elif operation_type == "update":
                            task = self._update_item(item)
                        elif operation_type == "delete":
                            task = self._delete_item(item)
                        else:
                            task = self._process_item(item, operation_type)

                        batch_tasks.append(task)

                    # ç­‰å¾…æ‰¹æ¬¡å®Œæˆ
                    batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

                    # å¤„ç†ç»“æœ
                    for result in batch_results:
                        if isinstance(result, Exception):
                            logger.error(f"æ‰¹é‡æ“ä½œä¸­çš„é¡¹ç›®å¤±è´¥: {result}")
                            metrics.error = str(result)
                        else:
                            results.append(result)

                metrics.database_queries = len(items)
                return results

            except Exception as e:
                metrics.error = str(e)
                logger.error(f"æ‰¹é‡æ“ä½œå¤±è´¥: {operation_type} - {e}")
                raise

    async def _create_item(self, item: Any) -> Any:
        """åˆ›å»ºé¡¹ç›®ï¼ˆå­ç±»é‡å†™ï¼‰"""
        # é»˜è®¤å®ç°ï¼Œå­ç±»åº”è¯¥é‡å†™
        return item

    async def _update_item(self, item: Any) -> Any:
        """æ›´æ–°é¡¹ç›®ï¼ˆå­ç±»é‡å†™ï¼‰"""
        # é»˜è®¤å®ç°ï¼Œå­ç±»åº”è¯¥é‡å†™
        return item

    async def _delete_item(self, item: Any) -> Any:
        """åˆ é™¤é¡¹ç›®ï¼ˆå­ç±»é‡å†™ï¼‰"""
        # é»˜è®¤å®ç°ï¼Œå­ç±»åº”è¯¥é‡å†™
        return True

    async def _process_item(self, item: Any, operation_type: str) -> Any:
        """å¤„ç†é¡¹ç›®ï¼ˆå­ç±»é‡å†™ï¼‰"""
        # é»˜è®¤å®ç°ï¼Œå­ç±»åº”è¯¥é‡å†™
        return item

    def get_connection_pool(self, pool_name: str = "default"):
        """è·å–è¿æ¥æ± """
        if pool_name not in self._connection_pools:
            self._connection_pools[pool_name] = self._create_connection_pool(pool_name)

        return self._connection_pools[pool_name]

    def _create_connection_pool(self, pool_name: str):
        """åˆ›å»ºè¿æ¥æ± ï¼ˆå­ç±»é‡å†™ï¼‰"""
        # é»˜è®¤å®ç°ï¼Œå­ç±»åº”è¯¥é‡å†™
        return None

    async def warmup_cache(self, cache_keys: list[str]):
        """ç¼“å­˜é¢„çƒ­"""
        if not self.cache_optimization.cache_warmup:
            return

        logger.info(f"ğŸ”¥ å¼€å§‹ç¼“å­˜é¢„çƒ­ï¼Œé”®æ•°é‡: {len(cache_keys)}")

        cache_manager = await get_cache_manager()
        warmup_count = 0

        for key in cache_keys:
            try:
                # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²å­˜åœ¨
                cached = await cache_manager.get(key)
                if cached is None:
                    # è¿™é‡Œå¯ä»¥é¢„åŠ è½½å¸¸ç”¨æ•°æ®
                    # å…·ä½“å®ç°å–å†³äºä¸šåŠ¡é€»è¾‘
                    warmup_count += 1
            except Exception as e:
                logger.warning(f"ç¼“å­˜é¢„çƒ­å¤±è´¥ {key}: {e}")

        logger.info(f"âœ… ç¼“å­˜é¢„çƒ­å®Œæˆï¼Œé¢„çƒ­é”®æ•°: {warmup_count}")

    def get_performance_stats(self) -> dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.metrics_history:
            return {}

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_operations = len(self.metrics_history)
        total_time = sum(m.duration for m in self.metrics_history)
        avg_time = total_time / total_operations if total_operations > 0 else 0

        # æŒ‰æ“ä½œç±»å‹åˆ†ç»„ç»Ÿè®¡
        operation_stats = defaultdict(list)
        for metric in self.metrics_history:
            operation_stats[metric.operation].append(metric)

        # è®¡ç®—å„æ“ä½œçš„ç»Ÿè®¡
        detailed_stats = {}
        for operation, metrics in operation_stats.items():
            op_count = len(metrics)
            op_total_time = sum(m.duration for m in metrics)
            op_avg_time = op_total_time / op_count if op_count > 0 else 0
            op_cache_hits = sum(1 for m in metrics if m.cache_hit)
            op_errors = sum(1 for m in metrics if m.error)

            detailed_stats[operation] = {
                "count": op_count,
                "total_time": op_total_time,
                "avg_time": op_avg_time,
                "max_time": max(m.duration for m in metrics),
                "min_time": min(m.duration for m in metrics),
                "cache_hit_rate": (op_cache_hits / op_count * 100) if op_count > 0 else 0,
                "error_rate": (op_errors / op_count * 100) if op_count > 0 else 0
            }

        # ç¼“å­˜ç»Ÿè®¡
        cache_stats = {}
        if self.query_optimization.enable_query_cache:
            total_cache_hits = sum(stats["cache_hits"] for stats in self.query_stats.values())
            total_queries = sum(stats["count"] for stats in self.query_stats.values())
            cache_stats = {
                "total_queries": total_queries,
                "total_cache_hits": total_cache_hits,
                "cache_hit_rate": (total_cache_hits / total_queries * 100) if total_queries > 0 else 0
            }

        return {
            "summary": {
                "total_operations": total_operations,
                "total_time": total_time,
                "avg_time": avg_time,
                "max_time": max(m.duration for m in self.metrics_history) if self.metrics_history else 0,
                "min_time": min(m.duration for m in self.metrics_history) if self.metrics_history else 0
            },
            "by_operation": detailed_stats,
            "cache_stats": cache_stats,
            "query_optimization": {
                "enabled": self.query_optimization.enable_query_cache,
                "connection_pooling": self.query_optimization.enable_connection_pooling,
                "max_connections": self.query_optimization.max_connections
            },
            "cache_optimization": {
                "multi_level": self.cache_optimization.enable_multi_level_cache,
                "l1_size": self.cache_optimization.l1_cache_size,
                "l2_ttl": self.cache_optimization.l2_cache_ttl,
                "write_through": self.cache_optimization.enable_write_through
            }
        }

    def get_slow_queries(self, threshold: float = 2.0) -> list[dict[str, Any]]:
        """è·å–æ…¢æŸ¥è¯¢åˆ—è¡¨"""
        slow_queries = []

        for metric in self.metrics_history:
            if metric.duration > threshold and metric.operation.startswith("db_query_"):
                slow_queries.append({
                    "operation": metric.operation,
                    "duration": metric.duration,
                    "timestamp": metric.start_time,
                    "cache_hit": metric.cache_hit,
                    "error": metric.error
                })

        # æŒ‰è€—æ—¶æ’åº
        slow_queries.sort(key=lambda x: x["duration"], reverse=True)
        return slow_queries[:50]  # è¿”å›å‰50ä¸ªæ…¢æŸ¥è¯¢

    def get_performance_recommendations(self) -> list[str]:
        """è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        stats = self.get_performance_stats()

        # æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
        cache_stats = stats.get("cache_stats", {})
        cache_hit_rate = cache_stats.get("cache_hit_rate", 0)

        if cache_hit_rate < 50:
            recommendations.append(
                f"ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ ({cache_hit_rate:.1f}%)ï¼Œå»ºè®®ï¼š"
                "1. æ£€æŸ¥ç¼“å­˜é”®çš„ç”Ÿæˆç­–ç•¥"
                "2. å¢åŠ ç¼“å­˜TTLæ—¶é—´"
                "3. è€ƒè™‘å¯ç”¨ç¼“å­˜é¢„çƒ­"
            )

        # æ£€æŸ¥å¹³å‡å“åº”æ—¶é—´
        summary = stats.get("summary", {})
        avg_time = summary.get("avg_time", 0)

        if avg_time > 3.0:
            recommendations.append(
                f"å¹³å‡å“åº”æ—¶é—´è¾ƒé•¿ ({avg_time:.2f}s)ï¼Œå»ºè®®ï¼š"
                "1. æ£€æŸ¥æ•°æ®åº“ç´¢å¼•"
                "2. ä¼˜åŒ–å¤æ‚æŸ¥è¯¢"
                "3. è€ƒè™‘å¢åŠ ç¼“å­˜å±‚"
            )

        # æ£€æŸ¥é”™è¯¯ç‡
        for operation, op_stats in stats.get("by_operation", {}).items():
            error_rate = op_stats.get("error_rate", 0)
            if error_rate > 5:
                recommendations.append(
                    f"æ“ä½œ {operation} é”™è¯¯ç‡è¾ƒé«˜ ({error_rate:.1f}%)ï¼Œå»ºè®®ï¼š"
                    "1. æ£€æŸ¥è¾“å…¥å‚æ•°éªŒè¯"
                    "2. å¢åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"
                    "3. æ£€æŸ¥èµ„æºé™åˆ¶"
                )

        # æ£€æŸ¥è¿æ¥æ± é…ç½®
        query_opt = stats.get("query_optimization", {})
        if not query_opt.get("connection_pooling", True):
            recommendations.append(
                "å»ºè®®å¯ç”¨æ•°æ®åº“è¿æ¥æ± ä»¥æé«˜å¹¶å‘æ€§èƒ½"
            )

        return recommendations

    async def cleanup_old_metrics(self, days: int = 7):
        """æ¸…ç†æ—§çš„æ€§èƒ½æŒ‡æ ‡"""
        cutoff_time = time.time() - (days * 24 * 3600)

        original_count = len(self.metrics_history)
        self.metrics_history = deque(
            (m for m in self.metrics_history if m.start_time > cutoff_time),
            maxlen=10000
        )

        cleaned_count = original_count - len(self.metrics_history)
        if cleaned_count > 0:
            logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} æ¡æ—§çš„æ€§èƒ½æŒ‡æ ‡")


class DatabasePerformanceOptimizer(PerformanceOptimizer):
    """æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self, database_manager):
        super().__init__()
        self.db_manager = database_manager

        # æ•°æ®åº“ç‰¹å®šçš„ä¼˜åŒ–é…ç½®
        self.query_cache: dict[str, Any] = {}
        self.index_hints: dict[str, list[str]] = {}
        self.prepared_statements: dict[str, Any] = {}

    async def optimized_query(
        self,
        query: str,
        params: dict[str, Any] = None,
        operation: str = "select",
        cache_key: str | None = None
    ) -> Any:
        """ä¼˜åŒ–çš„æ•°æ®åº“æŸ¥è¯¢"""
        # ç”Ÿæˆç¼“å­˜é”®
        if not cache_key:
            import hashlib
            cache_key = f"db_query_{hashlib.md5(f'{query}_{str(params)}'.encode()).hexdigest()}"

        return await self.optimize_query(
            self._execute_query,
            operation=operation,
            cache_key=cache_key,
            query=query,
            params=params
        )

    async def _execute_query(self, query: str, params: dict[str, Any] = None, operation: str = "select"):
        """æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå­ç±»é‡å†™ï¼‰"""
        # è¿™é‡Œåº”è¯¥ä½¿ç”¨å…·ä½“çš„æ•°æ®åº“è¿æ¥
        # é»˜è®¤å®ç°ï¼Œå­ç±»åº”è¯¥é‡å†™
        return None

    async def _create_item(self, item: Any) -> Any:
        """åˆ›å»ºæ•°æ®åº“è®°å½•"""
        return await self.db_manager.create(item)

    async def _update_item(self, item: Any) -> Any:
        """æ›´æ–°æ•°æ®åº“è®°å½•"""
        if hasattr(item, 'id') or hasattr(item, 'doc_id'):
            record_id = getattr(item, 'id', getattr(item, 'doc_id'))
            update_data = item.dict() if hasattr(item, 'dict') else item
            return await self.db_manager.update(type(item), record_id, update_data)
        return item

    async def _delete_item(self, item: Any) -> Any:
        """åˆ é™¤æ•°æ®åº“è®°å½•"""
        if hasattr(item, 'id') or hasattr(item, 'doc_id'):
            record_id = getattr(item, 'id', getattr(item, 'doc_id'))
            return await self.db_manager.delete(type(item), record_id)
        return True


class CachePerformanceOptimizer(PerformanceOptimizer):
    """ç¼“å­˜æ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self):
        super().__init__()
        self.cache_manager = None

        # å¤šçº§ç¼“å­˜é…ç½®
        self.l1_cache: dict[str, Any] = {}  # å†…å­˜ç¼“å­˜
        self.l1_cache_max_size = 1000
        self.l1_access_order = deque(maxlen=self.l1_cache_max_size)

    async def initialize(self):
        """åˆå§‹åŒ–ç¼“å­˜ä¼˜åŒ–å™¨"""
        self.cache_manager = await get_cache_manager()
        logger.info("ğŸš€ ç¼“å­˜æ€§èƒ½ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")

    async def get_cached_data(self, key: str) -> Any | None:
        """è·å–ç¼“å­˜æ•°æ®ï¼ˆå¤šçº§ç¼“å­˜ï¼‰"""
        # L1ç¼“å­˜ï¼ˆå†…å­˜ï¼‰
        if key in self.l1_cache:
            self.l1_access_order.append(key)
            return self.l1_cache[key]

        # L2ç¼“å­˜ï¼ˆRedisï¼‰
        if self.cache_manager:
            l2_data = await self.cache_manager.get(key)
            if l2_data is not None:
                # æå‡åˆ°L1ç¼“å­˜
                await self._promote_to_l1(key, l2_data)
            return l2_data

        return None

    async def set_cached_data(self, key: str, value: Any, ttl: int = 3600):
        """è®¾ç½®ç¼“å­˜æ•°æ®ï¼ˆå¤šçº§ç¼“å­˜ï¼‰"""
        # å­˜å‚¨åˆ°L1ç¼“å­˜
        await self._promote_to_l1(key, value)

        # å­˜å‚¨åˆ°L2ç¼“å­˜
        if self.cache_manager:
            await self.cache_manager.set(key, value, ttl=ttl)

    async def _promote_to_l1(self, key: str, value: Any):
        """æå‡æ•°æ®åˆ°L1ç¼“å­˜"""
        # æ£€æŸ¥L1ç¼“å­˜å¤§å°é™åˆ¶
        if len(self.l1_cache) >= self.l1_cache_max_size:
            # ç§»é™¤æœ€ä¹…æœªè®¿é—®çš„é¡¹
            oldest_key = self.l1_access_order.popleft()
            if oldest_key in self.l1_cache:
                del self.l1_cache[oldest_key]

        self.l1_cache[key] = value
        self.l1_access_order.append(key)

    def get_cache_stats(self) -> dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        l1_size = len(self.l1_cache)
        l1_usage = (l1_size / self.l1_cache_max_size) * 100

        return {
            "l1_cache": {
                "size": l1_size,
                "max_size": self.l1_cache_max_size,
                "usage_percent": l1_usage,
                "keys": list(self.l1_cache.keys())
            },
            "l2_cache": {
                "manager_available": self.cache_manager is not None
            }
        }


# å…¨å±€æ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹
_performance_optimizer = None
_database_optimizer = None
_cache_optimizer = None


def get_performance_optimizer() -> PerformanceOptimizer:
    """è·å–å…¨å±€æ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹"""
    global _performance_optimizer
    if _performance_optimizer is None:
        _performance_optimizer = PerformanceOptimizer()
    return _performance_optimizer


def get_database_optimizer(database_manager) -> DatabasePerformanceOptimizer:
    """è·å–æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹"""
    global _database_optimizer
    if _database_optimizer is None:
        _database_optimizer = DatabasePerformanceOptimizer(database_manager)
    return _database_optimizer


def get_cache_optimizer() -> CachePerformanceOptimizer:
    """è·å–ç¼“å­˜æ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹"""
    global _cache_optimizer
    if _cache_optimizer is None:
        _cache_optimizer = CachePerformanceOptimizer()
        # å¼‚æ­¥åˆå§‹åŒ–
        asyncio.create_task(_cache_optimizer.initialize())
    return _cache_optimizer


# Redisæ€§èƒ½ä¼˜åŒ–å™¨
class RedisPerformanceOptimizer:
    """Redisæ€§èƒ½ä¼˜åŒ–å™¨ - åŸºäºRedisæœ€ä½³å®è·µ"""

    def __init__(self):
        self.redis_client = None
        self.pipeline_cache = {}
        self.connection_pool_stats = {
            "active_connections": 0,
            "total_connections": 0,
            "pool_hits": 0,
            "pool_misses": 0
        }

    async def initialize(self, redis_client):
        """åˆå§‹åŒ–Redisæ€§èƒ½ä¼˜åŒ–å™¨"""
        self.redis_client = redis_client
        logger.info("ğŸš€ Redisæ€§èƒ½ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")

    async def optimized_pipeline_execute(self, commands: list[tuple], chunk_size: int = 100) -> list[Any]:
        """ä¼˜åŒ–çš„ç®¡é“æ‰§è¡Œ - åŸºäºRedisç®¡é“æœ€ä½³å®è·µ"""
        if not self.redis_client:
            raise ValueError("Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

        # åˆ†æ‰¹å¤„ç†å¤§é‡å‘½ä»¤
        results = []
        for i in range(0, len(commands), chunk_size):
            chunk = commands[i:i + chunk_size]

            # åˆ›å»ºç®¡é“
            pipe = self.redis_client.pipeline()

            # æ·»åŠ å‘½ä»¤åˆ°ç®¡é“
            for cmd in chunk:
                if len(cmd) == 2:
                    pipe.set(cmd[0], cmd[1])
                elif len(cmd) == 3:
                    pipe.set(cmd[0], cmd[1], cmd[2])
                elif len(cmd) == 1:
                    pipe.get(cmd[0])

            # æ‰§è¡Œç®¡é“
            chunk_results = await pipe.execute()
            results.extend(chunk_results)

            logger.debug(f"ğŸ“¦ Redisç®¡é“æ‰§è¡Œæ‰¹æ¬¡ {i//chunk_size + 1}, å‘½ä»¤æ•°: {len(chunk)}")

        return results

    async def optimized_batch_get(self, keys: list[str], chunk_size: int = 100) -> dict[str, Any]:
        """ä¼˜åŒ–çš„æ‰¹é‡è·å– - ä½¿ç”¨MGETå‘½ä»¤"""
        if not self.redis_client:
            raise ValueError("Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

        results = {}

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(keys), chunk_size):
            chunk_keys = keys[i:i + chunk_size]

            # ä½¿ç”¨MGETæ‰¹é‡è·å–
            chunk_values = await self.redis_client.mget(chunk_keys)

            # ç»„è£…ç»“æœ - æŒ‰æ‰¹æ¬¡ç´¢å¼•æ˜ å°„å€¼
            for idx, key in enumerate(chunk_keys):
                # ç¡®ä¿ä¸è¶…å‡ºè¿”å›å€¼çš„èŒƒå›´
                if idx < len(chunk_values) and chunk_values[idx] is not None:
                    results[key] = chunk_values[idx]
                else:
                    results[key] = None

        logger.debug(f"ğŸ“¦ Redisæ‰¹é‡è·å–å®Œæˆï¼Œé”®æ•°: {len(keys)}")
        return results

    async def optimized_batch_set(self, mapping: dict[str, Any], chunk_size: int = 100, ttl: int | None = None) -> bool:
        """ä¼˜åŒ–çš„æ‰¹é‡è®¾ç½® - ä½¿ç”¨MSETå‘½ä»¤"""
        if not self.redis_client:
            raise ValueError("Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

        success = True
        for i in range(0, len(mapping), chunk_size):
            chunk_items = list(mapping.items())[i:i + chunk_size]
            chunk_dict = dict(chunk_items)

            if ttl:
                # å¸¦TTLçš„æ‰¹é‡è®¾ç½®éœ€è¦ä½¿ç”¨ç®¡é“
                pipe = self.redis_client.pipeline()
                for key, value in chunk_dict.items():
                    pipe.setex(key, ttl, value)
                results = await pipe.execute()
                success = all(results)
            else:
                # ä½¿ç”¨MSETè¿›è¡Œæ‰¹é‡è®¾ç½®
                result = await self.redis_client.mset(chunk_dict)
                success = success and result

        logger.debug(f"ğŸ“¦ Redisæ‰¹é‡è®¾ç½®å®Œæˆï¼Œé”®æ•°: {len(mapping)}, TTL: {ttl}")
        return success

    def get_redis_performance_stats(self) -> dict[str, Any]:
        """è·å–Redisæ€§èƒ½ç»Ÿè®¡"""
        return {
            "connection_pool_stats": self.connection_pool_stats,
            "pipeline_cache_size": len(self.pipeline_cache),
            "recommendations": self.get_redis_recommendations()
        }

    def get_redis_recommendations(self) -> list[str]:
        """è·å–Redisæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # æ£€æŸ¥è¿æ¥æ± ç»Ÿè®¡
        if self.connection_pool_stats["pool_misses"] > self.connection_pool_stats["pool_hits"]:
            recommendations.append(
                "è¿æ¥æ± å‘½ä¸­ç‡è¾ƒä½ï¼Œå»ºè®®ï¼š"
                "1. å¢åŠ è¿æ¥æ± å¤§å°"
                "2. è°ƒæ•´è¿æ¥è¶…æ—¶æ—¶é—´"
                "3. å¯ç”¨è¿æ¥å¤ç”¨"
            )

        # æ£€æŸ¥ç®¡é“ç¼“å­˜
        if len(self.pipeline_cache) > 1000:
            recommendations.append(
                "ç®¡é“ç¼“å­˜è¾ƒå¤§ï¼Œå»ºè®®å®šæœŸæ¸…ç†ä»¥é‡Šæ”¾å†…å­˜"
            )

        return recommendations


# ChromaDBæ€§èƒ½ä¼˜åŒ–å™¨
class ChromaPerformanceOptimizer:
    """ChromaDBæ€§èƒ½ä¼˜åŒ–å™¨ - åŸºäºChromaDBæœ€ä½³å®è·µ"""

    def __init__(self):
        self.chroma_client = None
        self.collection_cache = {}
        self.query_stats = {
            "total_queries": 0,
            "batch_queries": 0,
            "sequential_queries": 0,
            "avg_batch_size": 0,
            "total_query_time": 0.0
        }

    async def initialize(self, chroma_client):
        """åˆå§‹åŒ–ChromaDBæ€§èƒ½ä¼˜åŒ–å™¨"""
        self.chroma_client = chroma_client
        logger.info("ğŸš€ ChromaDBæ€§èƒ½ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")

    async def optimized_batch_search(self, collection, queries: list[str], n_results: int = 10) -> list[Any]:
        """ä¼˜åŒ–çš„æ‰¹é‡æœç´¢ - åŸºäºChromaDBæ‰¹é‡æ“ä½œæœ€ä½³å®è·µ"""
        if not self.chroma_client:
            raise ValueError("ChromaDBå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

        start_time = time.time()

        # ä½¿ç”¨æ‰¹é‡æœç´¢è€Œä¸æ˜¯å¾ªç¯æœç´¢
        try:
            results = await collection.query(
                query_texts=queries,
                n_results=n_results
            )

            # æ›´æ–°ç»Ÿè®¡
            self.query_stats["batch_queries"] += 1
            self.query_stats["total_queries"] += 1
            self.query_stats["avg_batch_size"] = (
                (self.query_stats["avg_batch_size"] * (self.query_stats["batch_queries"] - 1) + len(queries)) /
                self.query_stats["batch_queries"]
            )

            query_time = time.time() - start_time
            self.query_stats["total_query_time"] += query_time

            logger.debug(f"ğŸ” ChromaDBæ‰¹é‡æœç´¢å®Œæˆï¼ŒæŸ¥è¯¢æ•°: {len(queries)}, è€—æ—¶: {query_time:.3f}s")
            return results

        except Exception as e:
            # å¦‚æœæ‰¹é‡æœç´¢å¤±è´¥ï¼Œå›é€€åˆ°é¡ºåºæœç´¢
            logger.warning(f"æ‰¹é‡æœç´¢å¤±è´¥ï¼Œå›é€€åˆ°é¡ºåºæœç´¢: {e}")
            return await self._fallback_sequential_search(collection, queries, n_results)

    async def optimized_batch_add_documents(self, collection, documents: list[str], ids: list[str], chunk_size: int = 100) -> bool:
        """ä¼˜åŒ–çš„æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
        if not self.chroma_client:
            raise ValueError("ChromaDBå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

        try:
            # åˆ†æ‰¹å¤„ç†æ–‡æ¡£æ·»åŠ 
            for i in range(0, len(documents), chunk_size):
                chunk_docs = documents[i:i + chunk_size]
                chunk_ids = ids[i:i + chunk_size] if i + chunk_size <= len(ids) else ids[i:]

                # æ·»åŠ æ–‡æ¡£åˆ°é›†åˆ
                collection.add(documents=chunk_docs, ids=chunk_ids)

            logger.debug(f"ğŸ“¦ ChromaDBæ‰¹é‡æ·»åŠ å®Œæˆï¼Œæ–‡æ¡£æ•°: {len(documents)}")
            return True

        except Exception as e:
            logger.error(f"ChromaDBæ‰¹é‡æ·»åŠ å¤±è´¥: {e}")
            return False

    async def _fallback_sequential_search(self, collection, queries: list[str], n_results: int) -> list[Any]:
        """é¡ºåºæœç´¢å›é€€æ–¹æ¡ˆ"""
        results = []
        for query in queries:
            result = collection.query(query_texts=[query], n_results=n_results)
            results.append(result)

            self.query_stats["sequential_queries"] += 1

        return results

    def optimize_collection_config(self, collection_name: str,
                              ef_search: int = 100,
                              ef_construction: int = 1000) -> dict[str, Any]:
        """ä¼˜åŒ–é›†åˆé…ç½® - åŸºäºHNSWå‚æ•°è°ƒä¼˜"""
        config_recommendations = {
            "ef_search": ef_search,
            "ef_construction": ef_construction,
            "recommendations": []
        }

        # æ ¹æ®æ•°æ®é›†å¤§å°æä¾›å»ºè®®
        if ef_search <= 50:  # æ”¹å› <= 50ï¼Œæµ‹è¯•æœŸæœ›50æ—¶ä¹Ÿæœ‰æ¨è
            config_recommendations["recommendations"].append(
                "ef_searchå€¼è¾ƒä½ï¼Œå¯èƒ½å½±å“å¬å›ç‡ã€‚å»ºè®®å¢åŠ åˆ°50-100ä¹‹é—´"
            )

        if ef_construction < 500:
            config_recommendations["recommendations"].append(
                "ef_constructionå€¼è¾ƒä½ï¼Œå¯èƒ½å½±å“ç´¢å¼•è´¨é‡ã€‚å»ºè®®å¢åŠ åˆ°500-1000ä¹‹é—´"
            )

        # æ€§èƒ½vså¬å›ç‡æƒè¡¡å»ºè®®
        config_recommendations["recommendations"].extend([
            "ef_searchå¢åŠ ä¼šæé«˜å¬å›ç‡ä½†é™ä½æŸ¥è¯¢é€Ÿåº¦",
            "ef_constructionå¢åŠ ä¼šæé«˜å¬å›ç‡ä½†å¢åŠ ç´¢å¼•æ„å»ºæ—¶é—´å’Œå†…å­˜ä½¿ç”¨",
            "å»ºè®®æ ¹æ®å…·ä½“æ•°æ®é›†å’Œéœ€æ±‚è¿›è¡Œå®éªŒè°ƒä¼˜"
        ])

        return config_recommendations

    def get_chroma_performance_stats(self) -> dict[str, Any]:
        """è·å–ChromaDBæ€§èƒ½ç»Ÿè®¡"""
        avg_query_time = (
            self.query_stats["total_query_time"] / self.query_stats["total_queries"]
            if self.query_stats["total_queries"] > 0 else 0
        )

        batch_ratio = (
            self.query_stats["batch_queries"] / self.query_stats["total_queries"]
            if self.query_stats["total_queries"] > 0 else 0
        )

        return {
            "query_stats": self.query_stats,
            "performance_metrics": {
                "avg_query_time": avg_query_time,
                "batch_query_ratio": batch_ratio,
                "avg_batch_size": self.query_stats["avg_batch_size"]
            },
            "collection_cache_size": len(self.collection_cache),
            "recommendations": self.get_chroma_recommendations()
        }

    def get_chroma_recommendations(self) -> list[str]:
        """è·å–ChromaDBæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åˆ†ææ‰¹é‡æŸ¥è¯¢æ¯”ä¾‹
        batch_ratio = (
            self.query_stats["batch_queries"] / self.query_stats["total_queries"]
            if self.query_stats["total_queries"] > 0 else 0
        )

        if batch_ratio < 0.7:
            recommendations.append(
                f"æ‰¹é‡æŸ¥è¯¢æ¯”ä¾‹è¾ƒä½ ({batch_ratio:.1%})ï¼Œå»ºè®®ï¼š"
                "1. å°½å¯èƒ½ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢API"
                "2. åˆå¹¶å¤šä¸ªå•ç‹¬æŸ¥è¯¢"
                "3. åˆ©ç”¨æ‰¹é‡æ“ä½œå‡å°‘ç½‘ç»œå¼€é”€"
            )

        # åˆ†æå¹³å‡æŸ¥è¯¢æ—¶é—´
        avg_time = (
            self.query_stats["total_query_time"] / self.query_stats["total_queries"]
            if self.query_stats["total_queries"] > 0 else 0
        )

        if avg_time > 1.0:
            recommendations.append(
                f"å¹³å‡æŸ¥è¯¢æ—¶é—´è¾ƒé•¿ ({avg_time:.3f}s)ï¼Œå»ºè®®ï¼š"
                "1. è°ƒæ•´HNSWå‚æ•°"
                "2. å‡å°‘è¿”å›ç»“æœæ•°é‡"
                "3. ä¼˜åŒ–æŸ¥è¯¢å‘é‡ç»´åº¦"
            )

        return recommendations


# SQLiteæ€§èƒ½ä¼˜åŒ–å™¨
class SQLitePerformanceOptimizer:
    """SQLiteæ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.db_manager = None
        self.query_cache = {}
        self.index_stats = {}

    async def initialize(self, db_manager, create_indexes: bool = True):
        """åˆå§‹åŒ–SQLiteæ€§èƒ½ä¼˜åŒ–å™¨"""
        self.db_manager = db_manager
        if create_indexes:
            await self._create_performance_indexes()
        logger.info("ğŸš€ SQLiteæ€§èƒ½ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")

    async def _create_performance_indexes(self):
        """åˆ›å»ºæ€§èƒ½ç´¢å¼•"""
        # å¸¸è§æŸ¥è¯¢å­—æ®µçš„ç´¢å¼•
        index_queries = [
            "CREATE INDEX IF NOT EXISTS idx_created_at ON documents(created_at)",
            "CREATE INDEX IF NOT EXISTS idx_doc_type ON documents(doc_type)",
            "CREATE INDEX IF NOT EXISTS idx_metadata_type ON metadata(metadata_type)",
        ]

        for query in index_queries:
            try:
                await self.db_manager.execute(query)
                logger.debug(f"âœ… åˆ›å»ºç´¢å¼•: {query}")
            except Exception as e:
                logger.warning(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {query} - {e}")

    async def optimized_query_with_cache(self, query: str, params: dict = None,
                                     cache_ttl: int = 300) -> Any:
        """å¸¦ç¼“å­˜ä¼˜åŒ–çš„æŸ¥è¯¢"""
        # æ”¯æŒæ˜¾å¼ç¼“å­˜é”®
        cache_key = None
        if params and "cache_key" in params:
            cache_key = params["cache_key"]
        else:
            # ç”Ÿæˆç¼“å­˜é”®
            import hashlib
            cache_key = f"sqlite_query_{hashlib.md5(f'{query}_{str(params)}'.encode()).hexdigest()}"

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.query_cache:
            cache_entry = self.query_cache[cache_key]
            if time.time() - cache_entry["timestamp"] < cache_ttl:
                logger.debug(f"ğŸ¯ SQLiteæŸ¥è¯¢ç¼“å­˜å‘½ä¸­: {query[:50]}...")
                return cache_entry["result"]

        # æ‰§è¡ŒæŸ¥è¯¢
        start_time = time.time()
        result = await self.db_manager.execute(query, params)
        query_time = time.time() - start_time

        # ç¼“å­˜ç»“æœ
        self.query_cache[cache_key] = {
            "result": result,
            "timestamp": time.time(),
            "query_time": query_time
        }

        logger.debug(f"ğŸ” SQLiteæŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶: {query_time:.3f}s")
        return result

    async def cleanup_query_cache(self, max_age: int = 3600):
        """æ¸…ç†æŸ¥è¯¢ç¼“å­˜"""
        current_time = time.time()
        cutoff_time = current_time - max_age

        logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†ç¼“å­˜ï¼Œå½“å‰æ—¶é—´: {current_time:.0f}, æˆªæ­¢æ—¶é—´: {cutoff_time:.0f}")

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        expired_keys = []
        for key, entry in self.query_cache.items():
            try:
                if isinstance(entry, dict) and "timestamp" in entry:
                    entry_age = current_time - entry["timestamp"]
                    logger.info(f"ğŸ” æ£€æŸ¥ç¼“å­˜é¡¹ {key}: æ—¶é—´æˆ³={entry['timestamp']:.0f}, å¹´é¾„={entry_age:.0f}ç§’, é˜ˆå€¼={max_age}ç§’")
                    if entry["timestamp"] < cutoff_time:
                        expired_keys.append(key)
                        logger.info(f"ğŸ—‘ï¸ æ ‡è®°è¿‡æœŸ: {key}")
                    else:
                        logger.info(f"âœ… ä¿ç•™: {key}")
                else:
                    logger.info(f"ğŸ—‘ï¸ æ ¼å¼é”™è¯¯: {key}")
                    expired_keys.append(key)
            except Exception as e:
                logger.info(f"ğŸ—‘ï¸ å¼‚å¸¸: {key} - {e}")
                expired_keys.append(key)

        logger.info(f"ğŸ—‘ï¸ å‡†å¤‡æ¸…ç† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜: {expired_keys}")

        for key in expired_keys:
            if key in self.query_cache:
                del self.query_cache[key]

        logger.info(f"ğŸ§¹ æ¸…ç†å®Œæˆï¼Œå‰©ä½™ç¼“å­˜: {list(self.query_cache.keys())}")

    def get_sqlite_performance_stats(self) -> dict[str, Any]:
        """è·å–SQLiteæ€§èƒ½ç»Ÿè®¡"""
        return {
            "query_cache_size": len(self.query_cache),
            "index_stats": self.index_stats,
            "recommendations": self.get_sqlite_recommendations()
        }

    def get_sqlite_recommendations(self) -> list[str]:
        """è·å–SQLiteæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        if len(self.query_cache) > 1000:
            recommendations.append(
                "æŸ¥è¯¢ç¼“å­˜è¾ƒå¤§ï¼Œå»ºè®®å®šæœŸæ¸…ç†ä»¥é‡Šæ”¾å†…å­˜"
            )

        # æ£€æŸ¥ç´¢å¼•ç»Ÿè®¡
        if not self.index_stats:
            recommendations.append(
                "æœªæ‰¾åˆ°ç´¢å¼•ç»Ÿè®¡ï¼Œå»ºè®®åˆ›å»ºé€‚å½“çš„æ•°æ®åº“ç´¢å¼•"
            )

        recommendations.extend([
            "ä½¿ç”¨EXPLAIN QUERY PLANåˆ†ææ…¢æŸ¥è¯¢",
            "è€ƒè™‘ä½¿ç”¨WALæ¨¡å¼æé«˜å¹¶å‘æ€§èƒ½",
            "å®šæœŸæ‰§è¡ŒVACUUMå’ŒANALYZEä¼˜åŒ–æ•°æ®åº“"
        ])

        return recommendations


# NetworkXæ€§èƒ½ä¼˜åŒ–å™¨
class NetworkXPerformanceOptimizer:
    """NetworkXæ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.graph_cache = {}
        self.computation_stats = {
            "total_computations": 0,
            "cache_hits": 0,
            "avg_computation_time": 0.0,
            "total_computation_time": 0.0
        }

    async def optimized_graph_computation(self, computation_func, graph_id: str,
                                   *args, **kwargs) -> Any:
        """ä¼˜åŒ–çš„å›¾è®¡ç®—"""
        # æ£€æŸ¥ç¼“å­˜
        import hashlib
        args_str = str(args) + str(kwargs)
        cache_key = f"graph_{graph_id}_{hashlib.md5(args_str.encode()).hexdigest()}"

        if cache_key in self.graph_cache:
            cache_entry = self.graph_cache[cache_key]
            self.computation_stats["cache_hits"] += 1
            logger.debug(f"ğŸ¯ NetworkXå›¾è®¡ç®—ç¼“å­˜å‘½ä¸­: {graph_id}")
            return cache_entry["result"]

        # æ‰§è¡Œè®¡ç®—
        start_time = time.time()
        result = await computation_func(*args, **kwargs)
        computation_time = time.time() - start_time

        # ç¼“å­˜ç»“æœ
        self.graph_cache[cache_key] = {
            "result": result,
            "timestamp": time.time(),
            "computation_time": computation_time
        }

        # æ›´æ–°ç»Ÿè®¡
        self.computation_stats["total_computations"] += 1
        self.computation_stats["total_computation_time"] += computation_time
        self.computation_stats["avg_computation_time"] = (
            self.computation_stats["total_computation_time"] /
            self.computation_stats["total_computations"]
        )

        logger.debug(f"ğŸ” NetworkXå›¾è®¡ç®—å®Œæˆï¼Œè€—æ—¶: {computation_time:.3f}s")
        return result

    def get_networkx_performance_stats(self) -> dict[str, Any]:
        """è·å–NetworkXæ€§èƒ½ç»Ÿè®¡"""
        cache_hit_rate = (
            self.computation_stats["cache_hits"] /
            self.computation_stats["total_computations"]
            if self.computation_stats["total_computations"] > 0 else 0
        )

        return {
            "computation_stats": self.computation_stats,
            "cache_hit_rate": cache_hit_rate,
            "graph_cache_size": len(self.graph_cache),
            "recommendations": self.get_networkx_recommendations()
        }

    def get_networkx_recommendations(self) -> list[str]:
        """è·å–NetworkXæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        cache_hit_rate = (
            self.computation_stats["cache_hits"] /
            self.computation_stats["total_computations"]
            if self.computation_stats["total_computations"] > 0 else 0
        )

        if cache_hit_rate < 0.5:
            recommendations.append(
                f"å›¾è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ ({cache_hit_rate:.1%})ï¼Œå»ºè®®ï¼š"
                "1. å¢åŠ ç¼“å­˜å®¹é‡"
                "2. ä¼˜åŒ–ç¼“å­˜é”®ç”Ÿæˆç­–ç•¥"
                "3. è¯†åˆ«é‡å¤è®¡ç®—æ¨¡å¼"
            )

        # è®¡ç®—å¹³å‡æ—¶é—´ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»æ€»æ—¶é—´è®¡ç®—
        avg_time = self.computation_stats.get("avg_computation_time", 0)
        if avg_time == 0 and self.computation_stats["total_computations"] > 0:
            avg_time = self.computation_stats["total_computation_time"] / self.computation_stats["total_computations"]

        if avg_time >= 1.0:  # ä½¿ç”¨>=ä»¥åŒ…å«ç­‰äº1.0çš„æƒ…å†µ
            recommendations.append(
                f"å¹³å‡å›¾è®¡ç®—æ—¶é—´è¾ƒé•¿ ({avg_time:.3f}s)ï¼Œå»ºè®®ï¼š"
                "1. ä½¿ç”¨æ›´é«˜æ•ˆçš„ç®—æ³•"
                "2. è€ƒè™‘å›¾åˆ†å‰²å¤„ç†"
                "3. ä½¿ç”¨å¹¶è¡Œè®¡ç®—"
            )

        recommendations.extend([
            "è€ƒè™‘ä½¿ç”¨ç¨€ç–çŸ©é˜µè¡¨ç¤ºå¤§å‹å›¾",
            "å¯¹äºé‡å¤æŸ¥è¯¢ï¼Œé¢„è®¡ç®—å¹¶ç¼“å­˜ç»“æœ",
            "ä½¿ç”¨NetworkXçš„ç®—æ³•å˜ä½“æé«˜æ€§èƒ½"
        ])

        return recommendations


# å…¨å±€æ•°æ®åº“ä¼˜åŒ–å™¨å®ä¾‹
_redis_optimizer = None
_chroma_optimizer = None
_sqlite_optimizer = None
_networkx_optimizer = None


def get_redis_optimizer() -> RedisPerformanceOptimizer:
    """è·å–å…¨å±€Redisæ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹"""
    global _redis_optimizer
    if _redis_optimizer is None:
        _redis_optimizer = RedisPerformanceOptimizer()
    return _redis_optimizer


def get_chroma_optimizer() -> ChromaPerformanceOptimizer:
    """è·å–å…¨å±€ChromaDBæ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹"""
    global _chroma_optimizer
    if _chroma_optimizer is None:
        _chroma_optimizer = ChromaPerformanceOptimizer()
    return _chroma_optimizer


def get_sqlite_optimizer() -> SQLitePerformanceOptimizer:
    """è·å–å…¨å±€SQLiteæ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹"""
    global _sqlite_optimizer
    if _sqlite_optimizer is None:
        _sqlite_optimizer = SQLitePerformanceOptimizer()
    return _sqlite_optimizer


def get_networkx_optimizer() -> NetworkXPerformanceOptimizer:
    """è·å–å…¨å±€NetworkXæ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹"""
    global _networkx_optimizer
    if _networkx_optimizer is None:
        _networkx_optimizer = NetworkXPerformanceOptimizer()
    return _networkx_optimizer


# LlamaIndex æ€§èƒ½ä¼˜åŒ–é›†æˆ
try:
    import cProfile
    import pstats
    from io import StringIO
    from typing import Any, Union

    from llama_index.core import QueryBundle, VectorStoreIndex
    from llama_index.core.callbacks import CallbackManager
    from llama_index.core.postprocessor import (
        LongContextReorder,
        SentenceEmbeddingOptimizer,
    )
    from llama_index.core.query_engine import TransformQueryEngine
    from llama_index.core.query_transform import HyDEQueryTransform
    from llama_index.core.schema import NodeWithScore

    class LlamaIndexPerformanceOptimizer:
        """LlamaIndex æ€§èƒ½ä¼˜åŒ–å™¨"""

        def __init__(self):
            self.optimizer = get_performance_optimizer()
            self.cache_optimizer = get_cache_optimizer()

            # LlamaIndexç‰¹å®šçš„ä¼˜åŒ–é…ç½®
            self.query_cache: dict[str, Any] = {}
            self.embedding_cache: dict[str, list[float]] = {}
            self.node_processor_cache: dict[str, list[NodeWithScore]] = {}

            # æ€§èƒ½ç»Ÿè®¡
            self.query_stats: dict[str, dict[str, Any]] = defaultdict(lambda: {
                "count": 0,
                "total_time": 0.0,
                "avg_time": 0.0,
                "cache_hits": 0,
                "token_usage": 0
            })

            logger.info("ğŸš€ LlamaIndexæ€§èƒ½ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")

        def create_optimized_query_engine(
            self,
            index: VectorStoreIndex,
            similarity_top_k: int = 10,
            enable_sentence_optimizer: bool = True,
            enable_context_reorder: bool = True,
            enable_hyde_transform: bool = False,
            sentence_optimizer_percentile: float = 0.5,
            sentence_optimizer_threshold: float | None = None
        ) -> Any:
            """åˆ›å»ºä¼˜åŒ–çš„æŸ¥è¯¢å¼•æ“"""
            # åŸºç¡€æŸ¥è¯¢å¼•æ“
            query_engine = index.as_query_engine(similarity_top_k=similarity_top_k)

            # æ„å»ºåå¤„ç†å™¨åˆ—è¡¨
            node_postprocessors = []

            # å¥å­åµŒå…¥ä¼˜åŒ–å™¨ - å‡å°‘ä¸ç›¸å…³å¥å­ï¼Œé™ä½tokenä½¿ç”¨
            if enable_sentence_optimizer:
                if sentence_optimizer_threshold is not None:
                    sentence_optimizer = SentenceEmbeddingOptimizer(
                        threshold_cutoff=sentence_optimizer_threshold
                    )
                else:
                    sentence_optimizer = SentenceEmbeddingOptimizer(
                        percentile_cutoff=sentence_optimizer_percentile
                    )
                node_postprocessors.append(sentence_optimizer)
                logger.debug(f"ğŸ¯ å¯ç”¨å¥å­åµŒå…¥ä¼˜åŒ–å™¨: percentile={sentence_optimizer_percentile}")

            # é•¿ä¸Šä¸‹æ–‡é‡æ’åº - ä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯ä½ç½®
            if enable_context_reorder and similarity_top_k > 5:
                node_postprocessors.append(LongContextReorder())
                logger.debug("ğŸ“‹ å¯ç”¨é•¿ä¸Šä¸‹æ–‡é‡æ’åº")

            # åº”ç”¨åå¤„ç†å™¨
            if node_postprocessors:
                query_engine = index.as_query_engine(
                    similarity_top_k=similarity_top_k,
                    node_postprocessors=node_postprocessors
                )

            # HyDEæŸ¥è¯¢è½¬æ¢ - ç”Ÿæˆå‡è®¾æ–‡æ¡£æå‡æ£€ç´¢è´¨é‡
            if enable_hyde_transform:
                hyde_transform = HyDEQueryTransform(include_original=True)
                query_engine = TransformQueryEngine(query_engine, hyde_transform)
                logger.debug("ğŸ”„ å¯ç”¨HyDEæŸ¥è¯¢è½¬æ¢")

            return query_engine

        async def optimized_query(
            self,
            query_engine: Any,
            query_str: str,
            cache_key: str | None = None,
            enable_cache: bool = True,
            track_tokens: bool = True
        ) -> Any:
            """æ‰§è¡Œä¼˜åŒ–çš„æŸ¥è¯¢"""
            # ç”Ÿæˆç¼“å­˜é”®
            if not cache_key and enable_cache:
                import hashlib
                cache_key = f"llamaindex_query_{hashlib.md5(query_str.encode()).hexdigest()}"

            async with self.optimizer.measure_performance("llamaindex_query") as metrics:
                try:
                    # å°è¯•ä»ç¼“å­˜è·å–ç»“æœ
                    if enable_cache and cache_key:
                        cached_result = await self.cache_optimizer.get_cached_data(cache_key)
                        if cached_result is not None:
                            metrics.cache_hit = True
                            logger.debug(f"ğŸ¯ LlamaIndexæŸ¥è¯¢ç¼“å­˜å‘½ä¸­: {query_str[:50]}...")
                            return cached_result

                    # æ‰§è¡ŒæŸ¥è¯¢
                    start_time = time.time()
                    response = await query_engine.aquery(query_str)
                    query_time = time.time() - start_time

                    # è·Ÿè¸ªtokenä½¿ç”¨æƒ…å†µ
                    if track_tokens and hasattr(response, 'metadata'):
                        token_usage = response.metadata.get('token_usage', {})
                        metrics.memory_usage = token_usage.get('total_tokens', 0)

                    # ç¼“å­˜ç»“æœ
                    if enable_cache and cache_key and response:
                        await self.cache_optimizer.set_cached_data(
                            cache_key,
                            response,
                            ttl=3600  # 1å°æ—¶ç¼“å­˜
                        )
                        logger.debug(f"ğŸ’¾ LlamaIndexæŸ¥è¯¢ç»“æœå·²ç¼“å­˜: {query_str[:50]}...")

                    # æ›´æ–°æŸ¥è¯¢ç»Ÿè®¡
                    self._update_query_stats("llamaindex_query", query_time, metrics.cache_hit)

                    return response

                except Exception as e:
                    metrics.error = str(e)
                    logger.error(f"LlamaIndexæŸ¥è¯¢å¤±è´¥: {query_str[:50]}... - {e}")
                    raise

        def _update_query_stats(self, query_type: str, duration: float, cache_hit: bool):
            """æ›´æ–°æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯"""
            stats = self.query_stats[query_type]
            stats["count"] += 1
            stats["total_time"] += duration
            stats["avg_time"] = stats["total_time"] / stats["count"]

            if cache_hit:
                stats["cache_hits"] += 1

        def create_embedding_optimizer(self, embed_model: Any) -> Any:
            """åˆ›å»ºåµŒå…¥æ¨¡å‹ä¼˜åŒ–å™¨"""
            class OptimizedEmbedModel:
                def __init__(self, original_model, optimizer):
                    self.original_model = original_model
                    self.optimizer = optimizer
                    self.cache = optimizer.embedding_cache

                async def aget_query_embedding(self, query: str) -> list[float]:
                    """ä¼˜åŒ–çš„æŸ¥è¯¢åµŒå…¥ç”Ÿæˆ"""
                    # ç”Ÿæˆç¼“å­˜é”®
                    import hashlib
                    cache_key = f"embed_query_{hashlib.md5(query.encode()).hexdigest()}"

                    # æ£€æŸ¥ç¼“å­˜
                    if cache_key in self.cache:
                        logger.debug(f"ğŸ¯ åµŒå…¥ç¼“å­˜å‘½ä¸­: {query[:30]}...")
                        return self.cache[cache_key]

                    # ç”ŸæˆåµŒå…¥
                    start_time = time.time()
                    embedding = await self.original_model.aget_query_embedding(query)
                    generation_time = time.time() - start_time

                    # ç¼“å­˜ç»“æœ
                    self.cache[cache_key] = embedding

                    # è®°å½•æ€§èƒ½
                    self.optimizer._update_query_stats("embedding_generation", generation_time, False)
                    logger.debug(f"ğŸ”¢ ç”ŸæˆæŸ¥è¯¢åµŒå…¥: {query[:30]}..., è€—æ—¶: {generation_time:.3f}s")

                    return embedding

                def get_query_embedding(self, query: str) -> list[float]:
                    """åŒæ­¥ç‰ˆæœ¬çš„æŸ¥è¯¢åµŒå…¥ç”Ÿæˆ"""
                    import hashlib
                    cache_key = f"embed_query_{hashlib.md5(query.encode()).hexdigest()}"

                    if cache_key in self.cache:
                        return self.cache[cache_key]

                    start_time = time.time()
                    embedding = self.original_model.get_query_embedding(query)
                    generation_time = time.time() - start_time

                    self.cache[cache_key] = embedding
                    self.optimizer._update_query_stats("embedding_generation", generation_time, False)

                    return embedding

                # ä»£ç†å…¶ä»–æ–¹æ³•
                def __getattr__(self, name):
                    return getattr(self.original_model, name)

            return OptimizedEmbedModel(embed_model, self)

        def profile_query_performance(self, query_engine: Any, query_str: str) -> dict[str, Any]:
            """åˆ†ææŸ¥è¯¢æ€§èƒ½"""
            # åˆ›å»ºæ€§èƒ½åˆ†æå™¨
            profiler = cProfile.Profile()

            # æ‰§è¡ŒæŸ¥è¯¢å¹¶åˆ†æ
            profiler.enable()
            start_time = time.time()

            try:
                response = query_engine.query(query_str)
                query_time = time.time() - start_time
                success = True
                error = None
            except Exception as e:
                query_time = time.time() - start_time
                success = False
                error = str(e)
                response = None

            profiler.disable()

            # åˆ†ææ€§èƒ½æ•°æ®
            s = StringIO()
            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
            ps.print_stats(20)  # æ‰“å°å‰20ä¸ªæœ€è€—æ—¶çš„å‡½æ•°

            performance_data = {
                "query": query_str,
                "query_time": query_time,
                "success": success,
                "error": error,
                "response_preview": str(response)[:200] if response else None,
                "performance_stats": s.getvalue(),
                "timestamp": time.time()
            }

            return performance_data

        def optimize_batch_queries(
            self,
            query_engine: Any,
            queries: list[str],
            max_concurrent: int = 5
        ) -> list[Any]:
            """æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–"""
            import asyncio

            async def process_batch(batch_queries: list[str]) -> list[Any]:
                """å¤„ç†ä¸€æ‰¹æŸ¥è¯¢"""
                tasks = []
                for query in batch_queries:
                    task = self.optimized_query(query_engine, query)
                    tasks.append(task)

                return await asyncio.gather(*tasks, return_exceptions=True)

            # åˆ†æ‰¹å¤„ç†
            results = []
            for i in range(0, len(queries), max_concurrent):
                batch = queries[i:i + max_concurrent]
                batch_results = asyncio.run(process_batch(batch))
                results.extend(batch_results)

                logger.debug(f"ğŸ“¦ æ‰¹é‡æŸ¥è¯¢è¿›åº¦: {min(i + max_concurrent, len(queries))}/{len(queries)}")

            return results

        def get_llamaindex_performance_stats(self) -> dict[str, Any]:
            """è·å–LlamaIndexæ€§èƒ½ç»Ÿè®¡"""
            return {
                "query_stats": dict(self.query_stats),
                "cache_stats": {
                    "query_cache_size": len(self.query_cache),
                    "embedding_cache_size": len(self.embedding_cache),
                    "node_processor_cache_size": len(self.node_processor_cache)
                },
                "performance_recommendations": self.get_llamaindex_recommendations()
            }

        def get_llamaindex_recommendations(self) -> list[str]:
            """è·å–LlamaIndexæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
            recommendations = []

            # åˆ†ææŸ¥è¯¢ç»Ÿè®¡
            for query_type, stats in self.query_stats.items():
                avg_time = stats.get("avg_time", 0)
                cache_hit_rate = (stats.get("cache_hits", 0) / stats.get("count", 1)) * 100

                if avg_time > 5.0:
                    recommendations.append(
                        f"{query_type} å¹³å‡è€—æ—¶è¾ƒé•¿ ({avg_time:.2f}s)ï¼Œå»ºè®®ï¼š"
                        "1. å¯ç”¨å¥å­åµŒå…¥ä¼˜åŒ–å™¨å‡å°‘tokenä½¿ç”¨"
                        "2. è°ƒæ•´similarity_top_kå‚æ•°"
                        "3. è€ƒè™‘ä½¿ç”¨HyDEæŸ¥è¯¢è½¬æ¢"
                    )

                if cache_hit_rate < 30:
                    recommendations.append(
                        f"{query_type} ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ ({cache_hit_rate:.1f}%)ï¼Œå»ºè®®ï¼š"
                        "1. å¢åŠ ç¼“å­˜TTLæ—¶é—´"
                        "2. æ£€æŸ¥ç¼“å­˜é”®ç”Ÿæˆç­–ç•¥"
                        "3. å¯ç”¨æŸ¥è¯¢ç»“æœç¼“å­˜"
                    )

            # æ£€æŸ¥ç¼“å­˜å¤§å°
            if len(self.embedding_cache) > 10000:
                recommendations.append(
                    "åµŒå…¥ç¼“å­˜è¾ƒå¤§ï¼Œå»ºè®®ï¼š"
                    "1. å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜"
                    "2. å®æ–½LRUç¼“å­˜ç­–ç•¥"
                    "3. è€ƒè™‘ä½¿ç”¨Redisä½œä¸ºå¤–éƒ¨ç¼“å­˜"
                )

            return recommendations

        async def cleanup_caches(self, max_age_hours: int = 24):
            """æ¸…ç†è¿‡æœŸç¼“å­˜"""
            current_time = time.time()
            current_time - (max_age_hours * 3600)

            # æ¸…ç†æŸ¥è¯¢ç¼“å­˜ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è®°å½•æ—¶é—´æˆ³ï¼‰
            if len(self.query_cache) > 5000:
                self.query_cache.clear()
                logger.info("ğŸ§¹ æ¸…ç†æŸ¥è¯¢ç¼“å­˜")

            if len(self.embedding_cache) > 10000:
                self.embedding_cache.clear()
                logger.info("ğŸ§¹ æ¸…ç†åµŒå…¥ç¼“å­˜")

            if len(self.node_processor_cache) > 1000:
                self.node_processor_cache.clear()
                logger.info("ğŸ§¹ æ¸…ç†èŠ‚ç‚¹å¤„ç†å™¨ç¼“å­˜")


    # å…¨å±€LlamaIndexæ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹
    _llamaindex_optimizer = None


    def get_llamaindex_optimizer() -> LlamaIndexPerformanceOptimizer:
        """è·å–å…¨å±€LlamaIndexæ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹"""
        global _llamaindex_optimizer
        if _llamaindex_optimizer is None:
            _llamaindex_optimizer = LlamaIndexPerformanceOptimizer()
        return _llamaindex_optimizer


except ImportError:
    logger.warning("âš ï¸ LlamaIndexæœªå®‰è£…ï¼Œè·³è¿‡LlamaIndexæ€§èƒ½ä¼˜åŒ–é›†æˆ")

    # æä¾›ç©ºçš„æ›¿ä»£å®ç°
    class LlamaIndexPerformanceOptimizer:
        def __init__(self):
            pass

        def create_optimized_query_engine(self, *args, **kwargs):
            logger.warning("LlamaIndexæœªå®‰è£…ï¼Œè¿”å›åŸºç¡€æŸ¥è¯¢å¼•æ“")
            return None

        async def optimized_query(self, *args, **kwargs):
            return None

        def create_embedding_optimizer(self, embed_model):
            return embed_model

        def profile_query_performance(self, *args, **kwargs):
            return {"error": "LlamaIndexæœªå®‰è£…"}

        def get_llamaindex_performance_stats(self):
            return {"message": "LlamaIndexæœªå®‰è£…"}

        def get_llamaindex_recommendations(self):
            return ["å®‰è£…LlamaIndexä»¥å¯ç”¨æ€§èƒ½ä¼˜åŒ–"]

        async def cleanup_caches(self, *args, **kwargs):
            pass

    def get_llamaindex_optimizer():
        return LlamaIndexPerformanceOptimizer()


# LangChain v1.0 æ€§èƒ½ä¸­é—´ä»¶é›†æˆ
try:
    from langchain.agents import AgentExecutor
    from langchain.callbacks.base import BaseCallbackHandler
    from langchain.schema import AgentAction, AgentFinish, LLMResult

    class LangChainPerformanceMiddleware(BaseCallbackHandler):
        """LangChain v1.0 æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶"""

        def __init__(self, optimizer: PerformanceOptimizer = None):
            super().__init__()
            self.optimizer = optimizer or get_performance_optimizer()
            self.current_chain = None
            self.current_agent = None
            self.start_time = None

        def on_chain_start(self, serialized: dict[str, Any], inputs: dict[str, Any], **kwargs) -> None:
            """é“¾å¼€å§‹æ—¶çš„å›è°ƒ"""
            self.current_chain = serialized.get("name", "unknown_chain")
            self.start_time = time.time()
            logger.debug(f"ğŸ”— LangChainé“¾å¼€å§‹: {self.current_chain}")

        def on_chain_end(self, outputs: dict[str, Any], **kwargs) -> None:
            """é“¾ç»“æŸæ—¶çš„å›è°ƒ"""
            if self.current_chain and self.start_time:
                duration = time.time() - self.start_time

                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                self.optimizer.metrics_history.append(
                    PerformanceMetrics(
                        operation=f"langchain_chain_{self.current_chain}",
                        start_time=self.start_time,
                        end_time=time.time(),
                        duration=duration
                    )
                )

                logger.debug(f"âœ… LangChainé“¾å®Œæˆ: {self.current_chain}, è€—æ—¶: {duration:.2f}s")

                # é‡ç½®çŠ¶æ€
                self.current_chain = None
                self.start_time = None

        def on_chain_error(self, error: Exception, **kwargs) -> None:
            """é“¾é”™è¯¯æ—¶çš„å›è°ƒ"""
            if self.current_chain and self.start_time:
                duration = time.time() - self.start_time

                # è®°å½•é”™è¯¯æŒ‡æ ‡
                self.optimizer.metrics_history.append(
                    PerformanceMetrics(
                        operation=f"langchain_chain_{self.current_chain}",
                        start_time=self.start_time,
                        end_time=time.time(),
                        duration=duration,
                        error=str(error)
                    )
                )

                logger.error(f"âŒ LangChainé“¾é”™è¯¯: {self.current_chain}, é”™è¯¯: {error}")

                # é‡ç½®çŠ¶æ€
                self.current_chain = None
                self.start_time = None

        def on_llm_start(self, serialized: dict[str, Any], prompts: list[str], **kwargs) -> None:
            """LLMå¼€å§‹æ—¶çš„å›è°ƒ"""
            self.start_time = time.time()
            logger.debug(f"ğŸ¤– LLMè°ƒç”¨å¼€å§‹: {serialized.get('name', 'unknown_llm')}")

        def on_llm_end(self, response: LLMResult, **kwargs) -> None:
            """LLMç»“æŸæ—¶çš„å›è°ƒ"""
            if self.start_time:
                duration = time.time() - self.start_time

                # è®¡ç®—tokenä½¿ç”¨æƒ…å†µ
                token_usage = response.llm_output.get("token_usage", {}) if response.llm_output else {}

                # è®°å½•LLMæ€§èƒ½æŒ‡æ ‡
                self.optimizer.metrics_history.append(
                    PerformanceMetrics(
                        operation="langchain_llm_call",
                        start_time=self.start_time,
                        end_time=time.time(),
                        duration=duration
                    )
                )

                logger.debug(
                    f"âœ… LLMè°ƒç”¨å®Œæˆ, è€—æ—¶: {duration:.2f}s, "
                    f"æç¤ºtokens: {token_usage.get('prompt_tokens', 0)}, "
                    f"å®Œæˆtokens: {token_usage.get('completion_tokens', 0)}"
                )

                self.start_time = None

        def on_llm_error(self, error: Exception, **kwargs) -> None:
            """LLMé”™è¯¯æ—¶çš„å›è°ƒ"""
            if self.start_time:
                duration = time.time() - self.start_time

                # è®°å½•LLMé”™è¯¯æŒ‡æ ‡
                self.optimizer.metrics_history.append(
                    PerformanceMetrics(
                        operation="langchain_llm_call",
                        start_time=self.start_time,
                        end_time=time.time(),
                        duration=duration,
                        error=str(error)
                    )
                )

                logger.error(f"âŒ LLMè°ƒç”¨é”™è¯¯: {error}")

                self.start_time = None

        def on_agent_action(self, action: AgentAction, **kwargs) -> Any:
            """AgentåŠ¨ä½œæ—¶çš„å›è°ƒ"""
            logger.debug(f"ğŸ¯ AgentåŠ¨ä½œ: {action.tool}, è¾“å…¥: {action.tool_input[:100]}...")

        def on_agent_finish(self, finish: AgentFinish, **kwargs) -> None:
            """Agentå®Œæˆæ—¶çš„å›è°ƒ"""
            logger.debug(f"ğŸ Agentå®Œæˆ: {finish.log[:100]}...")

        def on_text(self, text: str, **kwargs) -> None:
            """æ–‡æœ¬è¾“å‡ºæ—¶çš„å›è°ƒ"""
            pass  # é€šå¸¸ä¸éœ€è¦è®°å½•æ¯ä¸ªæ–‡æœ¬è¾“å‡º

        def on_tool_start(self, serialized: dict[str, Any], input_str: str, **kwargs) -> None:
            """å·¥å…·å¼€å§‹æ—¶çš„å›è°ƒ"""
            self.start_time = time.time()
            tool_name = serialized.get("name", "unknown_tool")
            logger.debug(f"ğŸ”§ å·¥å…·è°ƒç”¨å¼€å§‹: {tool_name}")

        def on_tool_end(self, output: str, **kwargs) -> None:
            """å·¥å…·ç»“æŸæ—¶çš„å›è°ƒ"""
            if self.start_time:
                duration = time.time() - self.start_time

                # è®°å½•å·¥å…·æ€§èƒ½æŒ‡æ ‡
                self.optimizer.metrics_history.append(
                    PerformanceMetrics(
                        operation="langchain_tool_call",
                        start_time=self.start_time,
                        end_time=time.time(),
                        duration=duration
                    )
                )

                logger.debug(f"âœ… å·¥å…·è°ƒç”¨å®Œæˆ, è€—æ—¶: {duration:.2f}s")

                self.start_time = None

        def on_tool_error(self, error: Exception, **kwargs) -> None:
            """å·¥å…·é”™è¯¯æ—¶çš„å›è°ƒ"""
            if self.start_time:
                duration = time.time() - self.start_time

                # è®°å½•å·¥å…·é”™è¯¯æŒ‡æ ‡
                self.optimizer.metrics_history.append(
                    PerformanceMetrics(
                        operation="langchain_tool_call",
                        start_time=self.start_time,
                        end_time=time.time(),
                        duration=duration,
                        error=str(error)
                    )
                )

                logger.error(f"âŒ å·¥å…·è°ƒç”¨é”™è¯¯: {error}")

                self.start_time = None


    class LangChainPerformanceOptimizer:
        """LangChain v1.0 æ€§èƒ½ä¼˜åŒ–å™¨"""

        def __init__(self):
            self.optimizer = get_performance_optimizer()
            self.middleware = LangChainPerformanceMiddleware(self.optimizer)

        def create_optimized_agent(self, agent_executor: AgentExecutor) -> AgentExecutor:
            """åˆ›å»ºä¼˜åŒ–çš„Agentæ‰§è¡Œå™¨"""
            # æ·»åŠ æ€§èƒ½ç›‘æ§å›è°ƒ
            if not hasattr(agent_executor, 'callbacks') or agent_executor.callbacks is None:
                agent_executor.callbacks = []

            if self.middleware not in agent_executor.callbacks:
                agent_executor.callbacks.append(self.middleware)

            return agent_executor

        async def optimize_langchain_chain(self, chain_func: Callable, *args, **kwargs) -> Any:
            """ä¼˜åŒ–LangChainé“¾æ‰§è¡Œ"""
            async with self.optimizer.measure_performance("langchain_chain_execution") as metrics:
                try:
                    # æ·»åŠ ç¼“å­˜é”®
                    cache_key = kwargs.pop("cache_key", None)

                    if cache_key:
                        # å°è¯•ä»ç¼“å­˜è·å–ç»“æœ
                        cache_manager = await get_cache_manager()
                        cached_result = await cache_manager.get(cache_key)

                        if cached_result is not None:
                            metrics.cache_hit = True
                            return cached_result

                    # æ‰§è¡Œé“¾
                    result = await chain_func(*args, **kwargs)

                    # ç¼“å­˜ç»“æœ
                    if cache_key and result is not None:
                        cache_manager = await get_cache_manager()
                        await cache_manager.set(cache_key, result, ttl=3600)

                    return result

                except Exception as e:
                    metrics.error = str(e)
                    raise

        def get_langchain_performance_stats(self) -> dict[str, Any]:
            """è·å–LangChainæ€§èƒ½ç»Ÿè®¡"""
            langchain_metrics = [
                m for m in self.optimizer.metrics_history
                if m.operation.startswith("langchain_")
            ]

            if not langchain_metrics:
                return {"message": "æš‚æ— LangChainæ€§èƒ½æ•°æ®"}

            # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
            operation_stats = defaultdict(list)
            for metric in langchain_metrics:
                operation_type = metric.operation.split("_", 2)[-1]  # æå–æ“ä½œç±»å‹
                operation_stats[operation_type].append(metric)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            detailed_stats = {}
            for op_type, metrics in operation_stats.items():
                op_count = len(metrics)
                op_total_time = sum(m.duration for m in metrics)
                op_avg_time = op_total_time / op_count if op_count > 0 else 0
                op_errors = sum(1 for m in metrics if m.error)

                detailed_stats[op_type] = {
                    "count": op_count,
                    "total_time": op_total_time,
                    "avg_time": op_avg_time,
                    "max_time": max(m.duration for m in metrics),
                    "min_time": min(m.duration for m in metrics),
                    "error_rate": (op_errors / op_count * 100) if op_count > 0 else 0
                }

            return {
                "summary": {
                    "total_langchain_operations": len(langchain_metrics),
                    "total_time": sum(m.duration for m in langchain_metrics),
                    "avg_time": sum(m.duration for m in langchain_metrics) / len(langchain_metrics)
                },
                "by_operation": detailed_stats,
                "recent_operations": [
                    {
                        "operation": m.operation,
                        "duration": m.duration,
                        "timestamp": m.start_time,
                        "error": m.error
                    }
                    for m in sorted(langchain_metrics, key=lambda x: x.start_time, reverse=True)[:10]
                ]
            }

        def get_langchain_recommendations(self) -> list[str]:
            """è·å–LangChainæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
            recommendations = []
            stats = self.get_langchain_performance_stats()

            if "message" in stats:
                return ["å¼€å§‹ä½¿ç”¨LangChainåŠŸèƒ½ä»¥æ”¶é›†æ€§èƒ½æ•°æ®"]

            # æ£€æŸ¥LLMè°ƒç”¨æ€§èƒ½
            llm_stats = stats.get("by_operation", {}).get("llm_call", {})
            avg_llm_time = llm_stats.get("avg_time", 0)

            if avg_llm_time > 10.0:
                recommendations.append(
                    f"LLMè°ƒç”¨å¹³å‡è€—æ—¶è¾ƒé•¿ ({avg_llm_time:.2f}s)ï¼Œå»ºè®®ï¼š"
                    "1. æ£€æŸ¥æ¨¡å‹é€‰æ‹©å’Œé…ç½®"
                    "2. ä¼˜åŒ–æç¤ºè¯é•¿åº¦"
                    "3. è€ƒè™‘ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è°ƒç”¨"
                )

            # æ£€æŸ¥å·¥å…·è°ƒç”¨æ€§èƒ½
            tool_stats = stats.get("by_operation", {}).get("tool_call", {})
            tool_error_rate = tool_stats.get("error_rate", 0)

            if tool_error_rate > 10:
                recommendations.append(
                    f"å·¥å…·è°ƒç”¨é”™è¯¯ç‡è¾ƒé«˜ ({tool_error_rate:.1f}%)ï¼Œå»ºè®®ï¼š"
                    "1. æ£€æŸ¥å·¥å…·è¾“å…¥å‚æ•°éªŒè¯"
                    "2. å¢åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"
                    "3. éªŒè¯å·¥å…·å¯ç”¨æ€§"
                )

            # æ£€æŸ¥é“¾æ‰§è¡Œæ€§èƒ½
            chain_stats = stats.get("by_operation", {}).get("chain_execution", {})
            avg_chain_time = chain_stats.get("avg_time", 0)

            if avg_chain_time > 15.0:
                recommendations.append(
                    f"é“¾æ‰§è¡Œå¹³å‡è€—æ—¶è¾ƒé•¿ ({avg_chain_time:.2f}s)ï¼Œå»ºè®®ï¼š"
                    "1. ç®€åŒ–é“¾ç»“æ„"
                    "2. å¹¶è¡ŒåŒ–ç‹¬ç«‹æ­¥éª¤"
                    "3. å¢åŠ ä¸­é—´ç»“æœç¼“å­˜"
                )

            return recommendations


    # å…¨å±€LangChainæ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹
    _langchain_optimizer = None


    def get_langchain_optimizer() -> LangChainPerformanceOptimizer:
        """è·å–å…¨å±€LangChainæ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹"""
        global _langchain_optimizer
        if _langchain_optimizer is None:
            _langchain_optimizer = LangChainPerformanceOptimizer()
        return _langchain_optimizer


except ImportError:
    logger.warning("âš ï¸ LangChainæœªå®‰è£…ï¼Œè·³è¿‡LangChainæ€§èƒ½ä¸­é—´ä»¶é›†æˆ")

    # æä¾›ç©ºçš„æ›¿ä»£å®ç°
    class LangChainPerformanceMiddleware:
        def __init__(self, *args, **kwargs):
            pass

    class LangChainPerformanceOptimizer:
        def __init__(self):
            pass

        def create_optimized_agent(self, agent_executor):
            return agent_executor

        async def optimize_langchain_chain(self, chain_func, *args, **kwargs):
            return await chain_func(*args, **kwargs)

        def get_langchain_performance_stats(self):
            return {"message": "LangChainæœªå®‰è£…"}

        def get_langchain_recommendations(self):
            return ["å®‰è£…LangChainä»¥å¯ç”¨æ€§èƒ½ç›‘æ§"]

    def get_langchain_optimizer():
        return LangChainPerformanceOptimizer()
