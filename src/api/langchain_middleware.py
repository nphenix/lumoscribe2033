"""
LangChain é›†æˆä¸­é—´ä»¶

åŸºäºŽ LangChain 1.0 Runnable/Callback ä½“ç³»çš„æœ¬åœ°åŒ–è¿½è¸ªæ–¹æ¡ˆï¼š
- è‡ªå®šä¹‰ AsyncCallbackHandler æ•èŽ· LLM/Runnable äº‹ä»¶
- æœ¬åœ° JSONL æ—¥å¿—ï¼ˆæ— éœ€ LangSmith æˆ–ä»»ä½•äº‘æœåŠ¡ï¼‰
- è¯·æ±‚çº§ RunnableConfig æ³¨å…¥ï¼Œæ–¹ä¾¿åœ¨ API/ä»»åŠ¡ä¸­ç»Ÿä¸€è¿½è¸ª
- LLM è¾“å…¥è¾“å‡ºã€æç¤ºè¯ã€æ€§èƒ½æŒ‡æ ‡å®Œæ•´å½’æ¡£
"""

from __future__ import annotations

import asyncio
import datetime
import json
import time
import uuid
from collections.abc import Callable
from pathlib import Path
from typing import Any, Optional

from fastapi import Request, Response
from langchain_core.callbacks.base import AsyncCallbackHandler
from langchain_core.outputs import LLMResult
from langchain_core.runnables.config import RunnableConfig
from loguru import logger

from src.framework.orchestrators.langchain_executor import (
    LangChainExecutor,
    get_executor_with_config,
)


class LangChainMiddleware:
    """
    LangChain è¿½è¸ªä¸­é—´ä»¶

    ä¸ºæ‰€æœ‰ LLM è°ƒç”¨æä¾›æœ¬åœ°è¿½è¸ªå’Œç›‘æŽ§åŠŸèƒ½
    """

    def __init__(
        self,
        project_name: str = "lumoscribe2033",
        tracing_enabled: bool = True,
        capture_io: bool = True,
        capture_metadata: bool = True,
        log_file: str = "logs/llm_traces.log",
        tracker: LLMCallTracker | None = None,
        prompt_logger: PromptLogger | None = None,
        default_tags: list[str] | None = None,
    ):
        self.project_name = project_name
        self.tracing_enabled = tracing_enabled
        self.capture_io = capture_io
        self.capture_metadata = capture_metadata
        self.log_file = Path(log_file)
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

        self.default_tags = default_tags or ["api-request"]
        self.tracker = tracker
        self.prompt_logger = prompt_logger

        if tracing_enabled:
            logger.info("âœ… LangChain æœ¬åœ°è¿½è¸ªå·²å¯ç”¨")

    async def __call__(self, request: Request, call_next: Callable) -> Response:
        """
        ä¸­é—´ä»¶è°ƒç”¨é€»è¾‘

        ä¸ºè¯·æ±‚æ·»åŠ  LangChain è¿½è¸ªä¸Šä¸‹æ–‡
        """
        if not self.tracing_enabled:
            return await call_next(request)

        trace_id = str(uuid.uuid4())
        request.state.trace_id = trace_id

        callback_handler = LocalTraceCallbackHandler(
            trace_id=trace_id,
            event_log=self.log_file,
            tracker=self.tracker,
            prompt_logger=self.prompt_logger,
            project_name=self.project_name,
        )

        runnable_config = RunnableConfig(
            callbacks=[callback_handler],
            tags=self.default_tags + [request.url.path],
            metadata={
                "trace_id": trace_id,
                "project": self.project_name,
                "path": request.url.path,
                "method": request.method,
            },
        )

        request.state.langchain_callback_handler = callback_handler
        request.state.langchain_runnable_config = runnable_config
        request.state.langchain_executor = _resolve_request_executor(runnable_config)

        start_time = time.time()

        try:
            response = await call_next(request)
            if self.capture_io:
                process_time = time.time() - start_time
                logger.debug(
                    f"ðŸ” [Trace-{trace_id}] è¯·æ±‚å®Œæˆ: {request.method} {request.url.path} ({process_time:.4f}s)"
                )
            return response
        except Exception as e:
            process_time = time.time() - start_time
            logger.error(
                f"âŒ [Trace-{trace_id}] è¯·æ±‚å¤±è´¥: {request.method} {request.url.path} - {str(e)} ({process_time:.4f}s)"
            )
            raise


class LLMCallTracker:
    """
    LLM è°ƒç”¨è¿½è¸ªå™¨

    ä¸“é—¨ç”¨äºŽæœ¬åœ°è¿½è¸ª LLM è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
    """

    def __init__(self, log_file: str = "logs/llm_calls.log"):
        self.log_file = Path(log_file)
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

    def track_llm_call(
        self,
        model_name: str,
        prompt: str,
        response: str,
        metadata: dict[str, Any] | None = None,
        tags: list[str] | None = None,
        trace_id: str | None = None
    ) -> None:
        """
        è¿½è¸ª LLM è°ƒç”¨

        Args:
            model_name: æ¨¡åž‹åç§°
            prompt: è¾“å…¥æç¤ºè¯
            response: æ¨¡åž‹å“åº”
            metadata: é¢å¤–å…ƒæ•°æ®
            tags: æ ‡ç­¾åˆ—è¡¨
            trace_id: è¿½è¸ª ID
        """
        try:
            # åˆ›å»ºè¿½è¸ªè®°å½•
            trace_data = {
                "timestamp": time.time(),
                "trace_id": trace_id or str(uuid.uuid4()),
                "model_name": model_name,
                "prompt": prompt,
                "response": response,
                "metadata": metadata or {},
                "tags": tags or [],
                "prompt_length": len(prompt),
                "response_length": len(response) if response else 0
            }

            # ä¿å­˜åˆ°æœ¬åœ°æ—¥å¿—æ–‡ä»¶
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(trace_data, ensure_ascii=False) + "\n")

            logger.debug(f"ðŸ“Š LLM è°ƒç”¨å·²è¿½è¸ª: {model_name}")

        except Exception as e:
            logger.error(f"âŒ LLM è°ƒç”¨è¿½è¸ªå¤±è´¥: {e}")

    def track_chain_execution(
        self,
        chain_name: str,
        inputs: dict[str, Any],
        outputs: dict[str, Any],
        steps: list[dict[str, Any]],
        metadata: dict[str, Any] | None = None,
        trace_id: str | None = None
    ) -> None:
        """
        è¿½è¸ªé“¾æ‰§è¡Œ

        Args:
            chain_name: é“¾åç§°
            inputs: è¾“å…¥æ•°æ®
            outputs: è¾“å‡ºæ•°æ®
            steps: æ‰§è¡Œæ­¥éª¤
            metadata: å…ƒæ•°æ®
            trace_id: è¿½è¸ª ID
        """
        try:
            # åˆ›å»ºé“¾è¿½è¸ªè®°å½•
            chain_data = {
                "timestamp": time.time(),
                "trace_id": trace_id or str(uuid.uuid4()),
                "chain_name": chain_name,
                "run_type": "chain",
                "inputs": inputs,
                "outputs": outputs,
                "metadata": metadata or {},
                "steps": steps,
                "step_count": len(steps)
            }

            # ä¿å­˜åˆ°æœ¬åœ°æ—¥å¿—æ–‡ä»¶
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(chain_data, ensure_ascii=False) + "\n")

            logger.debug(f"ðŸ”— é“¾æ‰§è¡Œå·²è¿½è¸ª: {chain_name}")

        except Exception as e:
            logger.error(f"âŒ é“¾æ‰§è¡Œè¿½è¸ªå¤±è´¥: {e}")


class PromptLogger:
    """
    æç¤ºè¯æ—¥å¿—è®°å½•å™¨

    ä¸“é—¨ç”¨äºŽè®°å½•å’Œåˆ†æžæç¤ºè¯ä½¿ç”¨æƒ…å†µ
    """

    def __init__(self, log_file: str = "logs/prompts.log"):
        self.log_file = Path(log_file)
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

    def log_prompt(
        self,
        prompt_type: str,
        prompt_content: str,
        model: str,
        response_time: float,
        token_usage: dict[str, int],
        success: bool = True,
        error: str | None = None,
        trace_id: str | None = None
    ) -> None:
        """
        è®°å½•æç¤ºè¯è°ƒç”¨

        Args:
            prompt_type: æç¤ºè¯ç±»åž‹
            prompt_content: æç¤ºè¯å†…å®¹
            model: ä½¿ç”¨çš„æ¨¡åž‹
            response_time: å“åº”æ—¶é—´
            token_usage: ä»¤ç‰Œä½¿ç”¨æƒ…å†µ
            success: æ˜¯å¦æˆåŠŸ
            error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æžœå¤±è´¥ï¼‰
            trace_id: è¿½è¸ª ID
        """
        import datetime

        log_data = {
            "timestamp": datetime.datetime.now().isoformat(),
            "trace_id": trace_id or str(uuid.uuid4()),
            "prompt_type": prompt_type,
            "prompt_content": prompt_content,
            "model": model,
            "response_time": response_time,
            "token_usage": token_usage,
            "success": success,
            "error": error,
            "prompt_length": len(prompt_content),
            "response_preview": prompt_content[:100] + "..." if len(prompt_content) > 100 else prompt_content
        }

        # ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶
        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_data, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.error(f"âŒ æç¤ºè¯æ—¥å¿—è®°å½•å¤±è´¥: {e}")

        logger.info(f"ðŸ“ æç¤ºè¯è°ƒç”¨: {prompt_type} - {model} - {response_time:.2f}s - {token_usage}")


class LLMRetryHandler:
    """
    LLM é‡è¯•å¤„ç†å™¨

    ä¸º LLM è°ƒç”¨æä¾›æ™ºèƒ½é‡è¯•æœºåˆ¶
    """

    def __init__(
        self,
        max_retries: int = 3,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        backoff_factor: float = 2.0
    ):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.backoff_factor = backoff_factor

    async def execute_with_retry(
        self,
        func: Callable,
        *args,
        retryable_errors: tuple = (Exception,),
        **kwargs
    ) -> Any:
        """
        æ‰§è¡Œå¸¦é‡è¯•çš„å‡½æ•°

        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            *args: å‡½æ•°ä½ç½®å‚æ•°
            retryable_errors: å¯é‡è¯•çš„é”™è¯¯ç±»åž‹
            **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°

        Returns:
            å‡½æ•°æ‰§è¡Œç»“æžœ
        """
        last_exception = None

        for attempt in range(self.max_retries + 1):
            try:
                return await func(*args, **kwargs)

            except retryable_errors as e:
                last_exception = e

                if attempt == self.max_retries:
                    logger.error(f"âŒ é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ ({self.max_retries}): {e}")
                    raise

                # è®¡ç®—é‡è¯•å»¶è¿Ÿ
                delay = min(
                    self.base_delay * (self.backoff_factor ** attempt),
                    self.max_delay
                )

                logger.warning(f"âš ï¸ LLM è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries + 1}): {e}")
                logger.info(f"ðŸ”„ {delay:.1f} ç§’åŽé‡è¯•...")

                await asyncio.sleep(delay)

        # å¦‚æžœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åŽä¸€æ¬¡çš„å¼‚å¸¸
        raise last_exception


class LocalTraceCallbackHandler(AsyncCallbackHandler):
    """åŸºäºŽ LangChain 1.0 Callback çš„æœ¬åœ°è¿½è¸ªå®žçŽ°"""

    def __init__(
        self,
        trace_id: str | None = None,
        event_log: Path | None = None,
        tracker: LLMCallTracker | None = None,
        prompt_logger: PromptLogger | None = None,
        project_name: str = "lumoscribe2033",
    ) -> None:
        self.trace_id = trace_id or str(uuid.uuid4())
        self.event_log = event_log or Path("logs/llm_traces.log")
        self.event_log.parent.mkdir(parents=True, exist_ok=True)
        self.tracker = tracker
        self.prompt_logger = prompt_logger
        self.project_name = project_name
        self._runs: dict[str, dict[str, Any]] = {}

    async def on_chain_start(self, serialized: dict[str, Any], inputs: dict[str, Any], run_id: str, parent_run_id: str | None, **kwargs: Any) -> None:
        self._write_event(
            "chain_start",
            {
                "run_id": str(run_id),
                "parent_run_id": str(parent_run_id) if parent_run_id else None,
                "chain": serialized.get("name"),
                "inputs": inputs,
            },
        )

    async def on_chain_end(self, outputs: dict[str, Any], run_id: str, parent_run_id: str | None, **kwargs: Any) -> None:
        self._write_event(
            "chain_end",
            {
                "run_id": str(run_id),
                "parent_run_id": str(parent_run_id) if parent_run_id else None,
                "outputs": outputs,
            },
        )

    async def on_chain_error(self, error: Exception, run_id: str, parent_run_id: str | None, **kwargs: Any) -> None:
        self._write_event(
            "chain_error",
            {
                "run_id": str(run_id),
                "parent_run_id": str(parent_run_id) if parent_run_id else None,
                "error": str(error),
            },
        )

    async def on_llm_start(self, serialized: dict[str, Any], prompts: list[str], run_id: str, parent_run_id: str | None, **kwargs: Any) -> None:
        prompt_text = "\n\n".join(prompts)
        self._runs[str(run_id)] = {
            "prompt": prompt_text,
            "model": serialized.get("id") or serialized.get("name") or "unknown",
            "start_time": time.time(),
            "metadata": kwargs.get("metadata") or {},
        }
        self._write_event(
            "llm_start",
            {
                "run_id": str(run_id),
                "parent_run_id": str(parent_run_id) if parent_run_id else None,
                "model": serialized.get("id") or serialized.get("name"),
                "prompt": prompt_text,
            },
        )

    async def on_llm_end(self, response: LLMResult, run_id, parent_run_id, **kwargs):
        context = self._runs.pop(str(run_id), {})
        response_text = self._extract_text(response)
        elapsed = time.time() - context.get("start_time", time.time())
        token_usage = self._extract_token_usage(response)

        if self.tracker:
            self.tracker.track_llm_call(
                model_name=context.get("model", "unknown"),
                prompt=context.get("prompt", ""),
                response=response_text,
                metadata={
                    "elapsed": elapsed,
                    "token_usage": token_usage,
                    "project": self.project_name,
                },
                trace_id=self.trace_id,
            )

        if self.prompt_logger and context.get("prompt"):
            self.prompt_logger.log_prompt(
                prompt_type=context.get("metadata", {}).get("type", "llm"),
                prompt_content=context["prompt"],
                model=context.get("model", "unknown"),
                response_time=elapsed,
                token_usage=token_usage,
                success=True,
                trace_id=self.trace_id,
            )

        self._write_event(
            "llm_end",
            {
                "run_id": str(run_id),
                "parent_run_id": str(parent_run_id) if parent_run_id else None,
                "response": response_text,
                "token_usage": token_usage,
                "elapsed": elapsed,
            },
        )

    async def on_llm_error(self, error: Exception, run_id: str, parent_run_id: str | None, **kwargs: Any) -> None:
        context = self._runs.pop(str(run_id), {})
        self._write_event(
            "llm_error",
            {
                "run_id": str(run_id),
                "parent_run_id": str(parent_run_id) if parent_run_id else None,
                "model": context.get("model"),
                "prompt": context.get("prompt"),
                "error": str(error),
            },
        )

    # Helper methods -----------------------------------------------------
    def _write_event(self, event_type: str, payload: dict[str, Any]) -> None:
        record = {
            "timestamp": datetime.datetime.utcnow().isoformat(),
            "trace_id": self.trace_id,
            "event": event_type,
            **payload,
        }
        with open(self.event_log, "a", encoding="utf-8") as fp:
            fp.write(json.dumps(record, ensure_ascii=False) + "\n")

    @staticmethod
    def _extract_text(result: LLMResult) -> str:
        texts: list[str] = []
        for generations in result.generations:
            for generation in generations:
                if hasattr(generation, "text") and generation.text:
                    texts.append(generation.text)
                elif hasattr(generation, "message") and getattr(generation.message, "content", None):
                    content = generation.message.content
                    if isinstance(content, list):
                        texts.extend(str(item) for item in content)
                    else:
                        texts.append(str(content))
        return "\n".join(texts)

    @staticmethod
    def _extract_token_usage(result: LLMResult) -> dict[str, int]:
        llm_output = result.llm_output or {}
        token_usage = llm_output.get("token_usage") or llm_output.get("usage") or {}
        return {k: int(v) for k, v in token_usage.items()} if isinstance(token_usage, dict) else {}


# å…¨å±€å®žä¾‹
_langchain_middleware: LangChainMiddleware | None = None
_llm_tracker: LLMCallTracker | None = None
_prompt_logger: PromptLogger | None = None
_retry_handler: LLMRetryHandler | None = None


def get_langchain_middleware() -> LangChainMiddleware | None:
    """èŽ·å– LangChain ä¸­é—´ä»¶å®žä¾‹"""
    return _langchain_middleware


def get_llm_tracker() -> LLMCallTracker | None:
    """èŽ·å– LLM è¿½è¸ªå™¨å®žä¾‹"""
    return _llm_tracker


def get_prompt_logger() -> PromptLogger | None:
    """èŽ·å–æç¤ºè¯æ—¥å¿—è®°å½•å™¨å®žä¾‹"""
    return _prompt_logger


def get_retry_handler() -> LLMRetryHandler | None:
    """èŽ·å–é‡è¯•å¤„ç†å™¨å®žä¾‹"""
    return _retry_handler


def get_request_runnable_config(request: Request) -> RunnableConfig | None:
    """ä»Ž FastAPI Request ä¸­èŽ·å– LangChain RunnableConfig"""
    return getattr(request.state, "langchain_runnable_config", None)


def get_request_executor(request: Request) -> LangChainExecutor | None:
    """ä»Ž FastAPI Request ä¸­èŽ·å– LangChainExecutor"""
    return getattr(request.state, "langchain_executor", None)


def initialize_langchain_middleware(
    project_name: str = "lumoscribe2033",
    tracing_enabled: bool = True,
    trace_log_file: str = "logs/llm_traces.log",
    llm_call_log: str = "logs/llm_calls.log",
    prompt_log_file: str = "logs/prompts.log",
    default_tags: list[str] | None = None,
) -> None:
    """
    åˆå§‹åŒ– LangChain ä¸­é—´ä»¶ç»„ä»¶ï¼ˆçº¯æœ¬åœ°å®žçŽ°ï¼‰
    """
    global _langchain_middleware, _llm_tracker, _prompt_logger, _retry_handler

    _llm_tracker = LLMCallTracker(log_file=llm_call_log)
    _prompt_logger = PromptLogger(log_file=prompt_log_file)
    _retry_handler = LLMRetryHandler()
    _langchain_middleware = LangChainMiddleware(
        project_name=project_name,
        tracing_enabled=tracing_enabled,
        log_file=trace_log_file,
        tracker=_llm_tracker,
        prompt_logger=_prompt_logger,
        default_tags=default_tags,
    )

    logger.info("ðŸš€ LangChain æœ¬åœ°ä¸­é—´ä»¶ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")


def create_langchain_middleware_factory(
    project_name: str = "lumoscribe2033",
    tracing_enabled: bool = True,
    trace_log_file: str = "logs/llm_traces.log",
    llm_call_log: str = "logs/llm_calls.log",
    prompt_log_file: str = "logs/prompts.log",
    default_tags: list[str] | None = None,
) -> LangChainMiddleware:
    """
    åˆ›å»ºæ–°çš„ LangChainMiddleware å®žä¾‹
    """
    tracker = LLMCallTracker(log_file=llm_call_log)
    prompt_logger = PromptLogger(log_file=prompt_log_file)
    return LangChainMiddleware(
        project_name=project_name,
        tracing_enabled=tracing_enabled,
        log_file=trace_log_file,
        tracker=tracker,
        prompt_logger=prompt_logger,
        default_tags=default_tags,
    )


def build_local_runnable_config(
    trace_id: str | None = None,
    metadata: dict[str, Any] | None = None,
    tags: list[str] | None = None,
) -> RunnableConfig:
    """
    æž„å»ºå¯åœ¨ä»»åŠ¡/è„šæœ¬ä¸­å¤ç”¨çš„ RunnableConfigï¼Œè‡ªåŠ¨æŽ¥å…¥æœ¬åœ°è¿½è¸ª
    """
    base_log = _langchain_middleware.log_file if _langchain_middleware else Path("logs/llm_traces.log")
    handler = LocalTraceCallbackHandler(
        trace_id=trace_id,
        event_log=base_log,
        tracker=_llm_tracker,
        prompt_logger=_prompt_logger,
    )
    merged_metadata = {"trace_id": handler.trace_id}
    if metadata:
        merged_metadata.update(metadata)
    return RunnableConfig(
        callbacks=[handler],
        metadata=merged_metadata,
        tags=tags or [],
    )


def _resolve_request_executor(config: RunnableConfig | None) -> LangChainExecutor | None:
    try:
        return get_executor_with_config(config)
    except RuntimeError:
        return None
